[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R for Economic Research",
    "section": "",
    "text": "Preface\nOver the past years I’ve received a lot of messages asking what I considered to be the most important subjects one should learn in order to start a career in economic research. R for Economic Research is my contribution to those who have some knowledge of R programming but still lack the necessary tools to carry out professional economic analysis. This is an intermediate-level book where the reader will find shortcuts to start working on a variety of tasks and also valuable references to delve into the details of more complex topics.\nThe reasoning behind the book can be described as follows.\nModern economic research requires a solid knowledge of a programming language. In fact, with a growing set of data now available through APIs it is possible to produce automated analyzes almost instantly using efficient techniques. In addition, unstructured data only becomes true information if correctly handled. I chose R because I strongly believe that Tidyverse is unrivaled as a data science workflow.\nIt does require more than just programming. Indeed, I have interviewed several applicants who were quite proficient in programming but lacked knowledge of basic topics on applied time series. For example, they didn’t know how to perform seasonal adjustment nor how to deflating nominal to real values. Filling in these gaps is crucial.\nKnowledge about forecasting is vital. I’m definitely not talking about state-of-the-art machine learning models. In fact, most problems can be addressed with traditional statistical methods. Setting up the workflow to generate reliable forecasts is the relevant skill here.\nI can’t help but talk about economic modelling. Estimating the relationship between economic variables and making projections is the core business of those who work with economic research. Despite being a topic that requires theoretical and applied training beyond the scope of this book, I believe that showing how to set up the framework for these models is a valuable contribution for those who want to start in the field.\nTaking it to the next level. Some tools allow us to considerably expand our possibilities. Learning how to build and estimate state-space models is undoubtedly a big step forward in becoming a senior analyst.\nI really hope that you enjoy reading this book and that it can benefit you in your career.\nAlso, I would be very happy to receive suggestions and feedback via email.\n\n\nAcknowledgements\nFirst and foremost, I’d like to thank all those who generously contribute to the R community – whether it’s developing packages, free content, or answering questions on Stack Overflow. Most of my learning I owe to you, and this book is, to a large extent, a way of giving back.\nI would also like to thank my therapist Fátima for all the emotional support on this journey. Writing this book was an act of making myself vulnerable in several ways: showing my skills publicly, writing in a language I am not a native speaker and much more.\nFinally, I dedicate this book to my family: my mom Célia, my dad João Bosco, my sister Nathália and her newborn boy João Gabriel.\n\n\nData\nAll the data sets used in the examples will be soon released as an R package. In the meantime, the relevant data are stored in the folder named data in the book official repository on Github.\n\n\nAuthor\nI’m a Senior Analyst at Kapitalo Investimentos, in charge of developing tools to improve the accuracy of economic forecasts. For more information about me and my work, visit rleripio.com.\n\n\nCitation\nLeripio, J. Renato. (2023) R for Economic Research: Essential tools for modern economic analysis. Edition 2023-04. Available at http://book.rleripio.com\n\n\nLicense\n\nR for Economic Research: Essential tools for modern economic analysis by J. Renato Leripio is licensed under Attribution-NonCommercial-ShareAlike 4.0 International"
  },
  {
    "objectID": "ds_tidyverse.html#whats-tidyverse",
    "href": "ds_tidyverse.html#whats-tidyverse",
    "title": "1  Introduction",
    "section": "1.1 What’s tidyverse?",
    "text": "1.1 What’s tidyverse?\nTo start, what’s tidyverse? According to the official website:\n\nThe tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures.\n\nFrom my own experience I can ensure you that tidyverse is all you need to carry out data analyses as neat and flexible as possible. By neat I mean succinct code without any redundancy, i.e, a single block of code (not a bunch of loose objects) doing all the desired task. Flexible, in turn, means the code is general enough. It depends on the nature of the task at hand, so take it more like a philosophy. For example, does the solution still work if the data is updated with new values? Or does it depend on changeable references such as column positions?\nMost of the time writing neat and flexible code takes longer and requires a dose of creativity, but it surely pays off. Moreover, with practice this will become more and more natural and the cost will get significantly lower. In the next sections we’ll review the most used functions to go through all the steps of everyday data analyses using core tidyverse packages designed to Import, Wrangle, Program and Plot."
  },
  {
    "objectID": "ds_tidyverse.html#import",
    "href": "ds_tidyverse.html#import",
    "title": "1  Introduction",
    "section": "1.2 Importing",
    "text": "1.2 Importing\n\n1.2.1 Reading from flat files\nreadr is the tidyverse’s package used to import data from flat files (basically .csv and .txt files). The most generic function is read_delim as it has a large set of parameters that allow us to declare the structure of the data we want to import. In practice, we often use the read_csv which is a special case of the former with some predefined settings suitable for .csv files – the most obvious being the comma as the column delimiter.\nIf you’re running RStudio IDE, I recommend you to click on Import Dataset at the Environment sheet (by default it’s located on the upper right panel) to manually define the appropriate settings for your file. I know we should never be encouraged to use windows, click on buttons or any kind of shortcuts provided by the IDE. This is the only exception I think is worth it. First, because it can be tedious to set many specific parameters via trial-and-error to correctly import your data. Second, because once you have the configuration done the code is displayed at the bottom of the window so you can copy it and paste it on your script and, eventually, get rid of this shortcut as you learn how things work. Lastly, you probably won’t repeat this action many times when working on a given task and there’s not much to improve on this process. Then, Yes, it’s a big waste of time trying to understand each single parameter.\nJust for the sake of illustration, let’s use readr to import the COVID data set from Our World in Data. It’s a standard .csv file, so we can use read_csv without any additional parameter. We just need to provide the file path:\n\nlibrary(tidyverse)\ndata_path  &lt;- 'data/owid-covid-data.csv'\ncovid_data &lt;- read_csv(data_path)\n\nor the URL:\n\nlibrary(tidyverse)\ndata_url   &lt;- 'https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/owid-covid-data.csv'\ncovid_data &lt;- read_csv(data_url)\n\nTidyverse also provides packages for reading other usual file types. For instance, we can use readxl to read MS Excel spreadsheets and haven to read data generated by popular statistical softwares (SPSS, Stata and SAS) in much the same way we did with readr for .csv files (you can check them at the Import Dataset dropdown menu) so we won’t cover them. Rather, let’s use the rest of this section to talk about something very important and that usually gets less attention in books: reading data from an API.\n\n\n1.2.2 Reading from API\nAlthough the use of traditional flat files are still widespread, organizations are increasingly switching to APIs to make data available to the public. This is a huge improvement since we’re able to customize our demand and import only the data we need for the specific task.\nIf you don’t know what an API is, think of it as an interface to a database. However, instead of reading all the content from this database you can inform what columns you want, the desired observations (time range if it’s a time series) and so on. These filters are embedded in a request and the right form to specify it is usually available in a documentation provided by the API maintainer.\nFor example, the Federal Reserve Bank of St. Louis maintains a huge repository with hundreds of thousands of economic time series and we can import them using an API. The information on how to build the request is available in the documentation, where we can see a link with instructions on how to ‘Get the observations or data values for an economic data series’. There, we find an example of a request for the US Real Gross National Product:\nhttps://api.stlouisfed.org/fred/series/observations?series_id=GNPCA&api_key=abcdefghijklmnopqrstuvwxyz123456&file_type=json\nWe can break this request down to see its parts in more detail:\n\nThe main (static) URL to access the Observations section:\n\n\\[\n\\underbrace{\\text{https://api.stlouisfed.org/fred/series/observations?}}_\\text{Static: API URL}\n\\]\n\nThe series ID which identifies the US Real Gross National Product data.\n\n\\[\n\\underbrace{\\text{series\\_id=GNPCA}}_\\text{Parameter: Series ID}\n\\]\n\nThe API Key. We must create an account and require a personal key in order to import data from the API. This one is an example for illustrative purpose only.\n\n\\[\n\\underbrace{\\text{api\\_key=abcdefghijklmnopqrstuvwxyz123456}}_\\text{Parameter: API Key}\n\\]\n\nThe file type for the output. There are several types, but I’d rather working with JSON.\n\n\\[\n\\underbrace{\\text{file\\_type=json}}_\\text{Parameter: File Type}\n\\]\nNote that all the parameters following the static part are separated by an &. Then, if we want to add any extra parameter it should be placed right after this character. Also, the order of the parameters is not relevant.\nSuppose we want to read monthly data for the US Consumer Price Index (CPI). The series_id is CPALTT01USM657N. Besides, we’d only like to read data between January 2010 and December 2022. How can we do so? There are two parameters – observation_start and observation_end – which set the range of the observation period (YYYY-MM-DD format).\nThe code below creates a separate object for each part of the request. Then, we use the glue function from the homonymous package to merge the pieces into a single string adding the & character between parameters. Note that we could create the full string all at once, but setting separate objects makes it easier to find and edit values when necessary as well as to transform this task into a custom function if we had to read data from this source often (more on this later).\nYou’ll need to register in order to be granted access to the API. Since this information is personal, I stored my key as an environmental variable named api_fred_key which can be accessed like any other object. If you want to create an environmental variable, the easiest way is through the function edit_r_environ() from the usethis package. This will open your .Renviron file, where you should place the following line: variable_name=variable_value. Then you must save it and restart your R session for changes to take effect. Now you can get the value from your environmental variable using Sys.getenv('variable_name')\n\nlibrary(glue)\napi_url       &lt;- 'https://api.stlouisfed.org/fred/series/observations?'\napi_fred_key  &lt;- Sys.getenv('api_fred_key')\napi_series_id &lt;- 'CPALTT01USM657N'\nobs_start     &lt;- '2010-01-01'\nobs_end       &lt;- '2022-12-01'\napi_filetype  &lt;- 'json'\napi_request   &lt;- glue('{api_url}series_id={api_series_id}&observation_start={obs_start}&observation_end={obs_end}&api_key={api_fred_key}&file_type={api_filetype}')\n\nNow we use the httr package to connect to the API, send the request and get the content. The other steps transform the content from JSON to a standard R object (a list) and then convert it to a tibble (the tidyverse’s improved format for data frames). Notice that the CPI data is stored in the list element named observations. However, this is specific for this API and if we were to import data from another source we’d have to check in which element the data would be stored.\n\nlibrary(httr)\nlibrary(jsonlite)\nlibrary(tidyverse)\ncpi_request &lt;- GET(url = api_request)\ncpi_content &lt;- content(cpi_request, as = 'text')\ncpi_list    &lt;- fromJSON(cpi_content, flatten = FALSE)\ncpi_tbl     &lt;- cpi_list[['observations']] %&gt;% as_tibble()\ncpi_tbl\n\n# A tibble: 156 × 4\n   realtime_start realtime_end date       value              \n   &lt;chr&gt;          &lt;chr&gt;        &lt;chr&gt;      &lt;chr&gt;              \n 1 2023-10-12     2023-10-12   2010-01-01 0.34174735701485   \n 2 2023-10-12     2023-10-12   2010-02-01 0.024920738207648  \n 3 2023-10-12     2023-10-12   2010-03-01 0.410628353657108  \n 4 2023-10-12     2023-10-12   2010-04-01 0.173688491069743  \n 5 2023-10-12     2023-10-12   2010-05-01 0.0775197354237721 \n 6 2023-10-12     2023-10-12   2010-06-01 -0.0976267084673888\n 7 2023-10-12     2023-10-12   2010-07-01 0.0211043057371509 \n 8 2023-10-12     2023-10-12   2010-08-01 0.138066427840807  \n 9 2023-10-12     2023-10-12   2010-09-01 0.0581736230715569 \n10 2023-10-12     2023-10-12   2010-10-01 0.124519888847696  \n# ℹ 146 more rows\n\n\nThis concludes our section on how to import data. If you’re struggling with other types of data, Tidyverse’s official website provides a comprehensive list of all the supported file formats and the respective packages used to handle them. Also, you can take a look at the rio package, which makes it easy to import/export data from/to different file extensions."
  },
  {
    "objectID": "ds_tidyverse.html#wrangling",
    "href": "ds_tidyverse.html#wrangling",
    "title": "1  Introduction",
    "section": "1.3 Wrangling",
    "text": "1.3 Wrangling\nThis is by far the most important section in this Chapter. The main goal here is to provide a general sense of how to get raw data ready to use. For this, we’ll focus on the roles of the main functions from dplyr package rather than the idiosyncrasies and generalizations of each one. More sophisticated applications will be shown in the next chapters.\n\n1.3.1 Data manipulation\nLet’s use the COVID data we imported in the last section. Starting with the glimpse function to have a grasp of the data, we can see some useful information such as the number of rows and columns, as well as columns’ names and classes (whether they’re character, double, etc).\n\ncovid_data %&gt;% \n  glimpse()\n\nWe’re usually not interested in all of these data, so the first couple of tasks we’d like to perform is to filter the relevant categories and select the desired columns. For example, we could be interested in analyzing new COVID cases (column new_cases) only in Brazil (rows equal to Brazil in column location). Furthermore, we’d like to get rid of duplicate rows if there’s any.\nOne of the great contributions of tidyverse is to assign names (verbs) to the functions according to the actions they perform – many are admittedly SQL-inspired. For example, distinct drops observations (rows) which are not unique, whereas select picks variables based on their names (or positions). The exception is filter, which retains the rows which satisfy a given condition (the analogue of WHERE in SQL).\nConditions are sentences which returns TRUE or FALSE. It’s straightforward to think of conditions using logical operators, such as ==, &gt;, &lt;, etc. Nevertheless, there’s a bunch of expressions in R which return TRUE or FALSE. Moreover, we can always create our own condition to generate the desired output. We’ll see many examples throughout this book.\nThe code below performs the initial steps described above to generate a subset of the data.\n\ncovid_data_sub1 &lt;- covid_data %&gt;% \n  distinct() %&gt;% \n  select(date, continent, location, new_cases) %&gt;% \n  filter(location == 'Brazil')\n\nNext, we may need to create additional columns. For instance, suppose that actual new cases in Brazil are much higher than reported because the country doesn’t have enough tests and a conservative estimate points to a number of, say, twice the official count. So we’d like to add a column which is twice the value of the original one representing our guess of the real situation. In addition, we’d also like to create a column to indicate the dominant strain at each period of time. We know that the Delta strain took over Gamma by the end of July 2021 and, then, Omicron took over Delta in the beginning of 2022.\nThe mutate verb can be used to create new variables as a function of existing ones. Also, it can be used to modify existing variables as new variables overwrites those with the same name. case_when is another SQL-inspired function used inside mutate to create a new variable based on conditions. It’s worth noting that it returns NA if no condition is met. In this case, a useful workaround is to define an extra condition as TRUE ~ value, thus assigning a value to all the unmet conditions – think of this as an else statement.\n\ncovid_data_sub2 &lt;- covid_data_sub1 %&gt;% \n  mutate(\n    real_new_cases = 2*new_cases,\n    dominant_strain  = case_when(\n      date &lt;= '2021-07-31'                        ~ 'Gamma',\n      date &gt;  '2021-07-31' & date &lt;= '2021-12-31' ~ 'Delta',\n      date &gt;  '2021-12-31' & date &lt;= '2022-02-01' ~ 'Omicron',\n      TRUE                                        ~ \"We don't know\"\n    )\n  )\n\nThe between function is a shortcut for numeric conditions that are bounded both on the left and on the right. It also works with dates if we declare the arguments as date objects. Therefore, we can replace conditions 2 and 3 in order to have a more compact and efficient code (it’s implemented in C++, like many modern R functions).\n\ncovid_data_sub2 &lt;- covid_data_sub1 %&gt;% \n  mutate(\n    real_new_cases = 2*new_cases,\n    dominant_strain  = case_when(\n      date &lt;= '2021-07-31'                                          ~ 'Gamma',\n      between(date, as.Date('2021-07-31'), as.Date('2021-12-31'))   ~ 'Delta',\n      between(date, as.Date('2021-12-31'), as.Date('2022-02-01'))   ~ 'Omicron',\n      TRUE                                                          ~ \"We don't know\"\n    )\n  )\n\nSo far we’ve worked on a single group of data: new cases in Brazil. However, we usually have many categories to work on. We might be interested in analyzing new cases in all European countries, for example. In this case, we’ll need the group_by function, which allows us to perform operations by group. group_by is often used in conjunction with mutate or summarise to create new data for each group. The latter uses aggregate functions (mean, max, min, etc) to produce a summary of the data.\nFor example, suppose we want to know which European country recorded the highest number of new covid cases in a single day by the mid of 2022. This might be achieved by grouping the data by location and then using summarise with max. In addition, we can use arrange to sort the rows by value (we use desc to sort in descending order). Don’t forget to ungroup data as soon as you no longer need to perform grouped operations.\n\ncovid_data_sub3 &lt;- covid_data %&gt;% \n  distinct() %&gt;% \n  filter(\n    continent == 'Europe',\n    date &lt;= '2022-06-30'\n    ) %&gt;% \n  group_by(location) %&gt;% \n  summarise(max_new_cases = max(new_cases)) %&gt;% \n  ungroup() %&gt;% \n  arrange(desc(max_new_cases))\ncovid_data_sub3\n\n# A tibble: 55 × 2\n   location       max_new_cases\n   &lt;chr&gt;                  &lt;dbl&gt;\n 1 France               2417043\n 2 Germany              1588891\n 3 Spain                 956506\n 4 Netherlands           626302\n 5 Portugal              405306\n 6 United Kingdom        275647\n 7 Ukraine               242942\n 8 Greece                241319\n 9 Italy                 228123\n10 Russia                203949\n# ℹ 45 more rows\n\n\nNote that some countries such as Spain, Portugal and France returned NA. This happened because they probably have any missing value. We could easily ignore it by passing the argument na.rm = TRUE to the max function. However, another problem would arise: countries with no data on new cases would return -Inf. To get around these two issues, we can filter all the missing values out in the data set using the logical operator is.na() (the ! before the condition works as a negation). In this case removing the missing values isn’t a problem, but be aware that for some tasks this may influence the outcome.\n\ncovid_data_sub4 &lt;- covid_data %&gt;% \n  distinct() %&gt;% \n  filter(\n    continent == 'Europe',\n    date &lt;= '2022-06-30',\n    !is.na(new_cases)\n    ) %&gt;% \n  group_by(location) %&gt;% \n  summarise(max_new_cases = max(new_cases)) %&gt;% \n  ungroup() %&gt;% \n  arrange(desc(max_new_cases))\ncovid_data_sub4\n\n# A tibble: 51 × 2\n   location       max_new_cases\n   &lt;chr&gt;                  &lt;dbl&gt;\n 1 France               2417043\n 2 Germany              1588891\n 3 Spain                 956506\n 4 Netherlands           626302\n 5 Portugal              405306\n 6 United Kingdom        275647\n 7 Ukraine               242942\n 8 Greece                241319\n 9 Italy                 228123\n10 Russia                203949\n# ℹ 41 more rows\n\n\nIn addition, we might want to know not only what the highest numbers were but when they did occur (the peak date). Since the summarise function is designed to return a single value, we must use an expression which returns a single value. If we used date = max(date), we’d keep the most recent date for the data in each country. Definitely, that’s not what we want. So a good way to address this issue is to combine a subset operation with a condition inside. In simpler terms, we’ll subset from the column date the observation where new cases were at its high. Since we can have multiple dates which satisfy this condition, we’ll keep the most recent one (the max of them).\n\ncovid_data_sub5 &lt;- covid_data %&gt;% \n  distinct() %&gt;% \n  filter(\n    continent == 'Europe',\n    date &lt;= '2022-06-30',\n    !is.na(new_cases)\n    ) %&gt;% \n  group_by(location) %&gt;% \n  summarise(\n    max_new_cases = max(new_cases),\n    peak_date = date[which(new_cases == max_new_cases)] %&gt;% max()\n    ) %&gt;% \n  ungroup() %&gt;% \n  arrange(desc(max_new_cases), peak_date) %&gt;% \n  slice(1:10)\ncovid_data_sub5\n\n# A tibble: 10 × 3\n   location       max_new_cases peak_date \n   &lt;chr&gt;                  &lt;dbl&gt; &lt;date&gt;    \n 1 France               2417043 2022-01-23\n 2 Germany              1588891 2022-03-27\n 3 Spain                 956506 2022-01-16\n 4 Netherlands           626302 2022-02-06\n 5 Portugal              405306 2022-01-30\n 6 United Kingdom        275647 2022-01-06\n 7 Ukraine               242942 2022-02-06\n 8 Greece                241319 2022-01-09\n 9 Italy                 228123 2022-01-19\n10 Russia                203949 2022-02-11\n\n\nIn case it went unnoticed, the previous code showed a really nice feature of mutate/summarise: you can use a variable you’ve just created in a subsequent task inside the same calling. In this example we used max_new_cases as an input in peak_date, all of this inside the same summarise calling as if all steps were performed sequentially.\nIn the last two lines, we used arrange to sort countries firstly by the maximum number of new cases and then by their peak date and slice to subset only the first ten countries out of the forty-nine in our data set. This ends our approach to single data sets for now.\nBut before we jump to the next section, there’s something very important to address: merging multiple data sets. There are two families of functions in dplyr to merge data frames, *_join and bind_*. Let’s see how they work.\nThe *_join functions are used to merge two data frames horizontally, matching their rows based on specified keys. For example, take the covid_data_sub5 data frame we created above. It contains the top ten European countries with the highest number of new Covid cases in a single day and their peak dates. Suppose we want to add information on the population size for each country, which is available in another data frame named europe_population displayed below.1\n\n\n# A tibble: 55 × 2\n   Country         Pop_size\n   &lt;chr&gt;              &lt;dbl&gt;\n 1 Russia         144713312\n 2 Germany         83369840\n 3 France          67813000\n 4 United Kingdom  67508936\n 5 Italy           59037472\n 6 England         56550000\n 7 Spain           47558632\n 8 Poland          39857144\n 9 Ukraine         39701744\n10 Romania         19659270\n# ℹ 45 more rows\n\n\nWhat we want here is to add the column Pop_Size from europe_population into covid_data_sub5 matching rows based on location (the column with countries in the main data set). For this, we can use the left_join function, which adds the content from the second data frame to the first one. The by argument is needed because the name of the key column differs across the two data sets.2\n\ncovid_data_sub5_with_pop &lt;- covid_data_sub5 %&gt;% \n  left_join(europe_population, by = c('location' = 'Country'))\ncovid_data_sub5_with_pop\n\n# A tibble: 10 × 4\n   location       max_new_cases peak_date   Pop_size\n   &lt;chr&gt;                  &lt;dbl&gt; &lt;date&gt;         &lt;dbl&gt;\n 1 France               2417043 2022-01-23  67813000\n 2 Germany              1588891 2022-03-27  83369840\n 3 Spain                 956506 2022-01-16  47558632\n 4 Netherlands           626302 2022-02-06  17564020\n 5 Portugal              405306 2022-01-30  10270857\n 6 United Kingdom        275647 2022-01-06  67508936\n 7 Ukraine               242942 2022-02-06  39701744\n 8 Greece                241319 2022-01-09  10384972\n 9 Italy                 228123 2022-01-19  59037472\n10 Russia                203949 2022-02-11 144713312\n\n\nWe could have been assigned a slightly different task. For example, adding to european_population the information on maximum daily number of new Covid cases and peak dates we have from covid_data_sub5. This can also be achieved with left_join by reversing the order of the data frames (and names of the key column since they’re not the same). An effortless alternative is to replace left_join with right_join, which adds to the second data frame (the one on the right) the information from the first data frame (the one on the left). In this case we don’t need to change the order of the parameters.\n\npop_with_covid_info &lt;- covid_data_sub5 %&gt;% \n  right_join(europe_population, by = c('location' = 'Country'))\npop_with_covid_info\n\n# A tibble: 55 × 4\n   location       max_new_cases peak_date   Pop_size\n   &lt;chr&gt;                  &lt;dbl&gt; &lt;date&gt;         &lt;dbl&gt;\n 1 France               2417043 2022-01-23  67813000\n 2 Germany              1588891 2022-03-27  83369840\n 3 Spain                 956506 2022-01-16  47558632\n 4 Netherlands           626302 2022-02-06  17564020\n 5 Portugal              405306 2022-01-30  10270857\n 6 United Kingdom        275647 2022-01-06  67508936\n 7 Ukraine               242942 2022-02-06  39701744\n 8 Greece                241319 2022-01-09  10384972\n 9 Italy                 228123 2022-01-19  59037472\n10 Russia                203949 2022-02-11 144713312\n# ℹ 45 more rows\n\n\nAt first glance it seems to produce the same outcome, but notice that the number of rows are different in the two resulting data sets. In the first case left_join kept all the rows from covid_data_sub5 and only the corresponding rows from europe_population, whereas right_join did the opposite. In summary, left_join and right_join keep all the rows from just one of the two data frames.\nSometimes, however, we want to keep all the rows from both data frames. For example, imagine that covid_data_sub5 had data not only on European but on South American countries as well. In addition, the european_population data frame also included the populations from Asian countries. If we merged the data frames using the functions above, we’d end up loosing observations on either Covid in South America or populations in Asia.\nIn order to merge the data frames by their common countries while keeping the remaining observations from both, we should employ the full_join function. On the other hand, if we’re willing to keep only the common countries in both data frames, this is the case for the inner_join function.\nThese four functions comprise what it’s called mutating-joins since they add columns from one data frame to the other. There are also the filtering-joins functions, which filter the rows from one data frame based on the presence (semi_join) or absence (anti_join) of matches in the other data frame. Given the latter category is used to a much lesser extent, I won’t go into detail right now. Eventually they’ll show up in the coming chapters.\nWe saw that *_join operations are particularly useful to merge data frames horizontally and by their matching rows according to key variables. More often than not we need to stack data vertically by their matching columns. For example, suppose the Covid data set was released as a single file for each country and we needed to perform some comparisons between France and the United Kingdom - covid_fr and covid_uk data sets shown below.\n\n\n# A tibble: 1,371 × 3\n   date       location       new_cases\n   &lt;date&gt;     &lt;chr&gt;              &lt;dbl&gt;\n 1 2020-01-03 United Kingdom         0\n 2 2020-01-04 United Kingdom         0\n 3 2020-01-05 United Kingdom         0\n 4 2020-01-06 United Kingdom         0\n 5 2020-01-07 United Kingdom         0\n 6 2020-01-08 United Kingdom         0\n 7 2020-01-09 United Kingdom         0\n 8 2020-01-10 United Kingdom         0\n 9 2020-01-11 United Kingdom         0\n10 2020-01-12 United Kingdom         0\n# ℹ 1,361 more rows\n\n\n# A tibble: 1,371 × 4\n   date       location new_cases total_cases\n   &lt;date&gt;     &lt;chr&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n 1 2020-01-03 France           0          NA\n 2 2020-01-04 France           0          NA\n 3 2020-01-05 France           0          NA\n 4 2020-01-06 France           0          NA\n 5 2020-01-07 France           0          NA\n 6 2020-01-08 France           0          NA\n 7 2020-01-09 France           0          NA\n 8 2020-01-10 France           0          NA\n 9 2020-01-11 France           0          NA\n10 2020-01-12 France           0          NA\n# ℹ 1,361 more rows\n\n\nThis can be easily accomplished by bind_rows which, unlike the *_join family, allows us to provide several data frames. What’s cool about bind_rows is that non-matching columns are kept with their values filled with NA for the data frames where the column is absent. In the example above, the covid_fr data set has a total cases column which is absent in covid_uk.\n\ncovid_fr_uk &lt;- bind_rows(covid_fr, covid_uk)\ncovid_fr_uk\n\n# A tibble: 2,742 × 4\n   date       location new_cases total_cases\n   &lt;date&gt;     &lt;chr&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n 1 2020-01-03 France           0          NA\n 2 2020-01-04 France           0          NA\n 3 2020-01-05 France           0          NA\n 4 2020-01-06 France           0          NA\n 5 2020-01-07 France           0          NA\n 6 2020-01-08 France           0          NA\n 7 2020-01-09 France           0          NA\n 8 2020-01-10 France           0          NA\n 9 2020-01-11 France           0          NA\n10 2020-01-12 France           0          NA\n# ℹ 2,732 more rows\n\n\nThe bind_cols function is more restrictive in this regard. It’s used to merge the columns of data frames, but matching by row position rather than a key variable. For this reason, we can’t have data frames of different lengths. In practice, it’s much more common (and safe) to rely on *_join functions when we need to merge data frames horizontally.\n\n\n1.3.2 Data layout\nWe’ve walked through the main functions of dplyr. Now, we turn to the tidyr package. According to the tidyverse’s website, the goal of tidyr is to help us create tidy data. It means a data set where every column is a variable; every row is an observation; and each cell is a single value.\nTidy data is also known as wide format – since it increases the number of columns and decreases the number of rows –, as opposed to the long format, where data are stacked increasing the number of rows and decreasing the number of columns. It took me a while before I could tell if a data set was either in wide or long format, so don’t worry if it’s not so clear right now. Perhaps a more direct way of thinking about this distinction is to ask yourself: Is all the information contained in a single cell? If so, it’s wide format. If not, then it’s long format.\nFor example, is the Covid data set in wide or long format? If we took a single cell from the new_cases column, does it convey all the information for this variable contained in the data set? No, it doesn’t. We know the number of new cases in a given date, but we don’t know the country that value refers to – is this from Germany? Nigeria? Chile?\nWe can use the pivot_wider function from tidyr package to convert from long to wide format. The syntax is very easy to understand: names_from is the column we want to widen, whereas values_from is the column containing the observations that will fill each cell. id_cols is a parameter used to declare the set of columns that uniquely identifies each observation. In practice, it drops all the other columns in the data set. Hence, if we want to keep all the other variables, just skip it.\n\ncovid_wide_01 &lt;- covid_data %&gt;% \n  pivot_wider(\n    id_cols     = 'date', \n    names_from  = 'location', \n    values_from = 'new_cases'\n  )\ncovid_wide_01\n\n# A tibble: 1,380 × 256\n   date       Afghanistan Africa Albania Algeria `American Samoa` Andorra Angola\n   &lt;date&gt;           &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;            &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1 2020-01-03           0      0       0       0                0       0      0\n 2 2020-01-04           0      0       0       0                0       0      0\n 3 2020-01-05           0      0       0       0                0       0      0\n 4 2020-01-06           0      0       0       0                0       0      0\n 5 2020-01-07           0      0       0       0                0       0      0\n 6 2020-01-08           0      0       0       0                0       0      0\n 7 2020-01-09           0      0       0       0                0       0      0\n 8 2020-01-10           0      0       0       0                0       0      0\n 9 2020-01-11           0      0       0       0                0       0      0\n10 2020-01-12           0      0       0       0                0       0      0\n# ℹ 1,370 more rows\n# ℹ 248 more variables: Anguilla &lt;dbl&gt;, `Antigua and Barbuda` &lt;dbl&gt;,\n#   Argentina &lt;dbl&gt;, Armenia &lt;dbl&gt;, Aruba &lt;dbl&gt;, Asia &lt;dbl&gt;, Australia &lt;dbl&gt;,\n#   Austria &lt;dbl&gt;, Azerbaijan &lt;dbl&gt;, Bahamas &lt;dbl&gt;, Bahrain &lt;dbl&gt;,\n#   Bangladesh &lt;dbl&gt;, Barbados &lt;dbl&gt;, Belarus &lt;dbl&gt;, Belgium &lt;dbl&gt;,\n#   Belize &lt;dbl&gt;, Benin &lt;dbl&gt;, Bermuda &lt;dbl&gt;, Bhutan &lt;dbl&gt;, Bolivia &lt;dbl&gt;,\n#   `Bonaire Sint Eustatius and Saba` &lt;dbl&gt;, `Bosnia and Herzegovina` &lt;dbl&gt;, …\n\n\nNotice that now we have 255 columns rather than 67 of the original data set with each cell conveying the whole information: new cases in a given date for a specific country. So, what if we wanted to have both new_cases and new_deaths columns in wide form? We just need to provide a vector with the desired variables in values_from. By default, the new columns will be named according to the following pattern: variable_location.\nSince the variables names already contain a single underscore, it’s a good idea to set a different character as separator – a double underscore works fine. This is because we might need to reverse the operation later for a given task, then it’s much easier to identify it. Otherwise, we’d have to use regular expression to inform the specific position of the repeated character – for example, whether it’s the first or the second underscore.\n\ncovid_wide_02 &lt;- covid_data %&gt;% \n  pivot_wider(\n    id_cols     = 'date', \n    names_from  = 'location', \n    values_from = c('new_cases', 'new_deaths'),\n    names_sep   = '__'\n  )\ncovid_wide_02\n\n# A tibble: 1,380 × 511\n   date       new_cases__Afghanistan new_cases__Africa new_cases__Albania\n   &lt;date&gt;                      &lt;dbl&gt;             &lt;dbl&gt;              &lt;dbl&gt;\n 1 2020-01-03                      0                 0                  0\n 2 2020-01-04                      0                 0                  0\n 3 2020-01-05                      0                 0                  0\n 4 2020-01-06                      0                 0                  0\n 5 2020-01-07                      0                 0                  0\n 6 2020-01-08                      0                 0                  0\n 7 2020-01-09                      0                 0                  0\n 8 2020-01-10                      0                 0                  0\n 9 2020-01-11                      0                 0                  0\n10 2020-01-12                      0                 0                  0\n# ℹ 1,370 more rows\n# ℹ 507 more variables: new_cases__Algeria &lt;dbl&gt;,\n#   `new_cases__American Samoa` &lt;dbl&gt;, new_cases__Andorra &lt;dbl&gt;,\n#   new_cases__Angola &lt;dbl&gt;, new_cases__Anguilla &lt;dbl&gt;,\n#   `new_cases__Antigua and Barbuda` &lt;dbl&gt;, new_cases__Argentina &lt;dbl&gt;,\n#   new_cases__Armenia &lt;dbl&gt;, new_cases__Aruba &lt;dbl&gt;, new_cases__Asia &lt;dbl&gt;,\n#   new_cases__Australia &lt;dbl&gt;, new_cases__Austria &lt;dbl&gt;, …\n\n\nOur new data set expanded to 510 columns. As we create more and more columns to get a wide data set it might become harder to perform some simple tasks. In fact, using filter is generally easier than using a conditional select when we want to keep only the relevant data.\nIn summary, long format may be preferable over wide format when the data set contains more than one grouping variable or we want to work on more than one variable. Besides, long format data sets are ideal for plotting with ggplot2 package as we’ll see later.\nTherefore, it’s not unusual to convert a data set from wide to long format. The syntax is very similar to what we saw earlier when converting from long to wide format. The only difference is in the cols argument, used to declare what columns we want to stack. However, since wide data sets usually have a large number of columns and we’re often interested in putting all of them in long format, it’s much easier to declare what columns we want to leave out (the - operator).\n\ncovid_long_01 &lt;- covid_wide_02 %&gt;% \n  pivot_longer(\n    cols      = -'date',\n    names_to  = c('variable', 'location'),\n    values_to = 'value',\n    names_sep = '__'\n  )\ncovid_long_01\n\n# A tibble: 703,800 × 4\n   date       variable  location            value\n   &lt;date&gt;     &lt;chr&gt;     &lt;chr&gt;               &lt;dbl&gt;\n 1 2020-01-03 new_cases Afghanistan             0\n 2 2020-01-03 new_cases Africa                  0\n 3 2020-01-03 new_cases Albania                 0\n 4 2020-01-03 new_cases Algeria                 0\n 5 2020-01-03 new_cases American Samoa          0\n 6 2020-01-03 new_cases Andorra                 0\n 7 2020-01-03 new_cases Angola                  0\n 8 2020-01-03 new_cases Anguilla                0\n 9 2020-01-03 new_cases Antigua and Barbuda     0\n10 2020-01-03 new_cases Argentina               0\n# ℹ 703,790 more rows\n\n\nNote that now even the variables (new_cases and new_deaths) are stored in a single column (variable). This is probably an abuse of language, but I call this a complete long format – as opposed to the original format where the data set was only partially in the long format (the variables were indeed in wide format). For most applications, I think this is the best way to organize the data.\nConverting a data set between wide and long formats might not completely solve our problem. Sometimes, two pieces of information are merged in a single column. For example, suppose that the location column had both the continent and country names instead of only the country as in the original data set. We’ll call this data set covid_loc_cont.\n\n\n# A tibble: 330,156 × 4\n   location         date       new_cases new_deaths\n   &lt;chr&gt;            &lt;date&gt;         &lt;dbl&gt;      &lt;dbl&gt;\n 1 Afghanistan_Asia 2020-01-03         0          0\n 2 Afghanistan_Asia 2020-01-04         0          0\n 3 Afghanistan_Asia 2020-01-05         0          0\n 4 Afghanistan_Asia 2020-01-06         0          0\n 5 Afghanistan_Asia 2020-01-07         0          0\n 6 Afghanistan_Asia 2020-01-08         0          0\n 7 Afghanistan_Asia 2020-01-09         0          0\n 8 Afghanistan_Asia 2020-01-10         0          0\n 9 Afghanistan_Asia 2020-01-11         0          0\n10 Afghanistan_Asia 2020-01-12         0          0\n# ℹ 330,146 more rows\n\n\nThis is undesirable since we can no longer use group_by or filter over either country or continents names alone. Hence, the best practice is to have a single column for each variable. This can be easily achieved using the separate function, with a highly self-explanatory syntax. Again, the separator character being unique in the string makes the job much easier.\n\ncovid_separate &lt;- covid_loc_cont %&gt;% \n  separate(\n    col  = 'location',\n    into = c('location', 'continent'),\n    sep  = '_'\n  )\ncovid_separate\n\n# A tibble: 330,156 × 5\n   location    continent date       new_cases new_deaths\n   &lt;chr&gt;       &lt;chr&gt;     &lt;date&gt;         &lt;dbl&gt;      &lt;dbl&gt;\n 1 Afghanistan Asia      2020-01-03         0          0\n 2 Afghanistan Asia      2020-01-04         0          0\n 3 Afghanistan Asia      2020-01-05         0          0\n 4 Afghanistan Asia      2020-01-06         0          0\n 5 Afghanistan Asia      2020-01-07         0          0\n 6 Afghanistan Asia      2020-01-08         0          0\n 7 Afghanistan Asia      2020-01-09         0          0\n 8 Afghanistan Asia      2020-01-10         0          0\n 9 Afghanistan Asia      2020-01-11         0          0\n10 Afghanistan Asia      2020-01-12         0          0\n# ℹ 330,146 more rows\n\n\nThe only caveat to all this simplicity is when we have non-trivial separators. For example, imagine that we had no underscore to separate location from continent.\n\n\n# A tibble: 330,156 × 4\n   location        date       new_cases new_deaths\n   &lt;chr&gt;           &lt;date&gt;         &lt;dbl&gt;      &lt;dbl&gt;\n 1 AfghanistanAsia 2020-01-03         0          0\n 2 AfghanistanAsia 2020-01-04         0          0\n 3 AfghanistanAsia 2020-01-05         0          0\n 4 AfghanistanAsia 2020-01-06         0          0\n 5 AfghanistanAsia 2020-01-07         0          0\n 6 AfghanistanAsia 2020-01-08         0          0\n 7 AfghanistanAsia 2020-01-09         0          0\n 8 AfghanistanAsia 2020-01-10         0          0\n 9 AfghanistanAsia 2020-01-11         0          0\n10 AfghanistanAsia 2020-01-12         0          0\n# ℹ 330,146 more rows\n\n\nHow could we manage to separate them? Ideally, we should provide a regular expression (or simply regex) to match the appropriate pattern to split the string into location and continent (the sep argument works with regex). If you don’t know what regex is, think of it as a code used to match patterns, positions and all kinds of features in a string.\nAt first glance, a natural choice would be to split the string as of the second uppercase letter. This would work for Afghanistan, France, Netherlands, Chile and all single-worded countries. However, this would fail for countries with two or more words: United States, New Zealand and many others.\nThen, you could argue that a more general approach would be to use regex to match the last uppercase letter in the string. Not actually, because we have a couple of two-worded continents: North America and South America. So, for example, CanadaNorth America would be split into CanadaNorth and America instead of Canada and North America.\nMore often than not, the direct solution is the most difficult to implement (or we simply don’t know how to accomplish it) and so we have to think about alternative ways. Data science is all about this. Since a regex solution alone might be very tough or even unfeasible, we’ll get back to this problem later when covering text manipulation with the stringr package.\nFor now, let’s just get done with the tidyr package looking at two other very commonly used functions. The first one is unite, which is the counterpart of separate. We can use it to convert the covid_separate data frame back to the original form as in covid_loc_cont.\n\ncovid_unite &lt;- covid_separate %&gt;% \n  unite(\n    col = 'location', \n    c('location', 'continent'),\n    sep = '_')\ncovid_unite\n\n# A tibble: 330,156 × 4\n   location         date       new_cases new_deaths\n   &lt;chr&gt;            &lt;date&gt;         &lt;dbl&gt;      &lt;dbl&gt;\n 1 Afghanistan_Asia 2020-01-03         0          0\n 2 Afghanistan_Asia 2020-01-04         0          0\n 3 Afghanistan_Asia 2020-01-05         0          0\n 4 Afghanistan_Asia 2020-01-06         0          0\n 5 Afghanistan_Asia 2020-01-07         0          0\n 6 Afghanistan_Asia 2020-01-08         0          0\n 7 Afghanistan_Asia 2020-01-09         0          0\n 8 Afghanistan_Asia 2020-01-10         0          0\n 9 Afghanistan_Asia 2020-01-11         0          0\n10 Afghanistan_Asia 2020-01-12         0          0\n# ℹ 330,146 more rows\n\n\nAnd finally the nest function. The nest function is usually used in conjunction with group_by in order to create a new format of data frame in which every cell is now a list rather than a single observation and, thus, might store any kind of object – data frames, lists, models, plots and so forth.\nThe example below shows how to create a nested data frame for the covid data grouped by country. Note that each cell on column data is now a data frame itself corresponding to the data for each country.\n\ncovid_eu_nest &lt;- covid_data %&gt;% \n  filter(continent == 'Europe') %&gt;% \n  group_by(location) %&gt;% \n  nest()\ncovid_eu_nest\n\n# A tibble: 55 × 2\n# Groups:   location [55]\n   location               data                 \n   &lt;chr&gt;                  &lt;list&gt;               \n 1 Albania                &lt;tibble [1,371 × 66]&gt;\n 2 Andorra                &lt;tibble [1,371 × 66]&gt;\n 3 Austria                &lt;tibble [1,371 × 66]&gt;\n 4 Belarus                &lt;tibble [1,371 × 66]&gt;\n 5 Belgium                &lt;tibble [1,371 × 66]&gt;\n 6 Bosnia and Herzegovina &lt;tibble [1,371 × 66]&gt;\n 7 Bulgaria               &lt;tibble [1,378 × 66]&gt;\n 8 Croatia                &lt;tibble [1,377 × 66]&gt;\n 9 Cyprus                 &lt;tibble [1,371 × 66]&gt;\n10 Czechia                &lt;tibble [1,378 × 66]&gt;\n# ℹ 45 more rows\n\n\nI find it most useful when we need to perform several tasks over whole data sets. For example, if we had to transform the raw data, create plots and fit models for each country in the covid data set. The base R solution in this case would be to create list of lists, which can be very confusing sometimes. We’ll come back to nested data frames when we talk about functional programming with the purrr package.\n\n\n1.3.3 Text manipulation\nThe need to handle text data has grown substantially in parallel with the popularity of Machine Learning models and, more specifically, Natural Language Processing (NLP). For these more advanced applications, the challenge is to standardize a large set of (messy) texts in order to extract features which can then feed the models and generate predictions.\nNevertheless, knowing the basics of text manipulation is critical for everyday tasks. It includes subsetting parts of a word, detecting if specific patterns are present, replacing a sequence of characters by something else and so forth. The functions from the stringr package do a terrific job in simplifying all these operations and go far beyond.\nSimilarly to what we did in the previous sections, we’ll focus on the most widely used functions. Let’s start with str_detect, which is conveniently used in conjunction with dplyr::filter since it returns TRUE if the specific pattern is found in the string and FALSE otherwise.\nFor example, let’s say we want to analyze the Covid data only for North and South Americas. We’ve learned how to do so using dplyr::filter.\n\ncovid_americas &lt;- covid_data %&gt;% \n  filter(continent %in% c('North America', 'South America')) \n\nThis one is not cumbersome. But let’s pretend there were, say, 50 continents on Earth with 25 having ‘America’ in their names. Would it still make sense to write all these 25 names in a vector? Absolutely not. Since all of them share a common pattern, we can easily employ str_detect to do the trick.\n\ncovid_americas &lt;- covid_data %&gt;% \n  filter(stringr::str_detect(continent, 'America'))\n\nNote that we could have left aside several characters using only, say, ‘Am’ or ‘Ame’. This would have worked fine if there was no other continent with this sequence of characters. Of course, this parsimony makes more sense for lengthy words or long sentences. For short words I recommend you to write the full word in order to prevent any undesirable output. ßIn addition, we can also provide multiple patterns to str_detect by separating them with a |.\n\ncovid_americas &lt;- covid_data %&gt;% \n  filter(stringr::str_detect(continent, 'South|North'))\n\nFinally, we may use the negate = TRUE argument if we’re interested in the opposite of the pattern we provide – pretty much like ! in conditions. It’s specially useful when we want to keep most categories but one (or a few). For example, suppose that now we want to analyze every continent except for Asia. Instead of writing them all, we could simply do the following:\n\ncovid_exAsia &lt;- covid_data %&gt;% \n  filter(str_detect(continent, 'Asia', negate = TRUE))\n\nAnother recurrent task when we’re dealing with strings is to remove a part of it. It’s generally required in order to establish a standard within categories so we can perform further operations. The complexity of this task varies depending on the form of this undesired part.\nThe simplest case is when we have to remove a sequence of characters which is fixed both in length and in position. For example, suppose we have a data frame covid_numCont in which the observations in the continent column starts with a random number from 0 to 9 – think of it as a typing error from the source, since the same reasoning applies if those random numbers were present only in a few observations.\n\n\n# A tibble: 10 × 4\n   continent       location                        date       new_cases\n   &lt;chr&gt;           &lt;chr&gt;                           &lt;date&gt;         &lt;dbl&gt;\n 1 4.Africa        Congo                           2023-05-28         0\n 2 8.Africa        Madagascar                      2023-07-20         0\n 3 9.North America Bonaire Sint Eustatius and Saba 2023-04-02        38\n 4 2.Europe        Vatican                         2022-07-06         0\n 5 2.Africa        Togo                            2022-01-14       328\n 6 6.Europe        Estonia                         2021-08-24         0\n 7 2.Europe        Romania                         2022-10-05      1419\n 8 1.Africa        Kenya                           2023-05-08         0\n 9 9.Europe        Norway                          2020-03-08        19\n10 7.South America French Guiana                   2021-01-11         0\n\n\nTo solve this is solely a matter of subsetting the string from position three onwards using str_sub. The end argument defaults to the last character, so we don’t need to make it explicit.\n\ncovid_numCont %&gt;% \n  mutate(continent = str_sub(continent, start = 3))\n\n# A tibble: 330,156 × 4\n   continent location    date       new_cases\n   &lt;chr&gt;     &lt;chr&gt;       &lt;date&gt;         &lt;dbl&gt;\n 1 Asia      Afghanistan 2020-01-03         0\n 2 Asia      Afghanistan 2020-01-04         0\n 3 Asia      Afghanistan 2020-01-05         0\n 4 Asia      Afghanistan 2020-01-06         0\n 5 Asia      Afghanistan 2020-01-07         0\n 6 Asia      Afghanistan 2020-01-08         0\n 7 Asia      Afghanistan 2020-01-09         0\n 8 Asia      Afghanistan 2020-01-10         0\n 9 Asia      Afghanistan 2020-01-11         0\n10 Asia      Afghanistan 2020-01-12         0\n# ℹ 330,146 more rows\n\n\nNice. But what if the random numbers ranged from 0 to 10 as in the data frame below?\n\n\n# A tibble: 10 × 4\n   continent       location      date       new_cases\n   &lt;chr&gt;           &lt;chr&gt;         &lt;date&gt;         &lt;dbl&gt;\n 1 2.North America Costa Rica    2021-04-19      1060\n 2 8.Africa        Benin         2023-09-02         0\n 3 8.Europe        Spain         2021-01-20         0\n 4 3.Oceania       Cook Islands  2020-07-09         0\n 5 5.Oceania       Niue          2021-12-07         0\n 6 3.Europe        Moldova       2021-09-19      1031\n 7 9.Asia          Philippines   2021-04-28      6862\n 8 3.Africa        Guinea-Bissau 2022-05-26         0\n 9 10.Europe       Slovakia      2022-03-19      9386\n10 2.Africa        Lesotho       2022-11-21         0\n\n\nWith an extra digit, we could no longer resort to the previous solution. I don’t intend to dedicate an exclusive section to regular expressions, but simple examples will eventually show up throughout the book. In this case, we could use a simple regular expression inside str_remove to get rid of everything before and up to the ., .*\\\\..\n\ncovid_numCont %&gt;% \n  mutate(continent = str_remove(continent, \".*\\\\.\"))\n\n# A tibble: 330,156 × 4\n   continent location    date       new_cases\n   &lt;chr&gt;     &lt;chr&gt;       &lt;date&gt;         &lt;dbl&gt;\n 1 Asia      Afghanistan 2020-01-03         0\n 2 Asia      Afghanistan 2020-01-04         0\n 3 Asia      Afghanistan 2020-01-05         0\n 4 Asia      Afghanistan 2020-01-06         0\n 5 Asia      Afghanistan 2020-01-07         0\n 6 Asia      Afghanistan 2020-01-08         0\n 7 Asia      Afghanistan 2020-01-09         0\n 8 Asia      Afghanistan 2020-01-10         0\n 9 Asia      Afghanistan 2020-01-11         0\n10 Asia      Afghanistan 2020-01-12         0\n# ℹ 330,146 more rows\n\n\nTyping errors may arise under different forms along the character column. These cases usually require a more thorough evaluation and, more often than not, the solution is to manually replace the wrong words by the correct ones. For instance, in the data frame below (named covid_typo) we can find two different typos for North America: there’s a missing h in rows 1 and 5; whereas there’s an extra h in rows 3 and 8.\n\n\n# A tibble: 56,221 × 4\n   continent      location date       new_cases\n   &lt;chr&gt;          &lt;chr&gt;    &lt;date&gt;         &lt;dbl&gt;\n 1 Nort America   Anguilla 2020-01-03         0\n 2 North America  Anguilla 2020-01-04         0\n 3 Northh America Anguilla 2020-01-05         0\n 4 North America  Anguilla 2020-01-06         0\n 5 Nort America   Anguilla 2020-01-07         0\n 6 North America  Anguilla 2020-01-08         0\n 7 North America  Anguilla 2020-01-09         0\n 8 Northh America Anguilla 2020-01-10         0\n 9 North America  Anguilla 2020-01-11         0\n10 North America  Anguilla 2020-01-12         0\n# ℹ 56,211 more rows\n\n\nSince the data frame contains a huge number of observations, it may be present in other positions as well. We can use str_replace to fix it for the whole column.\n\ncovid_typo %&gt;% \n  mutate(\n    continent = str_replace(continent, 'Nort America', 'North America'),\n    continent = str_replace(continent, 'Northh America', 'North America')\n  )\n\nOr we can simply pass a vector with all the replacements to the str_replace_all function.\n\ncovid_typo %&gt;% \n  mutate(\n    continent = str_replace_all(\n      continent, \n      c('Nort America'   = 'North America',\n        'Northh America' = 'North America')\n    )\n  )\n\nThere’s one last kind of typo we can’t help but mention: whitespace. Whitespaces are particularly troublesome when they’re misplaced in the start/end of string or repeated inside it. Because they’re very easy to go unnoticed the stringr package contains two functions to cope with them: str_trim and str_squish. The former removes whitespaces from start/end of the string, whereas the later removes them from inside of a string.\nThe data frame below (named covid_ws) uses the same example as above, but now with a whitespace in the end of observations 1 and 5; and a repeated whitespace inside the observations 3 and 8. Note that tibble automatically adds quotation marks around the strings to highlight the extra whitespaces. This is awesome!\n\n\n# A tibble: 56,221 × 4\n   continent        location date       new_cases\n   &lt;chr&gt;            &lt;chr&gt;    &lt;date&gt;         &lt;dbl&gt;\n 1 \"North America \" Anguilla 2020-01-03         0\n 2 \"North America\"  Anguilla 2020-01-04         0\n 3 \"North  America\" Anguilla 2020-01-05         0\n 4 \"North America\"  Anguilla 2020-01-06         0\n 5 \"North America \" Anguilla 2020-01-07         0\n 6 \"North America\"  Anguilla 2020-01-08         0\n 7 \"North America\"  Anguilla 2020-01-09         0\n 8 \"North  America\" Anguilla 2020-01-10         0\n 9 \"North America\"  Anguilla 2020-01-11         0\n10 \"North America\"  Anguilla 2020-01-12         0\n# ℹ 56,211 more rows\n\n\nWe can easily get rid of the whitespaces using those functions. It’s advisable to use them as a preprocessing step whenever we’re working with character columns that should not have these extra whitespaces.\n\ncovid_ws %&gt;% \n  mutate(\n    continent = continent %&gt;% \n      str_trim() %&gt;% \n      str_squish()\n  )\n\nTo finish up, let’s use the tools we’ve just learned to solve the problem we’ve left over from the previous subsection. To recap, we wanted to separate the column location into country and continent. The issue was that with no separator character between the two names, we should resort to any kind of complicated regular expression to do the trick.\n\n\n# A tibble: 330,156 × 4\n   location        date       new_cases new_deaths\n   &lt;chr&gt;           &lt;date&gt;         &lt;dbl&gt;      &lt;dbl&gt;\n 1 AfghanistanAsia 2020-01-03         0          0\n 2 AfghanistanAsia 2020-01-04         0          0\n 3 AfghanistanAsia 2020-01-05         0          0\n 4 AfghanistanAsia 2020-01-06         0          0\n 5 AfghanistanAsia 2020-01-07         0          0\n 6 AfghanistanAsia 2020-01-08         0          0\n 7 AfghanistanAsia 2020-01-09         0          0\n 8 AfghanistanAsia 2020-01-10         0          0\n 9 AfghanistanAsia 2020-01-11         0          0\n10 AfghanistanAsia 2020-01-12         0          0\n# ℹ 330,146 more rows\n\n\nAs I said earlier, data science is all about finding workarounds. Of course we’re often interested in general approaches, but sometimes we have to settle for a lower level solution which gets the job done. For example, in this case we could waste a long time figuring out the best possible solution whereas a simpler one is at hand.\nThis simpler solution consists of employing the str_extract function to extract the name of the continents in conjunction with str_remove to remove them from the original column. Remember that multiple patterns should be provided as a single string with these patterns separated by |.\n\ncontinents &lt;- c(\n  'Asia',\n  'Europe',\n  'Africa', \n  'South America',\n  'North America',\n  'Oceania'\n) %&gt;% \n  paste0(collapse = '|')\ncovid_loc_cont2 %&gt;% \n  mutate(\n    continent = str_extract(\n      location,\n      continents\n    ),\n    location = str_remove(\n      location,\n      continents\n    )\n  )\n\n# A tibble: 330,156 × 5\n   location    date       new_cases new_deaths continent\n   &lt;chr&gt;       &lt;date&gt;         &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;    \n 1 Afghanistan 2020-01-03         0          0 Asia     \n 2 Afghanistan 2020-01-04         0          0 Asia     \n 3 Afghanistan 2020-01-05         0          0 Asia     \n 4 Afghanistan 2020-01-06         0          0 Asia     \n 5 Afghanistan 2020-01-07         0          0 Asia     \n 6 Afghanistan 2020-01-08         0          0 Asia     \n 7 Afghanistan 2020-01-09         0          0 Asia     \n 8 Afghanistan 2020-01-10         0          0 Asia     \n 9 Afghanistan 2020-01-11         0          0 Asia     \n10 Afghanistan 2020-01-12         0          0 Asia     \n# ℹ 330,146 more rows\n\n\nJob done! Next we turn to dates, which is a special form of string. Handling them properly is essential to analyze time series data.\n\n\n1.3.4 Date manipulation\nHaving knowledge on date manipulation is crucial to perform a lot of tasks when we’re dealing with time series data. Date objects are very convenient since they allow us to extract features that can be used for many purposes. The subject is so vast that the lubridate package was created exclusively to handle date objects.\nThe first step when we’re working with dates is to convert the string to a date object. In order to get rid of ambiguity issues, lubridate contains a set of predefined functions that take into account the ordering of year, month and day in the string.\nFor instance, if we have a date in the standard YYYY-MM-DD format we can use the ymd function. Note that it works regardless of how these terms are separated: it might be like 2021-12-01, 2022/12/01 or even 20221201. Also, months names (full or abbreviated forms) are allowed – 2021-December-01 or 2021-Dec-01. The same logic applies to the whole family of related functions: mdy, dmy, ydm, dym, my, ym and so on.\n\nlibrary(lubridate)\nymd('2022/12/01')\n\n[1] \"2022-12-01\"\n\nmdy('december, 1, 2022')\n\n[1] \"2022-12-01\"\n\ndmy('01122022')\n\n[1] \"2022-12-01\"\n\nmy('Dec-2021')\n\n[1] \"2021-12-01\"\n\n\nIn case the string format doesn’t match any of the predefined patterns, we can use lubridate::as_date function and declare the unusual format using specific operators: %Y for year; %m for month; and %d for day. There are two other useful ones: %b for months names (full or abbreviated) and %y for two-digits year.\nNow, let’s see how to extract features from date objects and how to use them to perform common operations. Take the following data set (named brl_usd) which provides daily values of the Brazilian Real (BRL) versus the US Dollar from January 2010 to December 2021 where the column date is in the DD/MM/YYYY format.\n\n\n# A tibble: 3,014 × 2\n   date        brl\n   &lt;chr&gt;     &lt;dbl&gt;\n 1 4/1/2010   1.72\n 2 5/1/2010   1.72\n 3 6/1/2010   1.73\n 4 7/1/2010   1.74\n 5 8/1/2010   1.74\n 6 11/1/2010  1.73\n 7 12/1/2010  1.74\n 8 13/1/2010  1.74\n 9 14/1/2010  1.76\n10 15/1/2010  1.77\n# ℹ 3,004 more rows\n\n\nNote that the column date is in standard string (or character) format. We’ll first convert it to the appropriate date format using the functions we’ve just learned.\n\nbrl_usd_aux &lt;- brl_usd %&gt;% \n  mutate(date = dmy(date))\nbrl_usd_aux\n\n# A tibble: 3,014 × 2\n   date         brl\n   &lt;date&gt;     &lt;dbl&gt;\n 1 2010-01-04  1.72\n 2 2010-01-05  1.72\n 3 2010-01-06  1.73\n 4 2010-01-07  1.74\n 5 2010-01-08  1.74\n 6 2010-01-11  1.73\n 7 2010-01-12  1.74\n 8 2010-01-13  1.74\n 9 2010-01-14  1.76\n10 2010-01-15  1.77\n# ℹ 3,004 more rows\n\n\nNow, suppose we want to obtain BRL monthly average. We have two ways to do this, one that is more logical and one that is more compact (see later when we talk about rounding dates). In the logical way, all we have to do is to create the columns we need to perform a grouping operation: year and month.\n\nbrl_usd_monthly &lt;- brl_usd_aux %&gt;% \n  mutate(\n    year  = year(date),\n    month = month(date)\n  ) %&gt;% \n  group_by(year, month) %&gt;% \n  summarise(brl_monthly = mean(brl))\nbrl_usd_monthly\n\n# A tibble: 144 × 3\n# Groups:   year [12]\n    year month brl_monthly\n   &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;\n 1  2010     1        1.78\n 2  2010     2        1.84\n 3  2010     3        1.79\n 4  2010     4        1.76\n 5  2010     5        1.81\n 6  2010     6        1.81\n 7  2010     7        1.77\n 8  2010     8        1.76\n 9  2010     9        1.72\n10  2010    10        1.68\n# ℹ 134 more rows\n\n\nYou might be wondering how to recover the date format YYYY-MM-DD. We can do it by simply using the make_date function. This function creates a standard date object from user-provided year, month and day. Since we’ve aggregated daily into monthly, we have two common choices for the day parameter: we either set it to 1 (the default) or we set it to the last day of the month using days_in_month.\n\nbrl_usd_monthly %&gt;% \n  mutate(\n    date = make_date(\n      year  = year, \n      month = month, \n      day   = 1),\n    date2 = make_date(\n      year = year, \n      month = month,\n      day   = days_in_month(date)\n    )\n  )\n\n# A tibble: 144 × 5\n# Groups:   year [12]\n    year month brl_monthly date       date2     \n   &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt; &lt;date&gt;     &lt;date&gt;    \n 1  2010     1        1.78 2010-01-01 2010-01-31\n 2  2010     2        1.84 2010-02-01 2010-02-28\n 3  2010     3        1.79 2010-03-01 2010-03-31\n 4  2010     4        1.76 2010-04-01 2010-04-30\n 5  2010     5        1.81 2010-05-01 2010-05-31\n 6  2010     6        1.81 2010-06-01 2010-06-30\n 7  2010     7        1.77 2010-07-01 2010-07-31\n 8  2010     8        1.76 2010-08-01 2010-08-31\n 9  2010     9        1.72 2010-09-01 2010-09-30\n10  2010    10        1.68 2010-10-01 2010-10-31\n# ℹ 134 more rows\n\n\nNote that creating a column with the number of days in each month may be itself particularly useful. For instance, when we have only the monthly totals of a variable and we need to compute daily averages. And, of course, it works fine with February since it takes the year into account.\nThe same procedure applies if we want to get quarterly means. It’s just a matter of creating a column with quarters. Be aware, however, that the quarter function has a parameter named with_year that when set to TRUE eliminates the need to create a separate column for the year.\n\nbrl_usd_quarterly &lt;- brl_usd_aux %&gt;% \n  mutate(quarter = quarter(date, with_year = TRUE)) %&gt;% \n  group_by(quarter) %&gt;% \n  summarise(brl_quarterly = mean(brl))\nbrl_usd_quarterly\n\n# A tibble: 48 × 2\n   quarter brl_quarterly\n     &lt;dbl&gt;         &lt;dbl&gt;\n 1   2010.          1.80\n 2   2010.          1.79\n 3   2010.          1.75\n 4   2010.          1.70\n 5   2011.          1.67\n 6   2011.          1.60\n 7   2011.          1.64\n 8   2011.          1.80\n 9   2012.          1.77\n10   2012.          1.96\n# ℹ 38 more rows\n\n\nSpecial attention must be taken when we want to work with weeks, because lubridate has two different functions to extract this feature: week and isoweek. The former returns the number of complete seven day periods that have occurred between the date and January 1st, while the latter returns the number of the week (from Monday to Sunday) the date belongs to.\nTo get a better sense of the difference between them, suppose we provide the date ‘2022-01-01’. week will return 1, since it’s in the first seven days period after January 1st. On the other hand, isoweek will return 52 because it’s Saturday and thus part of the last week of the previous year. It’ll only return 1 as of ‘2022-01-03’ since it belongs to a new week.\n\nweek('2022-01-01')\n\n[1] 1\n\nisoweek('2022-01-01')\n\n[1] 52\n\nisoweek('2022-01-03')\n\n[1] 1\n\n\nTherefore, if we want to compute weekly averages and by weekly we mean a period of seven days in a row then we should pick isoweek instead of week. Another feature we can extract from dates is the week day. In addition to temporal aggregation, it’s often used to filter or label data we want to plot later.\n\nbrl_usd_aux %&gt;% \n  mutate(wday = wday(date, label = TRUE))\n\n# A tibble: 3,014 × 3\n   date         brl wday \n   &lt;date&gt;     &lt;dbl&gt; &lt;ord&gt;\n 1 2010-01-04  1.72 Mon  \n 2 2010-01-05  1.72 Tue  \n 3 2010-01-06  1.73 Wed  \n 4 2010-01-07  1.74 Thu  \n 5 2010-01-08  1.74 Fri  \n 6 2010-01-11  1.73 Mon  \n 7 2010-01-12  1.74 Tue  \n 8 2010-01-13  1.74 Wed  \n 9 2010-01-14  1.76 Thu  \n10 2010-01-15  1.77 Fri  \n# ℹ 3,004 more rows\n\n\nNow we turn to operations with date objects. The lubridate package contains two special operators %m+% and %m-% that work nicely with date objects to perform, respectively, addition and subtraction.\n\nd1 &lt;- ymd('2020-02-29')\nd1 %m+% years(2)\n\n[1] \"2022-02-28\"\n\nd1 %m-% months(3)\n\n[1] \"2019-11-29\"\n\nd1 %m+% days(1)\n\n[1] \"2020-03-01\"\n\n\nIn addition, there’s also the lesser known add_with_rollback function which we can use to have more control over the output. For example, when adding one month to 2022-01-31 we might want either 2022-02-28 (the last day of the next month) or 2022-03-01 (a period of one month). To get the latter, we set the roll_to_first parameter to TRUE.\n\nd2 &lt;- ymd('2022-01-31')\nadd_with_rollback(d2, months(1), roll_to_first = TRUE)\n\n[1] \"2022-03-01\"\n\nadd_with_rollback(d2, months(1), roll_to_first = FALSE)\n\n[1] \"2022-02-28\"\n\n\nI couldn’t help but mention two useful functions used to round dates: floor_date and ceiling_date. They take a date object and rounds it down or up, respectively, to the nearest boundary of the specified time unit.\n\nd3 &lt;- ymd('2021-03-13')\nfloor_date(d3,   unit = 'month')\n\n[1] \"2021-03-01\"\n\nceiling_date(d3, unit = 'month')\n\n[1] \"2021-04-01\"\n\nfloor_date(d3,   unit = 'quarter')\n\n[1] \"2021-01-01\"\n\nceiling_date(d3, unit = 'quarter')\n\n[1] \"2021-04-01\"\n\n\nThese functions can be helpful in several ways and you’ll find their benefits as you go through your own tasks. For example, if we want to match (or re-write) dates that refer to the same period but are written differently (monthly date in data set A is ‘2021-12-01’ and in data set B is ‘2021-12-31’).\nI use them very often as a simpler way to perform temporal aggregation. Remember that earlier in this section we computed monthly averages by creating two grouping columns, year and month. The logic was simply to treat every day in the same year/month as belonging to the same group. We can easily accomplish the same result by rounding dates down, with the great benefit of preserving the date column.\n\nbrl_usd_monthly2 &lt;- brl_usd_aux %&gt;% \n  mutate(date = floor_date(date, unit = 'month')) %&gt;% \n  group_by(date) %&gt;% \n  summarise(brl_monthly = mean(brl))\nbrl_usd_monthly2\n\n# A tibble: 144 × 2\n   date       brl_monthly\n   &lt;date&gt;           &lt;dbl&gt;\n 1 2010-01-01        1.78\n 2 2010-02-01        1.84\n 3 2010-03-01        1.79\n 4 2010-04-01        1.76\n 5 2010-05-01        1.81\n 6 2010-06-01        1.81\n 7 2010-07-01        1.77\n 8 2010-08-01        1.76\n 9 2010-09-01        1.72\n10 2010-10-01        1.68\n# ℹ 134 more rows\n\n\nTo finish up, let’s have a quick look at a family of functions: wday, mday, qday and yday. They’re used to get the number of days that have occurred within that time period, respectively.\n\nwday('2021-06-10') # 5th day of that week\n\n[1] 5\n\nqday('2021-06-10') # 71th day of the 2nd quarter of 2021\n\n[1] 71\n\nyday('2021-06-10') # 161th day of 2021\n\n[1] 161\n\n\nIt’s very useful, for example, when you need to compare observations from the same period in different years or create high frequency seasonal variables."
  },
  {
    "objectID": "ds_tidyverse.html#looping",
    "href": "ds_tidyverse.html#looping",
    "title": "1  Introduction",
    "section": "1.4 Looping",
    "text": "1.4 Looping\nIteration is an indispensable tool in programming and every language has its own structure. Essentially, loops are used to repeat an action over a set of values, thus preventing us from the annoying and risky copy-and-paste thing. Whenever we have any kind of redundancy, there’s a good reason to use loops.\nThe purrr package provides many interesting tools for working with functions and vectors. For our purposes, we’ll stick with the family of map functions. The logic will always be the same: applying a function – existing or user-defined – over a vector (or list) of arguments.\nLet’s start with a very simple example. Suppose we have three numeric vectors and we want to compute their means. Instead of calling mean over each vector separately, we can put them into a list and then use the map function in conjunction with the existing mean function.\n\nv1 &lt;- c(1,4,7,8)\nv2 &lt;- c(3,5,9,0) \nv3 &lt;- c(12,0,7,1)\nv_all &lt;- list(v1, v2, v3)\nmap(.x = v_all, .f = mean)\n\n[[1]]\n[1] 5\n\n[[2]]\n[1] 4.25\n\n[[3]]\n[1] 5\n\n\nNote that by default the output will be a list, but we can have other output formats using map_*: map_dbl will return the results as a vector, whereas map_dfc will return them in a (column) data frame. We only need to consider whether the output can be coerced to the desired class.\n\nmap_dbl(.x = v_all, .f = mean)\n\n[1] 5.00 4.25 5.00\n\nmap_dfc(.x = v_all, .f = mean)\n\n# A tibble: 1 × 3\n   ...1  ...2  ...3\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     5  4.25     5\n\n\nMost of the time I prefer to return the results as a list, because that makes it easier to apply further operations if needed. Data frames are usually a better choice for final results.\nNow, let’s introduce some degree of complexity to the exercise by providing our own function. For this, let’s use the example of importing data from an API we saw earlier in the Importing section. Suppose that in addition to CPI we also want to get the time series for GDP and Unemployment rate.\nRemember (or scroll up if necessary) that we created an object called api_series_id with the ID of the CPI time series and that was the only specific parameter – everything else would be the same for any other series we wanted. Therefore, our first task here is to create a function whose only parameter is the series ID. Then, we create a vector (or list) with the desired IDs and – guess what? – use them inside the map function.\nI’ll create the get_series function using the same content we already saw up there, but leaving the api_series_id as a parameter (series_id). Note that I’ll keep some objects with their original names – starting with cpi_ – just to avoid confusion. This has no practical effect, though.\n\nget_series &lt;- function(series_id){\n  api_url       &lt;- 'https://api.stlouisfed.org/fred/series/observations?'\n  api_key       &lt;- api_fred_key\n  api_series_id &lt;- series_id\n  obs_start     &lt;- '2010-01-01'\n  api_filetype  &lt;- 'json'\n  api_request   &lt;- glue::glue('{api_url}series_id={api_series_id}&observation_start={obs_start}&api_key={api_key}&file_type={api_filetype}')\n  cpi_request   &lt;- httr::GET(url = api_request)\n  cpi_content   &lt;- httr::content(cpi_request, as = 'text')\n  cpi_list      &lt;- jsonlite::fromJSON(cpi_content, flatten = FALSE)\n  cpi_tbl       &lt;- cpi_list[['observations']] %&gt;% tibble::as_tibble()\n  return(cpi_tbl)\n} \n\nWe can test our function using the ID for CPI we used before.\n\nget_series(series_id = 'CPALTT01USM657N')\n\n# A tibble: 164 × 4\n   realtime_start realtime_end date       value              \n   &lt;chr&gt;          &lt;chr&gt;        &lt;chr&gt;      &lt;chr&gt;              \n 1 2023-10-12     2023-10-12   2010-01-01 0.34174735701485   \n 2 2023-10-12     2023-10-12   2010-02-01 0.024920738207648  \n 3 2023-10-12     2023-10-12   2010-03-01 0.410628353657108  \n 4 2023-10-12     2023-10-12   2010-04-01 0.173688491069743  \n 5 2023-10-12     2023-10-12   2010-05-01 0.0775197354237721 \n 6 2023-10-12     2023-10-12   2010-06-01 -0.0976267084673888\n 7 2023-10-12     2023-10-12   2010-07-01 0.0211043057371509 \n 8 2023-10-12     2023-10-12   2010-08-01 0.138066427840807  \n 9 2023-10-12     2023-10-12   2010-09-01 0.0581736230715569 \n10 2023-10-12     2023-10-12   2010-10-01 0.124519888847696  \n# ℹ 154 more rows\n\n\nGreat, it’s working fine! The next step is to create a vector (or list) with the IDs for each series. Assigning names to the vector (list) elements is a good idea since these names are carried forward helping us to identify the elements in the output list.\n\nid_list &lt;- list(\n  'CPI'   = 'CPALTT01USM657N',\n  'GDP'   = 'GDPC1',\n  'Unemp' = 'UNRATE'\n)\nfred_data &lt;- purrr::map(.x = id_list, .f = get_series)\nfred_data\n\n$CPI\n# A tibble: 164 × 4\n   realtime_start realtime_end date       value              \n   &lt;chr&gt;          &lt;chr&gt;        &lt;chr&gt;      &lt;chr&gt;              \n 1 2023-10-12     2023-10-12   2010-01-01 0.34174735701485   \n 2 2023-10-12     2023-10-12   2010-02-01 0.024920738207648  \n 3 2023-10-12     2023-10-12   2010-03-01 0.410628353657108  \n 4 2023-10-12     2023-10-12   2010-04-01 0.173688491069743  \n 5 2023-10-12     2023-10-12   2010-05-01 0.0775197354237721 \n 6 2023-10-12     2023-10-12   2010-06-01 -0.0976267084673888\n 7 2023-10-12     2023-10-12   2010-07-01 0.0211043057371509 \n 8 2023-10-12     2023-10-12   2010-08-01 0.138066427840807  \n 9 2023-10-12     2023-10-12   2010-09-01 0.0581736230715569 \n10 2023-10-12     2023-10-12   2010-10-01 0.124519888847696  \n# ℹ 154 more rows\n\n$GDP\n# A tibble: 54 × 4\n   realtime_start realtime_end date       value    \n   &lt;chr&gt;          &lt;chr&gt;        &lt;chr&gt;      &lt;chr&gt;    \n 1 2023-10-12     2023-10-12   2010-01-01 16582.71 \n 2 2023-10-12     2023-10-12   2010-04-01 16743.162\n 3 2023-10-12     2023-10-12   2010-07-01 16872.266\n 4 2023-10-12     2023-10-12   2010-10-01 16960.864\n 5 2023-10-12     2023-10-12   2011-01-01 16920.632\n 6 2023-10-12     2023-10-12   2011-04-01 17035.114\n 7 2023-10-12     2023-10-12   2011-07-01 17031.313\n 8 2023-10-12     2023-10-12   2011-10-01 17222.583\n 9 2023-10-12     2023-10-12   2012-01-01 17367.01 \n10 2023-10-12     2023-10-12   2012-04-01 17444.525\n# ℹ 44 more rows\n\n$Unemp\n# A tibble: 165 × 4\n   realtime_start realtime_end date       value\n   &lt;chr&gt;          &lt;chr&gt;        &lt;chr&gt;      &lt;chr&gt;\n 1 2023-10-12     2023-10-12   2010-01-01 9.8  \n 2 2023-10-12     2023-10-12   2010-02-01 9.8  \n 3 2023-10-12     2023-10-12   2010-03-01 9.9  \n 4 2023-10-12     2023-10-12   2010-04-01 9.9  \n 5 2023-10-12     2023-10-12   2010-05-01 9.6  \n 6 2023-10-12     2023-10-12   2010-06-01 9.4  \n 7 2023-10-12     2023-10-12   2010-07-01 9.4  \n 8 2023-10-12     2023-10-12   2010-08-01 9.5  \n 9 2023-10-12     2023-10-12   2010-09-01 9.5  \n10 2023-10-12     2023-10-12   2010-10-01 9.4  \n# ℹ 155 more rows\n\n\nWe could make our function more general by allowing more parameters to vary. For example, we could have a different time span for each series (the obs_start object). The procedure would be almost the same: we would add an extra parameter in the function, create two vectors (lists) with the parameters values and use map2 instead of map.\n\nget_series2par &lt;- function(series_id, series_start){\n  api_url       &lt;- 'https://api.stlouisfed.org/fred/series/observations?'\n  api_key       &lt;- api_fred_key\n  api_series_id &lt;- series_id\n  obs_start     &lt;- series_start\n  api_filetype  &lt;- 'json'\n  api_request   &lt;- glue::glue('{api_url}series_id={api_series_id}&observation_start={obs_start}&api_key={api_key}&file_type={api_filetype}')\n  cpi_request   &lt;- httr::GET(url = api_request)\n  cpi_content   &lt;- httr::content(cpi_request, as = 'text')\n  cpi_list      &lt;- jsonlite::fromJSON(cpi_content, flatten = FALSE)\n  cpi_tbl       &lt;- cpi_list[['observations']] %&gt;% tibble::as_tibble()\n  return(cpi_tbl)\n} \n\ntime_list &lt;- list(\n  'CPI'   = '2010-01-01',\n  'GDP'   = '2012-04-01',\n  'Unemp' = '2014-06-01'\n  )  \nfred_data2 &lt;- purrr::map2(.x = id_list, .y = time_list, .f = get_series2par)\n\nWe must be careful with the example above because it may give the impression that map2 takes into account the variables names in the process. It doesn’t! It actually considers how elements are sorted in each list. So if we changed the CPI to the second position in time_list, then we would end up with GDP data with CPI time span.\nA safer, thus preferable, alternative is to use names rather than positions as indexes. Since we can use list[['element_name']] to access an element in a list, we may reduce our problem to a single dimension by looping over variables names which are the same in both lists.\n\nvars_fred &lt;- c('CPI', 'GDP', 'Unemp')\nfred_data2_names &lt;- map(\n  .x = vars_fred, \n  .f = function(x) get_series2par(\n    series_id = id_list[[x]],\n    series_start = time_list[[x]]\n  )\n) %&gt;% \n  magrittr::set_names(vars_fred)\n\nNotice how powerful this solution is: you can generalize it to as many parameters as you need without taking the risk of using the parameter value of one series into another, with the additional work being only to make explicit the parameters of the function in .f.\nTo finish this topic, let’s get back to the end of subsection on data layout. There, we had an overview of nested data frames and I stated that I find them most useful when we need to perform several tasks over whole data sets. Also, remember that nesting a data frame is, roughly speaking, converting every cell from a single observation into a list.\nLet’s print again the nested data frame we created up there, named covid_eu_nest.\n\n\n# A tibble: 55 × 2\n# Groups:   location [55]\n   location               data                 \n   &lt;chr&gt;                  &lt;list&gt;               \n 1 Albania                &lt;tibble [1,371 × 66]&gt;\n 2 Andorra                &lt;tibble [1,371 × 66]&gt;\n 3 Austria                &lt;tibble [1,371 × 66]&gt;\n 4 Belarus                &lt;tibble [1,371 × 66]&gt;\n 5 Belgium                &lt;tibble [1,371 × 66]&gt;\n 6 Bosnia and Herzegovina &lt;tibble [1,371 × 66]&gt;\n 7 Bulgaria               &lt;tibble [1,378 × 66]&gt;\n 8 Croatia                &lt;tibble [1,377 × 66]&gt;\n 9 Cyprus                 &lt;tibble [1,371 × 66]&gt;\n10 Czechia                &lt;tibble [1,378 × 66]&gt;\n# ℹ 45 more rows\n\n\nWe can use map to perform computations for every country at once. Moreover, we can create new columns to store the results, thus gathering all the information in the same object.\n\ncovid_eu_nest %&gt;% \n  mutate(\n    max_total_cases = map_dbl(\n      .x = data, \n      .f = function(x){\n        x %&gt;% \n          pull(total_cases) %&gt;% \n          max(na.rm = TRUE)}\n    ),\n    min_total_cases = map_dbl(\n      .x = data, \n      .f = function(x){\n        x %&gt;% \n          pull(total_cases) %&gt;% \n          min(na.rm = TRUE)}\n    )\n  )\n\n# A tibble: 55 × 4\n# Groups:   location [55]\n   location               data                  max_total_cases min_total_cases\n   &lt;chr&gt;                  &lt;list&gt;                          &lt;dbl&gt;           &lt;dbl&gt;\n 1 Albania                &lt;tibble [1,371 × 66]&gt;          334090               2\n 2 Andorra                &lt;tibble [1,371 × 66]&gt;           48015               1\n 3 Austria                &lt;tibble [1,371 × 66]&gt;         6081287               1\n 4 Belarus                &lt;tibble [1,371 × 66]&gt;          994037               1\n 5 Belgium                &lt;tibble [1,371 × 66]&gt;         4817196               1\n 6 Bosnia and Herzegovina &lt;tibble [1,371 × 66]&gt;          403155               2\n 7 Bulgaria               &lt;tibble [1,378 × 66]&gt;         1302188               4\n 8 Croatia                &lt;tibble [1,377 × 66]&gt;         1275337               1\n 9 Cyprus                 &lt;tibble [1,371 × 66]&gt;          660854               2\n10 Czechia                &lt;tibble [1,378 × 66]&gt;         4651538               3\n# ℹ 45 more rows\n\n\nAnd since each cell is a list rather than an observation, we’re by no means restricted to numeric elements. We could use the same strategy to create a column with plots, for example. Plotting is, by the way, the subject of the next section."
  },
  {
    "objectID": "ds_tidyverse.html#graphics",
    "href": "ds_tidyverse.html#graphics",
    "title": "1  Introduction",
    "section": "1.5 Graphics",
    "text": "1.5 Graphics\nWe’re almost there in our goal to review the basics of Tidyverse. Succinctly describing all the previous sections was challenging, but I believe you had enough information to start carrying out data analyzes on your own. When it comes to making graphics, extrapolating from the basics is somewhat harder because there are infinite possibilities for customization.\nIn fact, the grammar used in ggplot2 package is broad and far from simple at first glance. But as you practice, it becomes increasingly graspable. Still, I encourage you to dive into the amazing Wickham, Navarro, and Pedersen (2019) since getting fluent on this subject is indispensable.\nLet’s think about the process of making a graphic as a set of layers arranged sequentially. The first layer is the data you want to plot. We’ll use the CPI data frame from the Importing section. The second layer is the ggplot function. So far, there’s nothing to visualize. All these two layers do is to set the stage for what’s coming next.\n\ncpi_tbl %&gt;% \n  ggplot()\n\nThe third layer we must provide is the geom, which is the geometry used to represent our data. This is the most important layer as it contains a set of parameters that effectively creates a visualization of the data. For time series data we generally use a line graph, so geom_line is the appropriate geometry. In addition, we need to provide the values for x and y axes. In this case, they’re the dates and CPI values, respectively.\nBefore proceeding, we need to make sure that the values for x and y are in the appropriate class. We can use the glimpse function from dplyr package to check this out.\n\ncpi_tbl %&gt;%\n  glimpse()\n\nRows: 156\nColumns: 4\n$ realtime_start &lt;chr&gt; \"2023-10-12\", \"2023-10-12\", \"2023-10-12\", \"2023-10-12\",…\n$ realtime_end   &lt;chr&gt; \"2023-10-12\", \"2023-10-12\", \"2023-10-12\", \"2023-10-12\",…\n$ date           &lt;chr&gt; \"2010-01-01\", \"2010-02-01\", \"2010-03-01\", \"2010-04-01\",…\n$ value          &lt;chr&gt; \"0.34174735701485\", \"0.024920738207648\", \"0.41062835365…\n\n\nWe see that both date and value (the CPI value) are in character format and so we need to convert them to date and numeric format, respectively. One last thing we need to be aware of is that ggplot uses + instead of %&gt;% as an operator to chain actions. Therefore, every layer we add is preceded by a +. The code below produces the simplest plot from CPI data.\n\ncpi_tbl %&gt;%\n  mutate(\n    date  = ymd(date),\n    value = as.numeric(value)\n  ) %&gt;% \n  ggplot() + \n  geom_line(aes(x = date, y = value))\n\n\n\n\nWhat should we care about this plot? Well, every plot must contain a meaningful title and labels for both x and y axes. We can set them using the labs layer. Additionally, we might want to have shorter intervals for dates (x-axis) and values (y-axis). There are also specific layers to control axes settings.\n\ncpi_tbl %&gt;%\n  mutate(\n    date = ymd(date),\n    value = as.numeric(value)\n  ) %&gt;% \n  ggplot() + \n  geom_line(aes(x = date, y = value)) +\n  labs(\n    title = 'US CPI showing an upward trend as of 2021',\n    x     = 'Date',\n    y     = 'US Monthly CPI (%)'\n  ) +\n  scale_x_date(date_breaks = '1 year', date_labels = '%Y') +\n  scale_y_continuous(breaks = seq(-1, 1.5, 0.25), limits = c(-1,1.5))\n\n\n\n\nFinally, we could be interested in adding some feature to the plot in order to highlight the upward trend in CPI as of 2021. Either the 12-month-accumulated CPI or the 3-months-accumulated seasonally-adjusted CPI would be common choices for this task and we’ll see examples of how to perform this kind of computation later. For now, just to keep things simple we’ll use the average CPI from 2010 to 2019 (the pre-COVID period) as a measure of ‘normal’ CPI.\n\ncpi_tbl %&gt;%\n  mutate(\n    date = ymd(date),\n    value = as.numeric(value)\n  ) %&gt;% \n  mutate(value_avg = mean(value[which(year(date) %in% 2010:2019)])) %&gt;% \n  ggplot() + \n  geom_line(aes(x = date, y = value)) +\n  geom_line(aes(x = date, y = value_avg), color = 'red', lwd = 1) +\n  labs(\n    title    = 'US CPI showing an upward trend as of 2021',\n    subtitle = 'Red line is the 2010-2019 average',\n    x        = 'Date',\n    y        = 'US Monthly CPI (%)'\n  ) +\n  scale_x_date(date_breaks = '1 year', date_labels = '%Y') +\n  scale_y_continuous(breaks = seq(-1, 1, 0.25))\n\n\n\n\nSince the parameter x = date appears in both geom_line layers, we could make the code more compact by moving this parameter to ggplot() function. All the parameters inside ggplot() are carried forward to every subsequent layer. It saves a lot of effort when we’re adding geom_*’s to a graphic with fixed features. In this graphic, x will always be the date variable regardless of any layer I insert on it.\n\ncpi_tbl %&gt;%\n  mutate(\n    date = ymd(date),\n    value = as.numeric(value)\n  ) %&gt;% \n  mutate(value_avg = mean(value[which(year(date) %in% 2010:2019)])) %&gt;% \n  ggplot(aes(x = date)) + \n  geom_line(aes(y = value)) +\n  geom_line(aes(y = value_avg), color = 'red', lwd = 1) +\n  labs(\n    title    = 'US CPI showing an upward trend as of 2021',\n    subtitle = 'Red line is the 2010-2019 average',\n    x        = 'Date',\n    y        = 'US Monthly CPI (%)'\n  ) +\n  scale_x_date(date_breaks = '1 year', date_labels = '%Y') +\n  scale_y_continuous(breaks = seq(-1, 1.5, 0.25), limits = c(-1, 1.5))\n\nYou may be wondering why x and y parameters went inside the aes function whereas color went outside it. First of all, the parameters which set the graphic axes always go inside aes. Other parameters have a special role whether they appear either inside or outside aes. According to the documentation, aesthetic mappings describe how variables in the data are mapped to visual properties (aesthetics) of geoms. It’s much easier to understand through an example.\nIn the graphic above, we used color outside aes to define which color we wanted for the graphic line. However, if we were to use color as an attribute to highlight different groups from the data, color should go inside aes. In addition, we should pass a variable onto it instead of color names. This variable might be discrete, in which case we would have one color per group; or it might be continuous, in which case we would have a color gradient according to its magnitude.\nIn the piece of code below we create a new variable named covid_period to separate the CPI data between pre-Covid and Covid periods and use it as a color attribute.\n\ncpi_tbl %&gt;%\n  mutate(\n    date  = ymd(date),\n    value = as.numeric(value),\n    covid = if_else(between(date, ymd('2020-03-01'), ymd('2022-12-01')), 'Yes', 'No')\n    ) %&gt;%\n  ggplot() + \n  geom_line(aes(x = date, y = value, color = covid), lwd = 1) +\n  labs(\n    title    = 'US CPI showing an upward trend as of 2021',\n    x        = 'Date',\n    y        = 'US Monthly CPI (%)'\n    ) +\n  scale_x_date(date_breaks = '1 year', date_labels = '%Y') +\n  scale_y_continuous(breaks = seq(-1, 1.5, 0.25), limits = c(-1, 1.5))\n\n\n\n\nNote that ggplot automatically assigns colors for the attributes and adds a legend. So we could be interested in customize both the attributes’ colors and legend position. Again, this might be achieved by adding two specific layers. The logic will always be the same: parameters inside aes turn a variable into an attribute which might then be customized by specific layers.\n\ncpi_tbl %&gt;%\n  mutate(\n    date  = ymd(date),\n    value = as.numeric(value),\n    covid = if_else(between(date, ymd('2020-03-01'), ymd('2022-12-01')), 'Yes', 'No')\n    ) %&gt;% \n  ggplot() + \n  geom_line(aes(x = date, y = value, color = covid), lwd = 1) +\n  labs(\n    title    = 'US CPI showing an upward trend as of 2021',\n    x        = 'Date',\n    y        = 'US Monthly CPI (%)') +\n  scale_x_date(date_breaks = '1 year', date_labels = '%Y') +\n  scale_y_continuous(breaks = seq(-1, 1.5, 0.25), limits = c(-1, 1.5)) +\n  scale_color_manual(values = c('darkgreen', 'orange')) +\n  theme(legend.position = 'top')\n\n\n\n\nAs a final example, we’ll see how to build a scatter plot and explore some additional features from ggplot. Scatter plots are usually employed to highlight the relationship between two variables in a given point in time. For this task, let’s use again the Covid data set. We’ll first filter the data to a given point in time, say 2021-07-31. Next, we’ll plot people_fully_vaccinated_per_hundred and new_deaths_smoothed_per_million on the x and y axes, respectively.3\n\ncovid_data %&gt;% \n  filter(date == '2021-07-31') %&gt;% \n  ggplot() +\n  geom_point(\n    aes(\n      x = people_fully_vaccinated_per_hundred,\n      y = new_deaths_smoothed_per_million\n    )\n  )\n\nWarning: Removed 160 rows containing missing values (`geom_point()`).\n\n\n\n\n\nWe saw earlier how to manually set the color to each group. It’s worth emphasizing that we’re by no means bounded to using colors names as ggplot also works with other tools such as HEX codes and the awesome ColorBrewer. ColorBrewer is an amazing source for color schemes carefully designed to improve visual communication. You can check this out at https://colorbrewer2.org/.\nWe’ll stick with ColorBrewer since it makes it effortless to assign a different color to each of the six continents. Notice that after choosing your desired pattern and palette in the website, you get some meta data (type and scheme) that will be used here.\nIn addition, we’ll use theme_light() to make our plot cleaner. Basically, a theme is a set of pre-defined features for a graph. You can check other built-in themes, including theme_dark(), theme_minimal(), theme_classic() and so on.\n\ncovid_data %&gt;% \n  filter(\n    date == '2021-07-31',\n    !is.na(continent)\n  ) %&gt;% \n  ggplot() +\n  geom_point(\n    aes(\n      x = people_fully_vaccinated_per_hundred,\n      y = new_deaths_smoothed_per_million,\n      color = continent\n    ),\n    size = 3\n  ) +\n  labs(title = 'New deaths in late July were above 5 per million people in countries where \\n vaccination falls below 30% of population') +\n  scale_color_brewer(type = 'qual', palette = 2) +\n  theme_light()\n\nWarning: Removed 160 rows containing missing values (`geom_point()`).\n\n\n\n\n\nTo finish, instead of showing all the continents in a single plot we could separate them in multiple panels using facet_wrap. This feature is very useful when the range of values across the groups differ greatly in magnitude.\n\ncovid_data %&gt;% \n  filter(\n    date == '2021-07-31',\n    !is.na(continent)\n  ) %&gt;% \n  ggplot() +\n  geom_point(\n    aes(\n      x = people_fully_vaccinated_per_hundred,\n      y = new_deaths_smoothed_per_million\n    ),\n    size = 3\n  ) +\n  labs(\n    title = 'New deaths in late July were above 5 per million people in countries where \\n vaccination falls below 30% of population'\n    ) +\n  theme_light() +\n  facet_wrap(~ continent)\n\n\n\n\nSince we have a separate panel for each continent, it’s no longer necessary to assign a different color for each of them. As you go through the next Chapters, you’ll see more plotting features and tips on how to improve data visualization."
  },
  {
    "objectID": "ds_tidyverse.html#tables",
    "href": "ds_tidyverse.html#tables",
    "title": "1  Introduction",
    "section": "1.6 Tables",
    "text": "1.6 Tables\nGraphs are the right tool to show historical data or relationship between variables. Nonetheless, we often need to send reports with detailed data from the latest release of a certain variable. Tables are much better suited for this type of task and the gt package is awesome when it comes to customizing tables. It’s worth taking a look at the package’s website to get a better sense of the syntax.\nIn the following example, we will see how to organize the data properly to convert it into a nice-looking table. The data frame us_gdp below contains data on the contributions to percent change in real gross domestic product (annual rates) for the US economy released by the Bureau of Economic Analysis of the U.S Department of Commerce (BEA).\n\n\n# A tibble: 15 × 5\n   Quarter   Goods Services Structures `Motor vehicle output`\n   &lt;yearqtr&gt; &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;                  &lt;dbl&gt;\n 1 2019 Q2    0.1      1.79       0.83                   0.14\n 2 2019 Q3    1.59     1.45       0.57                   0.16\n 3 2019 Q4    0.52     1.49      -0.22                  -0.43\n 4 2020 Q1   -0.7     -4.75       0.83                  -0.98\n 5 2020 Q2   -7.99   -19.0       -2.88                  -3.86\n 6 2020 Q3   19.6     14.2        1.55                   5.92\n 7 2020 Q4    0.52     1.97       1.42                  -0.64\n 8 2021 Q1    2.5      3.46       0.35                   0.12\n 9 2021 Q2    3.01     4.54      -0.55                  -0.22\n10 2021 Q3   -0.02     3.16      -0.49                  -1.42\n11 2021 Q4    5.52     1.91      -0.47                   0.98\n12 2022 Q1   -1.76     0.57      -0.44                  -0.1 \n13 2022 Q2   -0.96     1.83      -1.44                  -0.1 \n14 2022 Q3    2.28     2.3       -1.34                   0.07\n15 2022 Q4    1.83     1.44      -0.7                    0.26\n\n\nThe data frame is in long format, which is not suitable for a table. So, we’ll start by pivoting our data frame so as the dates goes across the columns and the variables (the breakdown of the contributions to the US GDP) goes across the rows. Then, we’ll transform it in a gt object.\n\nlibrary(gt)\nus_gdp_gt &lt;- us_gdp %&gt;%\n  pivot_longer(-Quarter, names_to = 'var', values_to = 'value') %&gt;% \n  filter(Quarter &gt;= '2021 Q1') %&gt;%\n  pivot_wider(names_from = Quarter, values_from = value) %&gt;% \n  gt(\n    rowname_col = \"var\"\n    ) \nus_gdp_gt\n\n\n\n\n\n  \n    \n    \n      \n      2021 Q1\n      2021 Q2\n      2021 Q3\n      2021 Q4\n      2022 Q1\n      2022 Q2\n      2022 Q3\n      2022 Q4\n    \n  \n  \n    Goods\n2.50\n3.01\n-0.02\n5.52\n-1.76\n-0.96\n2.28\n1.83\n    Services\n3.46\n4.54\n3.16\n1.91\n0.57\n1.83\n2.30\n1.44\n    Structures\n0.35\n-0.55\n-0.49\n-0.47\n-0.44\n-1.44\n-1.34\n-0.70\n    Motor vehicle output\n0.12\n-0.22\n-1.42\n0.98\n-0.10\n-0.10\n0.07\n0.26\n  \n  \n  \n\n\n\n\nOkay, we have a rough draft of a table. Now, we can add some labs such as title, subtitle and source note.\n\nus_gdp_gt &lt;- us_gdp_gt %&gt;% \n  tab_header(\n    title = \"Contributions to percent change in real gross domestic product\",\n    subtitle = \"Annual rates\"\n  ) %&gt;% \n  tab_source_note(\n    source_note = \"Source: U.S Bureau of Economic Analysis (BEA)\"\n  )\nus_gdp_gt\n\n\n\n\n\n  \n    \n      Contributions to percent change in real gross domestic product\n    \n    \n      Annual rates\n    \n    \n      \n      2021 Q1\n      2021 Q2\n      2021 Q3\n      2021 Q4\n      2022 Q1\n      2022 Q2\n      2022 Q3\n      2022 Q4\n    \n  \n  \n    Goods\n2.50\n3.01\n-0.02\n5.52\n-1.76\n-0.96\n2.28\n1.83\n    Services\n3.46\n4.54\n3.16\n1.91\n0.57\n1.83\n2.30\n1.44\n    Structures\n0.35\n-0.55\n-0.49\n-0.47\n-0.44\n-1.44\n-1.34\n-0.70\n    Motor vehicle output\n0.12\n-0.22\n-1.42\n0.98\n-0.10\n-0.10\n0.07\n0.26\n  \n  \n    \n      Source: U.S Bureau of Economic Analysis (BEA)\n    \n  \n  \n\n\n\n\nThe tab_spanner feature allows us to organize columns under a common label. We can use one of its variation (tab_spanner_delim) to organize the quarters under the respective year in order to make our table more compact and nice-looking.\n\nus_gdp_gt &lt;- us_gdp_gt %&gt;% \n  tab_spanner_delim(\n    columns = -var,\n    delim = ' ' \n  )\nus_gdp_gt\n\n\n\n\n\n  \n    \n      Contributions to percent change in real gross domestic product\n    \n    \n      Annual rates\n    \n    \n      \n      \n        2021\n      \n      \n        2022\n      \n    \n    \n      Q1\n      Q2\n      Q3\n      Q4\n      Q1\n      Q2\n      Q3\n      Q4\n    \n  \n  \n    Goods\n2.50\n3.01\n-0.02\n5.52\n-1.76\n-0.96\n2.28\n1.83\n    Services\n3.46\n4.54\n3.16\n1.91\n0.57\n1.83\n2.30\n1.44\n    Structures\n0.35\n-0.55\n-0.49\n-0.47\n-0.44\n-1.44\n-1.34\n-0.70\n    Motor vehicle output\n0.12\n-0.22\n-1.42\n0.98\n-0.10\n-0.10\n0.07\n0.26\n  \n  \n    \n      Source: U.S Bureau of Economic Analysis (BEA)\n    \n  \n  \n\n\n\n\nWe could keep adding more features (a color scheme, for instance), but we can be content for now. We have a well-formatted and very functional table that can be used to convey a nice summary of the US GDP release.\n\n\n\n\nGrolemund, G., and H. Wickham. 2017. R for Data Science, 1st Edition. O’Reilly Media.\n\n\nWickham, H., Danielle Navarro, and Thomas Lin Pedersen. 2019. Ggplot2: Elegant Graphics for Data Analysis, 3rd Edition. Springer."
  },
  {
    "objectID": "ds_tidyverse.html#footnotes",
    "href": "ds_tidyverse.html#footnotes",
    "title": "1  Introduction",
    "section": "",
    "text": "Actually, these information are contained in the covid_data object, but let’s pretend it’s not.↩︎\nAnother case for the by argument is when we have more than one column with the same name across the data sets, but we don’t want to use them all as keys.↩︎\nBe aware that we’re not trying to establish a causal relationship, nor to find a threshold for vaccination above which people are safe. We must be careful when interpreting this graphic, since omitted variables such as age and previous exposure to the virus may influence the outcome.↩︎"
  },
  {
    "objectID": "ds_googlemob.html#importing-data",
    "href": "ds_googlemob.html#importing-data",
    "title": "2  Scaling-up tasks",
    "section": "2.1 Importing data",
    "text": "2.1 Importing data\nGoogle offers two ways to download its mobility data. You can either get a unique .csv file with all the available countries or you can get a .zip file with a separate .csv file for each country. We will stick with the latter for a simple reason: we are probably not interested in analyzing every single country and as data sets grow larger, it’s much more efficient to import only the data we are interested in.\nWe begin by downloading the .zip file to the data folder.\n\ndownload.file(\n  url      = 'https://www.gstatic.com/covid19/mobility/Region_Mobility_Report_CSVs.zip',\n  destfile = 'data/Region_Mobility_Report_CSVs.zip'\n  )\n\nNow suppose we want to analyze a small group of countries. In this case, a better approach is to only import the .csv corresponding to these countries and then bind them together. After a brief inspection of the .zip file, we can see a clear pattern in file names: year_country-code_Region_Mobility_Report.csv. For example, the file containing data for Brazil in 2021 is: 2021_BR_Region_Mobility_Report.csv.\nSo, our first task is to produce a vector with the desired file names. This vector will be used later to extract the corresponding .csv from the .zip file. We can call Sys.Date() to recover the current year and use it as the end point of our sequence of years. Whether you are reading this book in 2022 or in 2030 and Google still releases mobility data with file names following the same pattern, then you can safely use the solution below.\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(glue)\ncountries_codes  &lt;- c('BR', 'US', 'DE', 'ZA', 'SG', 'AU')\nyears            &lt;- seq(from = 2020, \n                        to = year(Sys.Date()), \n                        by = 1\n                        )\ngoogle_filenames &lt;- \n  cross2(years, countries_codes) %&gt;% \n  map_chr(\n    .f = ~ .x %&gt;% \n      glue_collapse(sep = '_') %&gt;%\n      glue('_Region_Mobility_Report.csv')\n  )\n\nThe cross2 function creates all the combinations between codes and years we need to replicate the first part of the file names. The final part is static, so it’s just a matter of pasting this piece into each element of the vector. We can check a piece of the result below:\n\n\n [1] \"2020_BR_Region_Mobility_Report.csv\" \"2021_BR_Region_Mobility_Report.csv\"\n [3] \"2022_BR_Region_Mobility_Report.csv\" \"2023_BR_Region_Mobility_Report.csv\"\n [5] \"2020_US_Region_Mobility_Report.csv\" \"2021_US_Region_Mobility_Report.csv\"\n [7] \"2022_US_Region_Mobility_Report.csv\" \"2023_US_Region_Mobility_Report.csv\"\n [9] \"2020_DE_Region_Mobility_Report.csv\" \"2021_DE_Region_Mobility_Report.csv\"\n\n\nTo finish up, we now resort to the map function to extract each file name from the .zip file. At this stage, I would like to draw your attention to something very important. Remember that we are ultimately interested in binding together all the .csv files in google_filenames. Since Google’s files contain columns for both country and date, we could safely use map_dfr to automatically stack the data – it would save us a few lines of code.\nHowever, it could be that these files did not contain identifying columns – this information being only in the file name. It happens more often than not in real-life applications. So, if we naively stacked these files we would never be able to distinguish which country or date period each piece of the resulting data frame refers to.\nAnother issue that could cause us problems is if any of the elements in google_filenames did not exist. For example, if there wasn’t data for Germany in 2021. The map function would throw an error and interrupt the task, regardless all the other files were present. To prevent this problem, we can use the possibly function from purrr package, which replaces the error (or any side effect) with another output. In this case, we can replace the error by a NULL element in the list.\nTherefore, the efficient strategy in this case is:\n\nUse the map function to import each file as an element in a list using the possibly function to avoid any error to stop the whole process.\nAssign meaningful names for each element of that list with set_names from magrittr package.\nCall the ldply function from plyr package to stack them.\n\nThe ldply function is very convenient here because it carries the names of the elements in the list into the resulting data frame as a new column. In addition, it also has several other useful features such as applying a generic function to each element of the list before stacking it.\nIn this example, the file names contain both the country code and year for each data set. Thankfully, we have a very simple pattern and we can extract the relevant information from the first seven characters of each element in our vector google_filenames. More complicated patterns would require the use of regular expressions.\n\nmobility_data &lt;- \n  map(\n    .x = google_filenames, \n    .f = possibly(\n      ~ read_csv(unz('data/Region_Mobility_Report_CSVs.zip', .x)),\n      otherwise = NULL\n    )\n  ) %&gt;% \n  set_names(str_sub(\n    google_filenames, start = 1, end = 7)\n  ) %&gt;% \n  plyr::ldply(.id = 'year_country')"
  },
  {
    "objectID": "ds_googlemob.html#preparing-the-data",
    "href": "ds_googlemob.html#preparing-the-data",
    "title": "2  Scaling-up tasks",
    "section": "2.2 Preparing the data",
    "text": "2.2 Preparing the data\nNow the we have successfully imported the data for the selected countries, it is time to produce useful content. Let’s begin with a closer look on the structure of the data set. We can remove the year_country column since it was only for pedagogical purposes and we won’t need it.\n\nmobility_data %&gt;%\n  dplyr::glimpse()\n\nRows: 4,772,663\nColumns: 15\n$ country_region_code                                &lt;chr&gt; \"BR\", \"BR\", \"BR\", \"…\n$ country_region                                     &lt;chr&gt; \"Brazil\", \"Brazil\",…\n$ sub_region_1                                       &lt;chr&gt; NA, NA, NA, NA, NA,…\n$ sub_region_2                                       &lt;chr&gt; NA, NA, NA, NA, NA,…\n$ metro_area                                         &lt;lgl&gt; NA, NA, NA, NA, NA,…\n$ iso_3166_2_code                                    &lt;chr&gt; NA, NA, NA, NA, NA,…\n$ census_fips_code                                   &lt;chr&gt; NA, NA, NA, NA, NA,…\n$ place_id                                           &lt;chr&gt; \"ChIJzyjM68dZnAARYz…\n$ date                                               &lt;date&gt; 2020-02-15, 2020-0…\n$ retail_and_recreation_percent_change_from_baseline &lt;dbl&gt; 5, 2, -2, -3, -1, 1…\n$ grocery_and_pharmacy_percent_change_from_baseline  &lt;dbl&gt; 4, 3, 0, -1, -2, 7,…\n$ parks_percent_change_from_baseline                 &lt;dbl&gt; -5, -13, -12, -11, …\n$ transit_stations_percent_change_from_baseline      &lt;dbl&gt; 8, 3, 9, 9, 8, 11, …\n$ workplaces_percent_change_from_baseline            &lt;dbl&gt; 6, 0, 19, 15, 14, 1…\n$ residential_percent_change_from_baseline           &lt;dbl&gt; 0, 1, -1, -1, -1, -…\n\n\nWe can see our data set has about 4.7 million rows and 15 columns. The most relevant information are stored in the columns ending with percent_change_from_baseline. These are precisely the measures of mobility for categorized places. The other columns of interest are those containing region and, of course, the column date. I recommend you to take some time exploring the data set. You will notice that the sub_region_* columns refer to regional breakdowns such as states and municipalities. They are NA for aggregate levels.\nSuppose our ultimate goal is to have a plot with the average mobility across all the categories for each country at national level. We know in advance that it’s very likely that a strong seasonal pattern is present. For example, mobility in workplaces should be higher on weekdays and lower on weekends. The opposite should be true for parks. Creating a 7-days rolling mean of the original time series should solve the problem.\nFinally, we need to invert the residential mobility since a higher (lower) residential mobility means a lower (higher) mobility elsewhere. So, if we are to aggregate all the mobility categories into one single measure (the average) they must point to the same direction.\nHence, our task is to produce a data frame with only the relevant variables. This involves, for each country, the following sequence of actions:\n\nFilter the national data.\nInvert the direction of the residential mobility (change the sign).\nTransform each mobility category column into a 7-days moving average.\nCreate a column with the average mobility of categories.\nRemove the irrelevant variables.\n\nThis should not be quite a challenge and we can accomplish it with a few lines of code using the right features from dplyr package. I consider Items 3 and 4 the most important because we are tempted to offer a cumbersome solution that can be easily avoided with the proper tools. But before jumping to the best approach, let’s figure out how an inefficient approach might look like for Item 3. Using the roll_meanr function from the RcppRoll package to compute 7-days rolling means, our first solution could be something like this:\n\nlibrary(RcppRoll)\n\nmutate_try1 &lt;- mobility_data %&gt;% \n  group_by(country_region) %&gt;% \n  arrange(date) %&gt;% \n  mutate(\n    retail_and_recreation_percent_change_from_baseline = roll_meanr(retail_and_recreation_percent_change_from_baseline, 7, na.rm = TRUE),\n    grocery_and_pharmacy_percent_change_from_baseline  = roll_meanr(grocery_and_pharmacy_percent_change_from_baseline, 7, na.rm = TRUE),\n    parks_percent_change_from_baseline                 = roll_meanr(parks_percent_change_from_baseline, 7, na.rm = TRUE),\n    transit_stations_percent_change_from_baseline      = roll_meanr(transit_stations_percent_change_from_baseline, 7, na.rm = TRUE),\n    workplaces_percent_change_from_baseline            = roll_meanr(workplaces_percent_change_from_baseline, 7, na.rm = TRUE),\n    residential_percent_change_from_baseline           = roll_meanr(residential_percent_change_from_baseline, 7, na.rm = TRUE)\n  ) %&gt;% \n  ungroup()\n\nThis solution is terrible, nevertheless I come across it very often. Fortunately, we already have a way to avoid it. The first step towards a better solution would be to use the across function from dplyr package to replace the variable name in the right-hand side by .x. This will eliminate part of the redundancies.\n\nlibrary(RcppRoll)\n\nmutate_try2 &lt;- mobility_data %&gt;% \n  group_by(country_region) %&gt;% \n  arrange(date) %&gt;% \n  mutate(\n    across(retail_and_recreation_percent_change_from_baseline, ~ roll_meanr(.x, 7)),\n    across(grocery_and_pharmacy_percent_change_from_baseline, ~ roll_meanr(.x, 7)),\n    across(parks_percent_change_from_baseline, ~ roll_meanr(.x, 7)),\n    across(transit_stations_percent_change_from_baseline, ~ roll_meanr(.x, 7)),\n    across(workplaces_percent_change_from_baseline, ~ roll_meanr(.x, 7)),\n    across(residential_percent_change_from_baseline, ~ roll_meanr(.x, 7))\n    ) %&gt;% \n  ungroup()\n\nOk, we’ve made some progress in cutting part of the repetitions but we can certainly do better. Note that the variables we are interested in show a clear pattern: they all end with percent_change_from_baseline or simply baseline. We can take advantage of this to further improve our solution using select helpers. These are expressions that can be used to refer to specific patterns. For instance, here we could use the select helper ends_with to create the 7-days rolling mean for all the variables ending with baseline.\nIn addition, we can also use the argument .names to assign a glue-style name to the new variables: {.col} gets the column name and {.fun} gets the name of the function. This is great to identify which function we applied to each variable. Here, we can use a ma7d suffix which stands for moving-average 7-days.\n\nlibrary(RcppRoll)\n\nmutate_topsolution &lt;- mobility_data %&gt;% \n  group_by(country_region) %&gt;% \n  arrange(date) %&gt;% \n  mutate(across(ends_with('baseline'), ~ roll_meanr(.x, na.rm = TRUE), .names = '{.col}_ma7d')) %&gt;% \n  ungroup()\n\nThe main lesson here is to avoid using variables names to compute the operations. Instead, whenever possible we must rely on the combination of across and select helpers. This avoids unnecessarily writing variables names so many times and therefore allows us to easily scale up the work.\nThe same reasoning applies to Item 4. Can you see how? Remember that Item 4 asks us to create a column with the average mobility across categories. Well, all these columns end with baseline. Therefore, we don’t need to rewrite all the variables names to get a new column with the mean – we can resort to select helpers. The only difference is that now we need an operation across the rows rather than across the columns.\nWe can accomplish it by using the rowwise function from dplyr package. Roughly speaking, this function turns every row of the data frame into a single group. Then you can perform your calculation on that group (the row). In addition, we have to replace the across function by the c_across function. The c_across is simply the equivalent of across when we’re using the rowwise mode. Remember to call ungroup to turn row-wise off and get back to the default column-wise mode when you don’t need operations across the rows anymore.\nBelow the full solution for Items 1 to 5.\n\nlibrary(RcppRoll)\n\nmobility_final &lt;- mobility_data %&gt;% \n  filter(is.na(sub_region_1)) %&gt;%\n  mutate(across(starts_with('residential'), ~ -1*.x)) %&gt;%\n  group_by(country_region) %&gt;%\n  arrange(date) %&gt;% \n  mutate(across(ends_with('baseline'), ~ roll_meanr(.x, 7, na.rm = TRUE), .names = '{.col}_ma7d')) %&gt;%\n  ungroup() %&gt;% \n  rowwise() %&gt;% \n  mutate(avg_mobility = mean(c_across(ends_with('ma7d')), na.rm = TRUE)) %&gt;%\n  ungroup() %&gt;% \n  select(date, country_region, ends_with('ma7d'), avg_mobility)"
  },
  {
    "objectID": "ds_googlemob.html#plot-information",
    "href": "ds_googlemob.html#plot-information",
    "title": "2  Scaling-up tasks",
    "section": "2.3 Plot information",
    "text": "2.3 Plot information\nWe have mobility data for six countries and we should now decide how to plot them. Time series are usually better presented as lines, but there are some choices to be made. The most important one is whether we should display the countries data in a single or in a multiple panel. It depends on the purpose of the plot. If we are interested in observing the differences among countries in the same time period, then a single graph is a natural choice. On the other hand, if our goal is to observe in more detail the evolution in each country, then a separate plot is more convenient.\nLet’s stick with the latter in this example using the facet_wrap feature. In this case we’re segmenting our plot by country, but we’re not constrained to segment it by only one variable. Besides, we could use the argument scales = 'free_y' to make the scales of each graph more adjusted to the limits of the data. It’s not desirable here as we want to make visual comparisons between countries straightforward.\n\nmobility_final %&gt;% \n  ggplot(aes(x = date, y = avg_mobility)) +\n  geom_line() +\n  facet_wrap(~ country_region) +\n  labs(title = 'Average mobility in selected countries - % change from baseline',\n       subtitle = '7-days moving average',\n       x = '',\n       y = 'Average mobility (% change from baseline)',\n       caption = 'Data source: Google') +\n  geom_hline(yintercept = 0, linetype = 2) +\n  theme_light()"
  },
  {
    "objectID": "ds_googlemob.html#from-code-to-function",
    "href": "ds_googlemob.html#from-code-to-function",
    "title": "2  Scaling-up tasks",
    "section": "2.4 From code to function",
    "text": "2.4 From code to function\nWe have written the full code to import, prepare and visualize the data. Perhaps this analysis will become part of our routine or that of a stakeholder. And if that happens, it’s desirable that we can look at other countries as well. So a good practice in this case is to wrap our code as a function.\nCreating a function is highly recommended whenever we have a repeated action on a different set of arguments. Here, we can think of three arguments that we would like to change eventually: the country, the time span and the window size of the rolling mean. Therefore our task is to gather all the code we have produced so far and to transform these three inputs into arguments of the function.\nNote, however, that converting code into a function raises some issues. For example, when writing the code we used a vector to import data for the selected countries. It’s not the most efficient approach, because each file has a significant size and thus the execution may be very slow. This is a truly concern when we’re writing a function, because functions are most of the time used to loop over a large set of arguments – many countries, for example. Hence, we would like to process this task in parallel rather than serially.\nSurely we can perform this parallel processing inside the function, but I always prefer to keep things simpler and more transparent. This means to write a function that plots only a single country and, if necessary, we can use map to get as many countries as we want – and eventually in parallel.2\nOther minor yet important issue is that in a function we have to use the arguments as inputs everywhere, not only in obvious places. For example, when preparing the data we included a ma7d to the column names to indicate they were transformed into 7-days rolling mean. This label was also used in many actions later – when computing the average mobility, in the plot subtitle, etc. Therefore, we need to ensure that this argument will be considered in those actions as well. To achieve this, we’ll use glue() function to create custom labels.\n\nplot_mobility &lt;- function(country_code, start_date, end_date, ma_window){\n  library(lubridate)\n  library(tidyverse)  \n  library(glue)\n  # Import data\n  countries_codes &lt;- country_code\n  years &lt;- seq(from = 2020,\n               to   = year(Sys.Date()), \n               by   = 1)\n  google_filenames &lt;- \n    cross2(years, countries_codes) %&gt;% \n    map_chr(\n      .f = ~ .x %&gt;% \n        glue_collapse(sep = '_') %&gt;%\n        glue('_Region_Mobility_Report.csv')\n    )\n  mobility_data &lt;- \n    map_dfr(\n      .x = google_filenames, \n      .f = possibly(\n        ~ readr::read_csv(unz('data/Region_Mobility_Report_CSVs.zip', .x)),\n        otherwise = NULL\n      )\n    )\n  # Prepare data\n  mobility_prep &lt;- mobility_data %&gt;% \n    filter(is.na(sub_region_1)) %&gt;%\n    mutate(across(starts_with('residential'), ~ -1*.x)) %&gt;%\n    group_by(country_region) %&gt;%\n    arrange(date) %&gt;% \n    mutate(across(ends_with('baseline'), ~ roll_meanr(.x, ma_window, na.rm = TRUE), .names = '{.col}_ma{ma_window}d')) %&gt;%\n    ungroup() %&gt;% \n    rowwise() %&gt;% \n    mutate(avg_mobility = mean(c_across(ends_with(glue('ma{ma_window}d'))), na.rm = TRUE)) %&gt;%\n    ungroup() %&gt;% \n    select(date, country_region, ends_with('baseline'), avg_mobility) \n  # Output: plot\n  mobility_prep %&gt;% \n    filter(between(date, ymd(start_date), ymd(end_date))) %&gt;% \n    ggplot(aes(x = date, y = avg_mobility)) +\n    geom_line() +\n    labs(\n      title = glue('Average mobility in {country_code} - % change from baseline'),\n      subtitle = glue('{ma_window}-days moving average'),\n      caption = 'Data source: Google'\n    ) +\n    geom_hline(yintercept = 0, linetype = 2) +\n    theme_light()\n}\n\nWe can now use the function plot_mobility to plot any country we want and with the desired time span and window for the rolling mean.\n\nplot_mobility('BR', '2020-03-01', '2022-10-15', 14)\n\n\n\n\nOr we can use map to build the plot for several countries and the special operators from patchwork package 3 to arrange them in a convenient way.\n\nlibrary(patchwork)\ncountries &lt;- c('BR', 'FR')\nmobility_countries &lt;- map(\n  .x = countries, \n  .f = plot_mobility, \n  '2020-03-01', \n  '2022-10-15',\n  14\n  ) %&gt;% \n  set_names(countries)\nmobility_countries[[1]] / mobility_countries[[2]]\n\n\n\n\nTo finish up, we must keep in mind that a function that return a plot is not very flexible and maybe we should consider the output to be the processed data instead of the plot – or both. This would enable us to customize the plot and to perform other analysis as well. It’s not efficient either to download and prepare the data every time the function is called. The best solution in this case would require the processed data to be stored in a data base, for instance."
  },
  {
    "objectID": "ds_googlemob.html#footnotes",
    "href": "ds_googlemob.html#footnotes",
    "title": "2  Scaling-up tasks",
    "section": "",
    "text": "As of 2022-10-15, Community Mobility Reports are no longer updated, although all historical data remains publicly available.↩︎\nThe furrr package makes it incredible simple to run the map function in parallel. See: https://furrr.futureverse.org/ for a comprehensive approach.↩︎\nSee https://ggplot2-book.org/arranging-plots.html to learn how to use the special operators provided by patchwork to arrange the plots.↩︎"
  },
  {
    "objectID": "ds_windowing.html#rolling-means",
    "href": "ds_windowing.html#rolling-means",
    "title": "3  Rolling, cumulative and lagged/leading values",
    "section": "3.1 Rolling means",
    "text": "3.1 Rolling means\nRolling operations are routinely employed as a means of smoothing volatile time series or to mitigate the presence of seasonal effects. Take the Google Mobility data for Brazil in 2021 we saw in the first Chapter. Remember that this data has a daily frequency and that mobility in workplaces is higher on weekdays. Therefore, a simple strategy to remove this seasonal pattern is to take the 7-days rolling mean.\nFor this, we can use the roll_mean function from the RcppRoll package. In addition to mean, the package provides functions to compute several other rolling functions – minimum/maximum, median, standard deviations, products and so on. Also, we can use the suffixes l(eft)/c(enter)/r(ight) instead of the align parameter inside the function call to align the window used in the calculations.\n\n\nShow the code\nlibrary(tidyverse)\ngmob_data_br &lt;- read_csv(\n  unz(\n    'data/Region_Mobility_Report_CSVs.zip', \"2021_BR_Region_Mobility_Report.csv\"\n  )\n) %&gt;% \n  filter(is.na(sub_region_1)) %&gt;% \n  select(date, mobility_workplaces = contains('workplaces'))\n\n\n\nlibrary(RcppRoll)\ngmob_data_br_7dma &lt;- gmob_data_br %&gt;%\n  arrange(date) %&gt;% \n  mutate(\n    mobility_workplaces_7dma = roll_meanr(\n      mobility_workplaces, \n      n = 7, \n      na.rm = TRUE\n      )\n    )\n\n\ngmob_data_br_7dma %&gt;% \n  ggplot(aes(x = date)) +\n  geom_line(aes(y = mobility_workplaces, color = 'Mobility in Workplaces'), lwd = 1) +\n  geom_line(aes(y = mobility_workplaces_7dma, color = 'Mobility in Workplaces - 7d MA'), lwd = 1) +\n  theme(legend.position = 'top') +\n  labs(\n    title = 'Brazil: Mobility in workplaces (% change from baseline)',\n    x = '',\n    y = '',\n    color = ''\n    )"
  },
  {
    "objectID": "ds_windowing.html#accumulated-in-n-periods",
    "href": "ds_windowing.html#accumulated-in-n-periods",
    "title": "3  Rolling, cumulative and lagged/leading values",
    "section": "3.2 Accumulated in n-periods",
    "text": "3.2 Accumulated in n-periods\nTaking the rolling mean to smooth out very volatile time series or to mitigate the seasonal pattern is a natural choice when we are interested in the level of the series. However, when dealing with ratios the most appropriate procedure is to compute the accumulated values in twelve months for monthly series or in four quarters for quarterly series. For instance, take the monthly US CPI data we saw on the first Chapter.\n\ncpi_12m &lt;- cpi_tbl %&gt;% \n  arrange(date) %&gt;% \n  mutate(\n    value_12m = (roll_prodr(1+value/100, n = 12)-1)*100\n    )\n\n\n\nShow the code\ncpi_12m %&gt;% \n  ggplot(aes(x = date)) +\n  geom_line(aes(y = value_12m), lwd = 1) +\n  theme(legend.position = 'top') +\n  labs(\n    title = 'US: CPI accumulated in 12-months (%)',\n    x = '',\n    y = '',\n    color = ''\n    )"
  },
  {
    "objectID": "ds_windowing.html#from-changes-to-level",
    "href": "ds_windowing.html#from-changes-to-level",
    "title": "3  Rolling, cumulative and lagged/leading values",
    "section": "3.3 From changes to level",
    "text": "3.3 From changes to level\nSometimes we are interested in looking at the series in terms of its level rather than its variation. This is particularly useful when we have reasons to believe that the data should lie within a given range or return to an expected path. In order to get the level of a series from its variations, all we need to do is accumulate its variations over time. Using the data on US CPI, we have:\n\ncpi_level &lt;- cpi_tbl %&gt;% \n  arrange(date) %&gt;% \n  mutate(\n    value_level = cumprod(1+value/100),\n    value_level = (value_level/first(value_level))*100\n    )\n\n\n\nShow the code\ncpi_level %&gt;% \n  ggplot(aes(x = date)) +\n  geom_line(aes(y = value_level), lwd = 1) +\n  theme(legend.position = 'top') +\n  scale_x_date(date_breaks = '1 year', date_labels = '%Y') +\n  labs(\n    title = 'US: CPI in level (Jan/2010 = 100)',\n    x = '',\n    y = '',\n    color = ''\n    )\n\n\n\n\n\nLooking at the series in level make it easier for the analyst to conjecture possible scenarios for inflation. For example, it could either remain constant by extrapolating the last value or progressively return to the pre-covid path."
  },
  {
    "objectID": "ds_windowing.html#lagged-and-leading-values",
    "href": "ds_windowing.html#lagged-and-leading-values",
    "title": "3  Rolling, cumulative and lagged/leading values",
    "section": "3.4 Lagged and leading values",
    "text": "3.4 Lagged and leading values\nLeads and Lags of a time series are generally used in regressions, but occasionally appear in graphs that seek to compare two or more series that have a non-contemporary relationship. Also, knowing how to refer to past or future values of a series can be useful for performing calculations – computing changes from a baseline, for example. The lead and lag functions from dplyr package make this task very easy.\n\nlibrary(tidyverse)\ncpi_lag_lead &lt;- cpi_tbl %&gt;% \n  mutate(\n    value_lag1  = lag(value, 1),\n    value_lag6  = lag(value, 6),\n    value_lead2 = lead(value, 2)\n  )"
  },
  {
    "objectID": "ds_seasonality.html",
    "href": "ds_seasonality.html",
    "title": "4  Seasonal adjustment",
    "section": "",
    "text": "Seasonality is commonly defined as the expected fluctuation in time series data that occurs within a regular frequency not exceeding a one-year-period. For instance, temperatures are higher in the summer months and lower in the winter months. These fluctuations often create difficulties for the correct identification of trends in data. For example, it doesn’t help to know that the average temperature increased in July compared to June. The relevant information is how much did the average temperature exceed the historical pattern for this period. Therefore, in most applications with time series it’s mandatory to remove the seasonal component before carrying out any further analysis.\nThere are some methods available for seasonal adjustment, X-13-ARIMA from the US Census Bureau being the most used. Roughly speaking, it makes use of a standard ARIMA model with external regressors accounting for outliers, permanent or transitory shifts, holidays and so on. The Seasonal Adjustment Q&A section of the Census website provides details on how the tool works, as well as useful information on the practice of seasonal adjustment. It’s worth giving it a read.\nIn the following sections we’ll see how to identify and remove the seasonal pattern from Brazilian Retail Sales data (PMC provided by IBGE, the Brazilian official bureau of statistics) using X-13-ARIMA.\n\n4.0.1 Spotting a seasonal pattern\nIt’s very common for time series to show a seasonal pattern strong enough that it’s possible to identify it simply by visual inspection. The data on Brazilian Retail Sales is an example of this, as we can clearly see the regularly spaced peaks throughout the sample.\n\n\n\n\n\nSometimes, however, the mix of trends and random noise may hinder our ability to spot the seasonal pattern. In these cases, we can resort to some tools. For example, the ggmonthplot function from the forecast package is a nice shortcut to build a plot where the data are grouped by period and so we can get a better sense of whether values are typically higher or lower for an specific period.\n\npmc_ts_nsa %&gt;%\n  ggmonthplot() +\n  labs(\n    title = 'Retail sales in Brazil - Volume Index (2014 = 100)',\n    x = '',\n    y = 'Index (2014 = 100)'\n  )\n\n\n\n\nAs the graph makes clear, we can expect values on December to be, on average, higher than on any other month. This is obviously related to year-end sales, as you might suspect.\n\n\n4.0.2 Removing the seasonal pattern\nX-13-ARIMA is available for R users through the seasonal package. In order to make it as simple as possible, the function seas can automatically select the model that fits the data best. Therefore, we can perform seasonal adjustment without any specific knowledge. Notice that the seas function will return the model selected for seasonal adjustment. We should next call the final function to get the seasonally adjusted values.\n\nlibrary(seasonal)\npmc_sa_autox13 &lt;- seas(pmc_ts_nsa)\npmc_sa_autox13 %&gt;% \n  final() %&gt;% \n  autoplot() +\n  autolayer(pmc_ts_nsa, series = 'Retail NSA') +\n labs(\n    title = 'Retail sales in Brazil - Volume Index (2014 = 100)',\n    subtitle = 'Seasonally-Adjusted',\n    x = '',\n    y = 'Index (2014 = 100)'\n  )\n\n\n\n\nIt did a good job of getting rid of those peaks in December – and possibly other undesirable hidden stuff. We can use the ggmonthplot function we’ve seem in the previous section to check that there’s no seasonality left in the data.\n\npmc_sa_autox13 %&gt;%\n  final() %&gt;% \n  ggmonthplot() +\n  labs(\n    title = 'Retail sales in Brazil - Volume Index (2014 = 100)',\n    subtitle = 'Seasonally-Adjusted',\n    x = '',\n    y = 'Index (2014 = 100)'\n  )\n\n\n\n\nIt looks pretty good! Remember that no seasonal treatment is perfect and the goal is always to have no apparent seasonality in the data. In this case, we could safely make meaningful comparisons between periods. To finish, it’s worth mentioning that we can assess relevant information about the selected model using standard methods for lm objects. For example, information on the estimated parameters are available through the summary function, while the checkresiduals function from the forecast package can be used to check the properties of the residuals (or directly perform any test based on model residuals using the residuals function).\n\npmc_sa_autox13 %&gt;% \n  summary()\npmc_sa_autox13 %&gt;% \n  forecast::checkresiduals()\n\n\n\n4.0.3 Moving to a custom specification\nSometimes we just can’t rely on the automatic model selection. Either because we would like to incorporate additional features not available in the function – special moving holidays is a common issue – or because we need to replicate the seasonal adjustment provided by the source or any other third-party.\nFor instance, IBGE releases its own seasonally-adjusted retail sales data. So if we were to analyze or forecast seasonally-adjusted data using IBGE releases as our target, we would have to invariably adopt its specification. Let’s first compare the automatic seasonal adjustment we computed in the previous section with the official seasonally-adjusted series provided by IBGE.\n\n\nCode\npmc_sa_ibge &lt;- \n  pmc_tidy %&gt;% \n  dplyr::filter(var == 'retail_sa') %&gt;% \n  dplyr::pull(value) %&gt;% \n  ts(start = c(2000,1), frequency = 12)\npmc_sa_autox13 %&gt;% \n  final() %&gt;% \n  autoplot() +\n  autolayer(pmc_sa_ibge, series = 'Retail Sales SA (official)') +\n labs(\n    title = 'Retail sales in Brazil - Volume Index (2014 = 100)',\n    subtitle = 'Seasonally-Adjusted',\n    x = '',\n    y = 'Index (2014 = 100)'\n  )\n\n\n\n\n\nWe can see that for most of the sample our automatic seasonal adjustment follows closely the official’s, but it clearly goes off track right after the COVID shock in early 2000’s. Fortunately, IBGE describes the model specification it uses for the seasonal adjustment of this series in a technical note. Some relevant information are:\n\nThe model specification is SARIMA(0,1,1)(0,1,1);\nIt incorporates Carnival and Corpus Christi – two important moving holidays in Brazil – in the model besides the usual trading days and Easter; and\nIt also includes two level shifts – April 2020 and December 2020 – and a temporary change – April 2020. All of them arguably to cope with the effects of the COVID shock.\n\nHow can we add these features to the seasonal adjustment model? Starting with the moving holidays, we need to create a vector with the dates of the holidays. However, for some holidays it might be well that their effect may extend beyond the day on which they occur. Carnival in Brazil is a good example. Even though the holiday happens on a Tuesday, the celebration starts on Monday and ends on Wednesday. Hence, it’s very important to include these two extra days in the input vector.\nThe genhol function makes this task much simpler as it automatically extends our date vector by including a number of earlier and/or later dates defined by the offsetting parameters start and end. Since Carnival and Corpus Christi occur, respectively, 47 and 60 days after Easter we can build the associated vectors from the latter – the seasonal package has built-in dates for Easter inside the easter vector. Otherwise, we would have to build them by ourselves (or import it from somewhere).\nLevel shifts and temporary changes can easily be incorporated using textual shortcuts in the regression.variables parameter. For level shifts we use lsYEAR.MONTH, whereas for transitory changes we use tcYEAR.MONTH. More information on the parameters can be found in the X-13-ARIMA Reference Manual.\nBelow we can see the full specification for the custom model intended to replicate IBGE’s along with the resulting plot.\n\nlibrary(lubridate)\ncarnival         &lt;- easter %m-% days(47)\ncorpus_christi   &lt;- easter %m+% days(60)\ncarnival_holiday &lt;- seasonal::genhol(\n  carnival, \n  start = -1, \n  end = 1, \n  frequency = 12, \n  center = 'calendar'\n)\ncorpus_christi_holiday &lt;- seasonal::genhol(\n  corpus_christi,\n  frequency = 12, \n  center = 'calendar'\n)\npmc_sa_customx13 &lt;- seas(\n  x = pmc_ts_nsa,\n  regression.variables = c(\n    \"td\", \"easter[1]\", \"ls2020.apr\", \"tc2020.apr\", \"ls2020.dec\"\n  ),\n  xreg = ts.union(carnival_holiday, corpus_christi_holiday), \n  regression.usertype = \"holiday\",\n  arima.model = \"(0 1 1)(0 1 1)\", \n  regression.aictest = NULL,\n  outlier = NULL, \n  transform.function = \"log\", \n  x11 = \"\"\n)\npmc_sa_customx13 %&gt;% \n  final() %&gt;% \n  autoplot() +\n  autolayer(pmc_sa_ibge, series = 'Retail SA (official)') +\n  labs(\n    title = 'Retail sales in Brazil - Volume Index (2014 = 100)',\n    subtitle = 'Seasonally-Adjusted',\n    x = '',\n    y = 'Index (2014 = 100)'\n  )\n\n\n\n\nThe new specification produced an almost perfect match with the official seasonally-adjusted data, specially for the post-COVID period. Some deviations are arguably due to slight differences in the holiday vector, but for now we’ll consider the goal achieved."
  },
  {
    "objectID": "ds_nominal2real.html#footnotes",
    "href": "ds_nominal2real.html#footnotes",
    "title": "5  Deflating nominal values to real values",
    "section": "",
    "text": "pluck(.x, 1,2,1) is equivalent to .x[[1]][[2]][[1]]↩︎"
  },
  {
    "objectID": "ds_hpfilter.html",
    "href": "ds_hpfilter.html",
    "title": "6  Hodrick-Prescott Filter",
    "section": "",
    "text": "In many economic applications, it’s necessary to decompose the time series into trend and cycle in order to relate the observed data with its theoretical counterpart. A classic example is the GDP series, where the trend component is often used as the empirical counterpart for potential GDP and the cycle component as the empirical counterpart for the output gap. In other applications, decomposition may be employed simply to make time series smoother by removing transitory events.\nThere are several statistical filters that perform this task, the best known being the Hodrick-Prescott filter. The HP-Filter requires only a single parameter, \\(\\lambda\\), which controls the sensitivity of the trend to short-term fluctuations. The rule of thumb is to use \\(\\lambda = 1600\\) for quarterly data; \\(\\lambda = 14400\\) for monthly data; and \\(\\lambda = 100\\) for yearly data. The hpfilter function from the mFilter package implements the HP-Filter with default values for \\(\\lambda\\) defined by the frequency of the time series object ts.\nTo see how it works in practice, let’s import data on Brazilian GDP using the sidrar package which is an interface to IBGE’s API. I chose to define the date variable using the as.yearqtr function from the zoo package because it’s compatible with ts objects – which is the default input for the hpfilter function.\n\nlibrary(tidyverse)\nlibrary(lubridate)\ngdp_br &lt;- sidrar::get_sidra(api = '/t/1620/n1/all/v/all/p/all/c11255/90707/d/v583%202')\ngdp_br &lt;- gdp_br %&gt;% \n  select(quarter = `Trimestre (Código)`, gdp = Valor) %&gt;% \n  mutate(quarter = zoo::as.yearqtr(quarter, format = '%Y%q'))\ngdp_br_ts &lt;- ts(gdp_br$gdp, start = first(gdp_br$quarter), frequency = 4)\n\n\n\nShow the code\nlibrary(forecast)\ngdp_br_ts %&gt;%\n  autoplot() +\n  labs(\n    title = 'Brazilian Quarterly GDP (Index: 1995 = 100)',\n    x = '',\n    y = 'Brazilian Quarterly GDP (Index: 1995 = 100)'\n  )\n\n\n\n\n\nSince the HP-Filter is not explicitly designed to deal with seasonality, I first remove the seasonal component using the automatic selection model provided by the seas function. The remaining pieces of code just apply the HP-Filter to the GDP time series and arrange the relevant output into a data frame.\n\nlibrary(seasonal)\nlibrary(mFilter)\ngdp_br_sa &lt;- final(seas(gdp_br_ts))\ngdp_br_hp &lt;- hpfilter(gdp_br_sa)\nhp_out    &lt;- tibble(\n  'quarter' = gdp_br$quarter,\n  'cycle'   = gdp_br_hp$cycle %&gt;% c(),\n  'trend'   = gdp_br_hp$trend %&gt;% c()\n)\n\n\n\nShow the code\nhp_out %&gt;% \n  pivot_longer(-quarter, names_to = 'var', values_to = 'value') %&gt;% \n  ggplot(aes(x = quarter)) +\n  geom_line(aes(y = value), lwd = 1) +\n  facet_wrap(~ var, scales = 'free_y', ncol = 1) +\n  labs(\n    title = 'HP-Filter decomposition of Brazilian GDP',\n    x = '', \n    y = ''\n    )\n\n\n\n\n\nDespite its vast popularity and widespread use, the HP filter gets mixed reviews. Perhaps the best known of these is the end-point bias, whose most common workaround is to add projections to the end of the series before applying the filter. We won’t dive into the pros and cons of the HP filter, since it’s beyond the scope of this book. Hamilton (2017) formalized several of these issues and proposed a new filter that was supposed to overcome all of them. According to the author:\n\n“A regression of the variable at date \\(t+h\\) on the four most recent values as of date \\(t\\) offers a robust approach to detrending that achieves all the objectives sought by users of the HP filter with none of its drawbacks.”\n\nThe fitted values and the residuals from the equation below provide, respectively, the trend and cycle components of the Hamilton filter. As a practical guide, Hamilton suggested using \\(h=8\\) for quarterly data. Nonetheless, some series may require longer periods \\(h\\) or more lags \\(k\\) for the filter to be effective.\n\\[ y_{t+h} = \\alpha + \\sum_{p=1}^{4} \\beta_p y_{t+1-p} \\]\nWe can perform Hamilton’s filter by estimating the above equation and then arranging the corresponding output into a data frame as we did with the HP filter. The augment function from the broom package does a great job in converting the output in the lm object into a data frame.\n\ngdp_br_hamilton &lt;- tibble(\n  quarter = gdp_br$quarter,\n  gdp_sa  = gdp_br_sa %&gt;% c()\n) %&gt;% \n  mutate(\n    y   = gdp_sa,\n    y1  = lag(gdp_sa, 8),\n    y2  = lag(gdp_sa, 9),\n    y3  = lag(gdp_sa, 10),\n    y4  = lag(gdp_sa, 11)\n  ) \nhamilton_filter &lt;- lm(y ~ y1 + y2 + y3 + y4, gdp_br_hamilton)\nhamilton_out &lt;- hamilton_filter %&gt;% \n  broom::augment() %&gt;% \n  mutate(quarter = gdp_br_hamilton$quarter[as.numeric(.rownames)]) %&gt;% \n  select(quarter, trend = .fitted, cycle = .resid)\n\n\n\nShow code\nhamilton_out %&gt;% \n  pivot_longer(-quarter, names_to = 'var', values_to = 'value') %&gt;% \n  ggplot(aes(x = quarter)) +\n  geom_line(aes(y = value), lwd = 1) +\n  facet_wrap(~ var, scales = 'free_y', ncol = 1) +\n  labs(\n    title = 'Hamilton-Filter decomposition of Brazilian GDP',\n    subtitle = 'h = 8',\n    x = '', \n    y = ''\n    )\n\n\n\n\n\nWe can see a sharp drop-and-rise in the final part of the output series which is at odds with what we would expect from a trend component. This problem can be solved by setting \\(h = 12\\) (\\(h\\) should be a multiple of the time series frequency). The new plot is shown below.\n\n\nShow code\ngdp_br_hamilton2 &lt;- tibble(\n  quarter = gdp_br$quarter,\n  gdp_sa  = gdp_br_sa %&gt;% c()\n) %&gt;% \n  mutate(\n    y   = gdp_sa,\n    y1  = lag(gdp_sa, 12),\n    y2  = lag(gdp_sa, 13),\n    y3  = lag(gdp_sa, 14),\n    y4  = lag(gdp_sa, 15)\n  ) \nhamilton_filter2 &lt;- lm(y ~ y1 + y2 + y3 + y4, gdp_br_hamilton2)\nhamilton_out2 &lt;- hamilton_filter2 %&gt;% \n  broom::augment() %&gt;% \n  mutate(quarter = gdp_br_hamilton2$quarter[as.numeric(.rownames)]) %&gt;% \n  select(quarter, trend = .fitted, cycle = .resid)\nhamilton_out2 %&gt;% \n  pivot_longer(-quarter, names_to = 'var', values_to = 'value') %&gt;% \n  ggplot(aes(x = quarter)) +\n  geom_line(aes(y = value), lwd = 1) +\n  facet_wrap(~ var, scales = 'free_y', ncol = 1) +\n  labs(\n    title = 'Hamilton-Filter decomposition of Brazilian GDP',\n    subtitle = 'h = 12',\n    x = '', \n    y = ''\n    )\n\n\n\n\n\n\n\nShow code\nfinal_out &lt;- hamilton_out2 %&gt;% \n  mutate(type = 'Hamilton') %&gt;% \n  bind_rows(\n    hp_out %&gt;% \n      mutate(type = 'HP')\n  )\nfinal_out %&gt;% \n  pivot_longer(-c(quarter, type), names_to = 'var', values_to = 'value') %&gt;% \n  ggplot(aes(x = quarter)) +\n  geom_line(aes(y = value, color = type), lwd = 1) +\n  facet_wrap(~ var, scales = 'free_y', ncol = 1) +\n  theme(legend.position = 'top') +\n  labs(\n    title = 'Trend-Cycle Decomposition of Brazilian GDP',\n    x = '', \n    y = '',\n    color = ''\n  )\n\n\n\n\n\nWhich one should we choose in this case? From the statistical point of view, the HP Filter yielded a smoother path for the trend component and a desired stationary behavior for the cycle component. Nevertheless, theory suggests that there’s a link between the cycle component of the GDP and inflation. Thus, a common strategy is to evaluate what measure would have explained (core) inflation better in that period. Alternatively, other developments in the economy could favor the choice of either measure. Therefore, it would be up to the analyst to choose which one would best represent his view of the evolution of the economy in that period.\n\n\n\n\nHamilton, James D. 2017. “Why You Should Never Use the Hodrick-Prescott Filter.”"
  },
  {
    "objectID": "fc_abc.html#step-1-observe-the-time-series-features",
    "href": "fc_abc.html#step-1-observe-the-time-series-features",
    "title": "7  The abc",
    "section": "7.1 Step 1: Observe the time series features",
    "text": "7.1 Step 1: Observe the time series features\nForecasting is all about extrapolating patterns. When our forecast depends on other variables, we’re assuming that the historical relationship between the target and the explanatory variables holds into the future. Likewise, when we’re employing univariate methods the main assumption is that the time series features remain constant or evolve in an understandable way. Hence, the first step is to investigate the features of the time series.\nLet’s import the monthly CPI ex-regulated prices (Not Seasonally Adjusted) from the Brazilian Central Bank API using the rbcb package. Then, we’ll plot the series with the autoplot function from the forecast package. Note that it’s based on ggplot so we can add ggplot layers to the plot.\n\nlibrary(forecast)\nlibrary(tidyverse)\ncpi_br &lt;- \n  rbcb::get_series(\n    code = list(\"cpi\" = 11428),\n    start_date = \"2004-01-01\",\n    end_date   = \"2022-08-01\",\n    as = \"ts\")\n\n\nautoplot(cpi_br) + \n  labs(\n    title = 'Brazilian CPI ex-regulated prices - %MoM NSA',\n    y = '',\n    x = ''\n    ) +\n  scale_y_continuous(labels = function(x) paste0(x, '%'))\n\n\n\n\nFigure 7.1: Plot of Brazilian CPI ex-regulated prices - %MoM NSA\n\n\n\n\nThe figure above gives a general picture of the time series. We can visually infer that the average CPI was around 0.5% from 2004 to 2016, then dropped to half that from mid-2016 to early 2020 before the Covid-19 pandemic hit the economy. This episode started a trend where inflation increased to values close around 1%.\nA more useful plot can be obtained using the mstl function, which decomposes the series into trend, seasonal and remainder (noise) terms. These are precisely the features we’re most interested in understanding in order to make accurate predictions.\n\nmstl(cpi_br) %&gt;% \n  autoplot() +\n  labs(title = 'Brazilian CPI: time series decomposition')\n\n\n\n\nFigure 7.2: MSTL Decomposition of the Brazilian CPI ex-regulated prices time series\n\n\n\n\nWe can extract two important pieces of information from this graph. The first one is that the trend started in the aftermath of the Covid-19 pandemic is flattening, although with no clear sign of a reversion; and the second one is a noticeable change in the seasonal pattern as of 2016, with higher peaks and a different shape. So when selecting an appropriate forecasting model we should opt for those with a flexible approach to both trend and seasonality, in the case of univariate models, or choose explanatory variables which reproduce this pattern at some extent."
  },
  {
    "objectID": "fc_abc.html#step-2-split-the-sample",
    "href": "fc_abc.html#step-2-split-the-sample",
    "title": "7  The abc",
    "section": "7.2 Step 2: Split the sample",
    "text": "7.2 Step 2: Split the sample\nWe need to split the sample into training and testing sets in order to perform the evaluation of our model. There are several splitting schemes and the appropriate choice depends on the nature of the data and the sample size. For time series, the most robust scheme is block cross-validation, where many contiguous sections of the time series are selected at random to train the model and tests are performed on the adjacent observations. In practice, however, it’s very common to use the leave-one-out approach, where we train the model using observations up to \\(t-1\\) to predict the target at \\(t\\). This procedure is iterated over \\(t\\) in order to provide a representative set of (pseudo) out-of-sample forecasts that we can use to assess the model accuracy.\nNote that the structural changes we saw in Figure 7.2 have important implications for the choice of the split scheme. More specifically, the new seasonal pattern comprises about 40% of the sample, whereas the post-Covid trend is present in only 15%. Thus, testing our model over the whole sample – or a sample which overrepresents this period – could make us believe that we have a good model to predict the future when in fact we don’t.\nLet’s make a simple though very common choice here: take the sample as of 2016 and use the leave-one-out approach starting at Jan/2019 to predict the twelve months ahead. We’ll use the rsample package which is part of the tidymodels ecosystem of R packages specially designed to handle forecasting in a tidy way. Since it’s under the tidy philosophy, we need to first convert our data from ts to data frame or tibble. It can be easily achieved using the timetk package.\nThe rolling_origin function from rsample provides a convenient object where in each slice we have two components, the train set and the test set, which can be accessed by specific functions. The initial argument defines the size of the initial sample used for training the model; the assess argument defines the number of observations used for assessment in each step; and cumulative = TRUE means we’ll not drop the first value of the sample as we incorporate a new value so as to keep the sample size constant. Rather, we’ll have an expanding-window sample.\n\nlibrary(rsample)\nlibrary(timetk)\ncpi_df &lt;- \n  tk_tbl(cpi_br, rename_index = 'date') %&gt;% \n  filter(date &gt;= 'jan 2016')\ncpi_split &lt;- \n  rolling_origin(\n  cpi_df, \n  initial = which(cpi_df$date == 'dec 2018'),\n  assess = 1,\n  cumulative = TRUE\n  ) \ncpi_split\n\n# Rolling origin forecast resampling \n# A tibble: 44 × 2\n   splits         id     \n   &lt;list&gt;         &lt;chr&gt;  \n 1 &lt;split [36/1]&gt; Slice01\n 2 &lt;split [37/1]&gt; Slice02\n 3 &lt;split [38/1]&gt; Slice03\n 4 &lt;split [39/1]&gt; Slice04\n 5 &lt;split [40/1]&gt; Slice05\n 6 &lt;split [41/1]&gt; Slice06\n 7 &lt;split [42/1]&gt; Slice07\n 8 &lt;split [43/1]&gt; Slice08\n 9 &lt;split [44/1]&gt; Slice09\n10 &lt;split [45/1]&gt; Slice10\n# ℹ 34 more rows\n\n\nAs you can see, the cpi_split contains 43 slices – each slice is represented by a two-dimensional sample with [train set/test set]. The first slice contains a training set ranging from Jan/2016 to Dec/2018 (36 observations) and a test set with a single observation on Jan/2019. The second slice incorporates the observation on Jan/2019 to the training set (37 observations now) and leaves Feb/2019 as the test set. The same logic applies to the other slices until the end of the sample. With our split scheme done, we’re ready to go the next step."
  },
  {
    "objectID": "fc_abc.html#step-3-choose-the-model",
    "href": "fc_abc.html#step-3-choose-the-model",
    "title": "7  The abc",
    "section": "7.3 Step 3: Choose the model",
    "text": "7.3 Step 3: Choose the model\nChoosing an appropriate model involves several dimensions. First, we should decide whether or not to use explanatory variables – and if so, which variables to use. In addition, we should weigh the pros and cons of employing a machine learning model instead of a simpler method. Considerations on both processing time and interpretability play a significant role in many fields. Since the purpose here is to develop intuition about each step of the forecasting pipeline, I’ll avoid these issues by assuming we’re restricted to using univariate statistical models only. This is by no means a very strong assumption since in real-life we are often required to provide reasonable forecasts quickly rather than spending a great amount of time searching for the most accurate numbers.\nThe forecast package contains a large set of very useful univariate statistical models. The ETS is generally my choice when I have no obvious candidate, because it doesn’t impose any strong assumption about the data (eg. stationarity). In addition, it has proven to perform very well on a variety of data sets at the M Competition.\nI use TBATS as a first approach whenever I have to predict high frequency data (daily or higher) since it can handle multiple seasonal patterns. ARIMA is useful for stationary data, specially when ACF/PACF plots show a well-defined autocorrelation structure. However, it’s worth noting that in practice statistical assumptions are often overlooked when the purpose of the model is forecasting rather than making inference.\nAs a matter of fact, producing accurate forecasts is inevitably a trial and error process and, as we become experienced with the subject, some choices look more promising. For instance, we saw that the Brazilian CPI exhibit a changing trend – favoring a more flexible model like ETS. On the other hand, for most of the period this trend seems to evolve at a constant pace, which makes ARIMA models good candidates as well.\nIn addition, in the presence of a seasonal pattern models which are more successful in capturing the seasonality of the data have a clear advantage. It’s not unusual for one model to be better at capturing a specific feature of the data while another model does the same for other feature – that’s why combining the forecasts from different models usually improve accuracy.1 Therefore, in order to have a more reliable result we’ll produce forecasts from three sources: ETS, ARIMA and the average between them.\n\ncpi_fc &lt;- cpi_split %&gt;% \n  mutate(\n    ets_fc = map_dbl(\n      .x = splits, \n      .f = ~ (.x %&gt;% \n        analysis() %&gt;% \n        tk_ts(select = 'value', start = c(2016,1), frequency = 12) %&gt;% \n        ets() %&gt;% \n        forecast(h = 1)\n        )$mean\n      ),\n    arima_fc = map_dbl(\n      .x = splits, \n      .f = ~ (.x %&gt;% \n        analysis() %&gt;% \n        tk_ts(select = 'value', start = c(2016,1), frequency = 12) %&gt;% \n        auto.arima() %&gt;% \n        forecast(h = 1)\n        )$mean\n    ),\n    avg_fc = (arima_fc+ets_fc)/2,\n    date = map_chr(\n      .x = splits,\n      .f = ~ (.x %&gt;% \n                assessment()\n              )$date %&gt;% \n        as.character()\n    )  %&gt;% \n        zoo::as.yearmon()\n    ) %&gt;% \n  select(date, contains('fc')) %&gt;% \n  right_join(cpi_df, by = 'date')\ncpi_fc\n\n# A tibble: 80 × 5\n   date      ets_fc arima_fc avg_fc value\n   &lt;yearmon&gt;  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1 Jan 2019   0.166   0.330  0.248   0.41\n 2 Feb 2019   0.179   0.364  0.271   0.48\n 3 Mar 2019   0.285   0.414  0.349   0.75\n 4 Apr 2019   0.416   0.567  0.492   0.41\n 5 May 2019   0.444   0.377  0.411  -0.23\n 6 Jun 2019   0.368   0.200  0.284   0.08\n 7 Jul 2019   0.236   0.142  0.189   0.12\n 8 Aug 2019   0.187   0.132  0.160  -0.06\n 9 Sep 2019   0.187   0.0449 0.116  -0.1 \n10 Oct 2019   0.126  -0.0230 0.0516  0.18\n# ℹ 70 more rows\n\n\nNow we have a data frame with the predictions from each source plus the actual (realized) value for the CPI for the last 45 months – including periods pre-, during- and post-Covid. In the next section, we’ll see how to get a representative summary of the results so as we can conclude which model is better. You may have a clue on how to do this, but I can assure you that there are some relevant aspects worth exploring that are hardly found elsewhere. Before moving to the next section, we can take a look on how these forecasts look like.\n\n\nShow the code\ncpi_fc %&gt;% \n  pivot_longer(-date, names_to = 'model', values_to = 'forecast') %&gt;% \n  ggplot(aes(x = date)) +\n  geom_line(aes(y = forecast, color = model), lwd = 1) +\n  theme_light() +\n  scale_color_brewer(type = 'qual', palette = 6) +\n  theme(legend.position = 'top') +\n  labs(title = 'Brazilian CPI Forecasts - %MoM NSA',\n       y = 'CPI (%MoM NSA)',\n       color = '')\n\n\nWarning: Removed 108 rows containing missing values (`geom_line()`)."
  },
  {
    "objectID": "fc_abc.html#step-4-evaluate-the-model",
    "href": "fc_abc.html#step-4-evaluate-the-model",
    "title": "7  The abc",
    "section": "7.4 Step 4: Evaluate the model",
    "text": "7.4 Step 4: Evaluate the model\nChoosing the best forecasting model can be stated in mathematical terms as a problem of minimizing an error metric – the mean absolute error (MAE) or the root mean square error (RMSE) being common choices. These metrics are loss functions, i.e, they express an objective. Consequently, we should be fully aware of what our objective is in order to translate it into an appropriate metric (or function).\nFor example, MAE and RMSE are symmetric functions and use simple average to summarize the forecasting errors. Using both of them to evaluate model’s accuracy is equivalent to say: “I don’t care about the sign of the error – 2 units up or down equally impact my result; Also, it doesn’t matter when the largest errors occurred – over the last 3 observations or in the early part of the evaluation period”.\nSurely, these conditions don’t apply to all businesses. Someone interested in forecasting the demand for electricity in a large city might prefer to be surprised down than up. Also, a model with higher accuracy in the last 12 months might be better at capturing the current electricity demand pattern than a model with a great performance on the initial periods of the testing sample. In short, many situations require us to define what conditions the model must meet and this involves designing a specific function. This function should summarize the forecast errors in order to represent our objective.\nTo demonstrate this idea, I will propose two alternative accuracy metrics that are slight modifications of the well-known MAE. The first (accuracy_1) assigns double the weight to upside errors (predictions below actual values), whereas the second (accuracy_2) assigns (linearly) decreasing weights as further in the past the errors are. You should be aware that the results from the two metrics are not directly comparable, the ordering of models being the relevant information here.\n\naccuracy_1 &lt;- function(e){\n  .abs_weighted_errors      &lt;- ifelse(e &gt; 0, 2*e, abs(e))\n  .mean_abs_weighted_errors &lt;- mean(.abs_weighted_errors)\n  return(.mean_abs_weighted_errors)\n}\naccuracy_2 &lt;- function(e){\n  .abs_errors               &lt;- abs(e)\n  .weights                  &lt;- seq(from = 1, to = length(.abs_errors), by = 1)\n  .weights                  &lt;- .weights/sum(.weights)\n  .mean_abs_weighted_errors &lt;- weighted.mean(.abs_errors, .weights)\n  return(.mean_abs_weighted_errors)\n}\n\nBelow I plot the accuracy_1 function along with the original MAE function as a more effective way to give you a sense of what’s happening behind the scenes. Basically, for negative errors (realized value below the prediction) the weigh are the same as in the original MAE, while it’s somewhat higher for positive errors (realized value above the prediction).\n\n\nShow the code\nlibrary(ggtext)\nacc_demo &lt;- tibble(\n  x = seq(from = -2, to = 2, by = 0.01)\n) %&gt;% \n  mutate(\n    t   = 1:n(),\n    Loss_1 = ifelse(x &gt; 0, 2*x, abs(x)),\n    mae    = abs(x)\n  )\nacc_demo %&gt;% \nggplot(aes(x = x)) + \n  geom_line(aes(y = Loss_1), color = \"darkblue\", lwd = 1) +\n  geom_line(aes(y = mae), color = \"red\", lwd = 1) +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  theme_light() +\n  theme(plot.title = element_markdown(lineheight = 1.1),\n        axis.title = element_text(size = 13),\n        legend.position = \"none\") +\n  labs(title = \"&lt;span style='color:#002266;'&gt;&lt;b&gt;Custom Loss Function&lt;/b&gt;&lt;/span&gt; vs &lt;span style='color:#ff1a1a;'&gt;&lt;b&gt;MAE&lt;/b&gt;&lt;/span&gt;\",\n       x = \"Error\", y = \"Loss\")\n\n\n\n\n\nNow we’re ready to apply our two custom functions plus the MAE to the errors we computed from the three models in order to decide which one is the most accurate. Again, these metrics aren’t comparable to each other. Instead, we’re interested in the ordering within the same metric.\nIn addition, there’s no such a thing as the best metric. As we saw earlier in this section, the appropriate metric is the one that reflects as closely as possible our objective. Besides, we find several desirable characteristics in conventional metrics such as MAE or RMSE and I don’t mean to rule them out (see Robert J. Hyndman and Koehler (2005) for more on this). The main message here is that we must be fully aware of what our objective is and how to translate it into an appropriate function. In this regard, the knowledge of functional forms is essential.\n\ncpi_errors &lt;- cpi_fc %&gt;% \n  filter(date &gt;= 'Jan 2019') %&gt;% \n  mutate(across(contains('fc'), ~ value - .x, .names = 'error_{.col}')) %&gt;% \n  summarise(\n    across(contains('error'), \n           list(\n             'acc1' = ~ accuracy_1(.x), \n             'acc2' = ~ accuracy_2(.x),\n             'mae'  = ~ mean(abs(.x))\n           ), \n           .names = '{.col}-{.fn}')) %&gt;% \n  pivot_longer(everything(), names_to = 'model_metric', values_to = 'value') %&gt;% \n  separate('model_metric', c('model', 'metric'), '-') %&gt;% \n  pivot_wider(names_from = 'metric', values_from = 'value') %&gt;% \n  mutate(model = str_remove_all(model, 'error_|_fc'))\ncpi_errors\n\n# A tibble: 3 × 4\n  model  acc1  acc2   mae\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 ets   0.488 0.304 0.317\n2 arima 0.456 0.286 0.284\n3 avg   0.468 0.292 0.298\n\n\nThe results show that the ARIMA model is the most accurate by the three metrics, outperforming even the average model. At this point, I’d like to conclude with two considerations. The first is that combining models usually improves performance, but not always as the above exercise made clear.\nNevertheless, although the literature shows that using either the mean or median of the models is very difficult to be beaten, it’s possible to improve accuracy by optimizing the weights assigned to each model.\nFinally, we compared the models by means of their point forecasts. Despite being very common, this does not take into account the fact that each point forecast is one single realization of a random process and there is a vast literature suggesting the use of density forecasts and distributional accuracy measures.\n\n\n\n\nHyndman, R. J., and G. Athanasopoulos. 2018. Forecasting: Principles and Practice, 2nd Edition. OTexts: Melbourne, Australia.\n\n\nHyndman, Robert J., and Anne B. Koehler. 2005. “Another Look at Measures of Forecast Accuracy.”"
  },
  {
    "objectID": "fc_abc.html#footnotes",
    "href": "fc_abc.html#footnotes",
    "title": "7  The abc",
    "section": "",
    "text": "Also, it’s possible to fit the model for each time period – an approach called direct forecast – or even to use different models for different time periods. However, we’ll not explore these topics here.↩︎"
  },
  {
    "objectID": "fc_comparison.html#the-early-days-of-covid-19-in-brazil",
    "href": "fc_comparison.html#the-early-days-of-covid-19-in-brazil",
    "title": "8  Forecasting by comparison",
    "section": "8.1 The early days of COVID-19 in Brazil",
    "text": "8.1 The early days of COVID-19 in Brazil\nIt was the beginning of March, 2020 and we were seeing new cases of COVID spreading rapidly in different countries in Asia and Europe. In Brazil, it would start a few weeks later. At that time I was working at Itaú Asset Management, a well-known hedge fund in Brazil. Portfolio managers and analysts needed an accurate yet timely sense of how the situation would evolve domestically in order to make reasoned decisions and keep track of the potential impact on both economic and financial variables. However, Brazil was stepping into the early days of the epidemic curve so that no more than a handful of observations were available to estimate any model. How could I deliver a reliable answer?\nI started looking for patterns stemming from other countries which were already a couple of weeks ahead in this process. This search brought about some interesting clues. In particular, I noticed that for several countries the daily increase in cumulative cases was very noisy up to the hundredth confirmed case, but then it seemed to decrease slowly – though not steadily – as a function of time. We can check it in the plot below.\n\n\nShow the code\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(openxlsx)\ncovid_data &lt;- readRDS('data/ch07_covid_data.rds')\ncovid_data_aux &lt;- covid_data %&gt;%\n  filter(type == \"confirmed\") %&gt;%\n  group_by(Country.Region) %&gt;%\n  mutate(\n    acum_cases = cumsum(cases),\n    r = ((acum_cases/lag(acum_cases))-1)*100\n    ) %&gt;%\n  filter(acum_cases &gt;= 100) %&gt;%\n  mutate(t = 1:n()) %&gt;%\n  ungroup()\n\n\n\n\nShow the code\nlibrary(gghighlight)\nlibrary(ggrepel)\ncountries_out &lt;- c(\"Qatar\", \"Pakistan\", \"Dominican_Republic\")\ncovid_data_aux %&gt;%\n  dplyr::filter(\n    !Country.Region %in% countries_out,\n    t &lt;= 50,\n    date &lt;= '2020-03-17'\n  ) %&gt;%\n  ggplot(aes(x = t, y = r, color = Country.Region)) +\n  geom_line(lwd = 1) +\n  gghighlight(Country.Region %in% c(\"Brazil\", \"Italy\", \"Iran\")) +\n  theme_light() +\n  scale_y_continuous(labels = function(x) paste0(x, \"%\")) +\n  theme(legend.position = \"none\") +\n  labs(\n    title = \"Daily increase in total cases for available countries (%)\",\n    subtitle = 'Data up to 2020-03-17',\n    x = \"Days after 100th confirmed case\",\n    y = \"\", \n    color = \"\",\n    caption = \"Data: European Centre for Disease Prevention and Control.\"\n    )\n\n\n\n\n\nI could explore this fact to obtain future values for Brazil. Of course that reasoning assumed that the daily increase in total cases for Brazil would on average resemble the path of a given country or a pool of countries (if I used a panel regression) – not a very strong assumption, especially if we consider the lack of data.\nIn this regard, I decided to rule out countries which did not appear to be reasonable benchmarks such as China and South Korea since they had imposed tight restrictions very quickly – and I didn’t expect Brazil to do the same. Two countries were seemingly good candidates back then, Iran and Italy. Both of them shared the time pattern described above. Furthermore, expectations were that tougher restrictions would be implemented only progressively – something I suspected would be done in Brazil too.\nSo how did I make use of the information from these countries to produce forecasts for Brazil? Firstly, I estimated the curve above using a simple OLS regression with data as of the hundredth case – the period where the time trend was noticeable. I chose to model each country separately since I could assess each path later and eventually discard the one performing worse.\nLet \\(r_t\\) be the daily increase in total cases in period \\(t\\) and \\(i = [\\text{Italy, Iran}]\\). Then,\n\\[ log(r_{it}) = \\beta_0 + \\beta_1t_i \\hspace{0.5cm} (1)\\] The OLS estimate for this equation is shown below.\n## Data for regression\ndata_reg &lt;- covid_data_aux %&gt;%\n  dplyr::filter(\n    Country.Region %in% c(\"Italy\", \"Iran\")\n  ) %&gt;%\n  plyr::dlply(.variables = \"Country.Region\")\n## Model equation\nmod_eq &lt;- 'log(r) ~ t' %&gt;% as.formula()\n## Fit the model\nfit_reg &lt;- map(.x = data_reg, .f = ~ lm(mod_eq, data = .x))\n\n\nShow the code\nlibrary(jtools)\n## Plot model results\nexport_summs(\n  fit_reg$Italy, \n  fit_reg$Iran,\n  model.names = rev(names(fit_reg))\n)\n\n\nRegistered S3 methods overwritten by 'broom':\n  method            from  \n  tidy.glht         jtools\n  tidy.summary.glht jtools\n\n\n\n\n\n\nItaly\nIran\n\n\n(Intercept)\n4.05 ***\n3.79 ***\n\n\n\n(0.07)   \n(0.11)   \n\n\nt\n-0.06 ***\n-0.06 ***\n\n\n\n(0.00)   \n(0.00)   \n\n\nN\n55       \n52       \n\n\nR2\n0.95    \n0.86    \n\n\n*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05.\n\n\n\n\n\n\n\nNext, consider \\(C_t\\) the number of total cases in period \\(t\\). Thus the total cases in Brazil for \\(t+1\\) is given by:\n\\[ C_{t+1} = C_t \\times \\left(1+\\frac{\\hat{r_{it}}}{100}\\right) \\ (2)\\] where \\(\\hat{r_{it}} = e^{\\hat{\\beta^0} + \\hat{\\beta_1}t_i}\\)\nHence the path computed in \\(t\\) for \\(t+k\\), \\(k = 1,2,3,4, ... ,T\\) is given by:\n\\[ C_{t+k} = C_t \\times \\prod_{t}^{k}\\left(1+\\frac{\\hat{r_{it}}}{100}\\right) \\ (3)\\] Note that we can use the fitted values from Equation 1 to obtain \\(\\hat{r_{it}}\\) as long as \\(t+k\\) is available in the sample of the country \\(i\\). For a longer period not yet reached by the country \\(i\\), we could easily make forecasts of \\(\\hat{r_{it}}\\) since the only co-variate is \\(t\\) – the time in days after the hundredth confirmed case.\nLet us suppose we are on March 16 (the third day after Brazil exceeded 100 cases) and we wish to compute the path for the next three days. Thus we need \\(\\hat{r}_{it}\\) for \\(t =\\) 4, 5 and 6 as well as \\(C_3 = 235\\), the number of total cases recorded in March 16. We dubbed Iran-like and Italy-like the forecasts for Brazil based on the fitted values using data from these countries.\nFor example, to produce the Italy-like path on March 16 we should first take the (exponential of the) fitted values from the Italy model.\n\n1+exp(fitted(fit_reg$Italy))/100\n\n       1        2        3        4        5        6        7        8 \n1.536947 1.503969 1.473016 1.443964 1.416697 1.391104 1.367084 1.344538 \n       9       10       11       12       13       14       15       16 \n1.323377 1.303516 1.284875 1.267378 1.250957 1.235543 1.221077 1.207499 \n      17       18       19       20       21       22       23       24 \n1.194755 1.182793 1.171566 1.161029 1.151139 1.141856 1.133144 1.124966 \n      25       26       27       28       29       30       31       32 \n1.117291 1.110087 1.103326 1.096980 1.091024 1.085433 1.080186 1.075261 \n      33       34       35       36       37       38       39       40 \n1.070639 1.066300 1.062228 1.058406 1.054819 1.051452 1.048292 1.045326 \n      41       42       43       44       45       46       47       48 \n1.042542 1.039930 1.037477 1.035175 1.033015 1.030987 1.029084 1.027298 \n      49       50       51       52       53       54       55 \n1.025621 1.024048 1.022571 1.021184 1.019883 1.018662 1.017516 \n\n\nThen, we multiply the initial value (the number of cases on March 16) by the cumulative \\(\\hat{r}_{it}\\) starting on \\(t = 4\\). Note that we could produce forecasts for the next 55 days since Italy was at the 55th day after the 100th case. Also, we could produce a new forecast path everyday updating the actual value for \\(C\\). This would certainly make our forecasts more accurate, since we would not carry forward wrong values predicted for \\(t = 2, 3,..., T\\). But to keep the exercise simple, let’s suppose a single path completely built from \\(C_3 = 235\\).\n\nMarch 17: \\(235 \\times 1.44 = 338\\)\nMarch 18: \\(235 \\times 1.44 \\times 1.41 = 477\\)\nMarch 19: \\(235 \\times 1.44 \\times 1.41 \\times 1.39 = 663\\)\n\nWe can get rid of manual calculations by creating a function that takes these arguments – \\(C\\), \\(t\\) and the regression model – and deliver the estimated number of total cases for \\(t+k\\).\n\ncovid_fc &lt;- function(model, C, t){\n  mod_fitted &lt;- fitted(model)\n  r_t        &lt;- 1+exp(mod_fitted)/100 \n  r_t_cum    &lt;- r_t[t:length(r_t)] %&gt;%  cumprod()\n  out &lt;- round(C*r_t_cum, 0)\n  return(out)\n}\ncovid_fc(fit_reg$Italy, 235, 4)\n\n     4      5      6      7      8      9     10     11     12     13     14 \n   339    481    669    914   1229   1627   2120   2725   3453   4320   5337 \n    15     16     17     18     19     20     21     22     23     24     25 \n  6517   7869   9402  11120  13028  15126  17412  19882  22529  25345  28317 \n    26     27     28     29     30     31     32     33     34     35     36 \n 31435  34683  38046  41510  45056  48669  52332  56028  59743  63461  67167 \n    37     38     39     40     41     42     43     44     45     46     47 \n 70849  74494  78092  81632  85104  88503  91819  95049  98187 101230 104174 \n    48     49     50     51     52     53     54     55 \n107018 109760 112399 114936 117371 119705 121939 124074 \n\n\nSince our models are stored in a list object, we can use the map function to compute the results for both of them at once.\n\ncovid_br_fc &lt;- \n  map(\n    .x = fit_reg,\n    .f = ~ covid_fc(.x, 235, 4) %&gt;% \n      as.data.frame() %&gt;% \n      tibble::rownames_to_column() %&gt;% \n      magrittr::set_colnames(c('t', 'forecast')) %&gt;% \n      mutate(t = as.numeric(t))\n  ) %&gt;% \n  plyr::ldply(.id = 'country') %&gt;% \n  pivot_wider(names_from = 'country', values_from = 'forecast')\n\nIt’s much easier to look at the results graphically.\n\n\nShow the code\ndata_plot &lt;- covid_data_aux %&gt;% \n  filter(Country.Region == 'Brazil') %&gt;% \n  select(date, t, observed = acum_cases) %&gt;%\n  left_join(covid_br_fc) %&gt;% \n  select(-t)\ndata_plot %&gt;% \n  filter(date &lt;= '2020-04-01' & date &gt;= '2020-03-17') %&gt;% \n  pivot_longer(cols = -c('date'), names_to = 'var', values_to = 'value') %&gt;% \n  ggplot(aes(x = date)) +\n  geom_line(aes(y = value, color = var), lwd = 1) +\n  theme_light() +\n  scale_x_date(date_breaks = '3 days', date_labels = '%b %d') +\n  scale_y_continuous(labels = function(x) format(x, big.mark = '.')) +\n  theme(\n    legend.position = 'top', \n    axis.text = element_text(size = 13),\n    legend.text = element_text(size = 13)\n    ) +\n  labs(title = 'Covid total cases in Brazil - Observed vs. Forecasts',\n       subtitle = 'Estimated on March 16',\n       x = '', y = 'Total Cases', color = '') \n\n\n\n\n\nThe actual values for Brazil were very close to those recorded by the two countries in the short-term and lied between the two curves in the mid-term – although I have not updated the forecasts here, something I did at the time. This result shows how simple exercises can play a significant role in real cases. In fact, we successfully relied on this strategy for a couple of months. At the time, both models produced good one up to seven-step-ahead forecasts since the beginning. For longer horizons, the Iran-like model delivered fairly stable accuracy, while the Italy-like model surprisingly improved over time."
  },
  {
    "objectID": "fc_comparison.html#the-second-wave-in-brazil",
    "href": "fc_comparison.html#the-second-wave-in-brazil",
    "title": "8  Forecasting by comparison",
    "section": "8.2 The second wave in Brazil",
    "text": "8.2 The second wave in Brazil\nQuestions shift very rapidly in the financial markets. After some time, we were experiencing the second wave of Covid in Brazil and a set of containing restrictions were in place. As we approached important dates for local retail businesses, the big question was whether those restrictions would be lifted in time not to cause major damage to the economy.\nNote that it’s no more a matter of simply predicting new cases for the next days. This time we needed an estimate for the second wave peak and how fast the subsequent decline would be – these parameters served as triggers for policy decisions. Once again, Brazil was a latecomer in this process as several countries had already gone through the second wave. So with the appropriate ideas we could benefit from this condition.\nIn general, how long does it take to reach the peak after the second wave has started? How long does it take to go down to the bottom after reaching the peak? The first challenge here is that the second wave occurred in a non-synchronized way between countries. The number of days around the peak wasn’t the same either.\nHowever, looking at the plot below we can see a large number of peaks occurring in the period between November 2020 and March 2021. At the time, I assumed it as the typical period where the second wave took place. Note that some countries had started a third wave, but I couldn’t rely on these information since this new cycle was not yet complete.\n\n\nShow the code\ncovid_data &lt;- read_csv('data/owid-covid-data.csv')\ncovid_data %&gt;% \n  filter(date &lt;= '2021-05-01') %&gt;% \n  ggplot(aes(x = date)) +\n  geom_line(aes(y = new_cases_smoothed, color = location)) +\n  theme_light() +\n  theme(legend.position = 'none') +\n  scale_y_continuous(labels = function(x) format(x, big.mark = '.')) +\n  scale_x_date(date_breaks = '2 months', date_labels = '%b/%y') +\n  annotate(\n    \"rect\",\n    xmin = as.Date('2020-11-01'),\n    xmax = as.Date('2021-03-01'),\n    ymin = -Inf, ymax = Inf,\n    alpha = .2) +\n  labs(title = 'Covid New Cases (smoothed) by country',\n       subtitle = 'Shaded Area = assumed 2nd wave',\n       x = '', y = 'Covid New Cases (smoothed)')\n\n\n\n\n\nThe idea I came up with was to zoom-in on this period and then compute the typical behavior of new cases around the peak. First, I filtered the November 2020 - March 2021 period excluding Brazil from the data set. Next, I created a variable peak_date as the earliest date where each country recorded the maximum number of new cases in the period. I also created the variable t_around_peak to count the number of days before and after the peak date. Finally, I computed the median, first and third quartiles from the distribution of new cases for every t_around_peak. But notice that I had standardized (scaled) the data in order to prevent countries with higher (lower) numbers from over(under)weight the statistics.\n\ncovid_2nd_wave &lt;- covid_data %&gt;%\n  select(date, continent, location, new_cases_smoothed) %&gt;% \n  filter(\n    location != 'Brazil',\n    between(date, as.Date('2020-11-01'), as.Date('2021-04-01'))\n  ) %&gt;% \n  group_by(location) %&gt;%\n  mutate(\n    peak_date = min(date[which(new_cases_smoothed == max(new_cases_smoothed, na.rm = TRUE))]),\n    t_around_peak = (date - peak_date) %&gt;% as.numeric(),\n    new_cases_smoothed_std = scale(new_cases_smoothed)) %&gt;%\n  ungroup() %&gt;% \n  filter(\n    !is.na(peak_date)\n    ) %&gt;% \n  group_by(t_around_peak) %&gt;% \n  summarise(\n    median = median(new_cases_smoothed_std, na.rm = TRUE),\n    lower  = quantile(new_cases_smoothed_std, probs = 0.25, na.rm = TRUE),\n    upper  = quantile(new_cases_smoothed_std, probs = 0.75, na.rm = TRUE)\n  ) %&gt;% \n  ungroup()\n\n\n\nShow the code\ncovid_2nd_wave %&gt;% \n  filter(between(t_around_peak, -80, 80)) %&gt;% \n  ggplot(aes(x = t_around_peak)) +\n  geom_line(aes(y = median)) +\n  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.3, fill = 'grey70') +\n  theme_light() +\n  scale_x_continuous(breaks = seq(-80, 80, 20)) +\n  labs(\n    title = 'Covid 2nd wave typical distribution around the peak',\n    x = '# of days before (-) and after (+) peak',\n    y = 'New cases smoothed (scaled)'\n  )\n\n\n\n\n\nOn average, the number of new cases grew rapidly for 20 days until it peaked and then took about the same amount of days to reach the bottom. In addition, there was greater uncertainty about the behavior of the pre-peak period as showed in the shaded area. Once again, the exercise proved to be very helpful as the realized values came in line with expectations."
  },
  {
    "objectID": "fc_simulation.html#footnotes",
    "href": "fc_simulation.html#footnotes",
    "title": "9  Simulations",
    "section": "",
    "text": "It would be more realistic, for example, that the interest rate would assume the 4% value if inflation was above the target. We could create rules to account for this kind of dependency and use them to filter the variables_sim data frame.↩︎"
  },
  {
    "objectID": "single_equation.html",
    "href": "single_equation.html",
    "title": "10  Single equation models",
    "section": "",
    "text": "A fundamental part of the work in economic research consists of estimating relationships between variables. A good starting point is to use reduced forms of well-established models in the literature. For example, to analyze the effect of a certain variable on economic activity, we can use a specification derived from the IS curve. Similarly, we can use (a variation of) the Phillips Curve to measure the impact of a certain variable on inflation.\nIn this section, we’ll see how to estimate the parameters of a simple reduced form Phillips Curve, check basic properties of the model and extract some useful information from it. It’s worth noting that the same procedures applies to any other custom model.\nFor this exercise, we’ll use seasonally-adjusted quarterly data from the Brazilian economy ranging from 2004Q1 to 2022Q4. The basic Phillips Curve can assume the following form:\n\\[ \\pi_t = \\beta_1\\pi_{t-1} + \\beta_2\\pi^{e}_{t,t+4|t} + \\beta_3\\Delta e_{t-1} + \\beta_4\\tilde{y}_{t-1} + \\epsilon_t \\] where \\(\\pi_t\\) is a measure of inflation; \\(\\pi^{e}_{t,t+4|t}\\) is the expected inflation in \\(t\\) for \\(t+4\\); \\(e\\) is a measure of exchange rate or imported inflation; and \\(\\tilde{y}\\) is a measure of output gap. In this exercise, \\(\\pi_t\\) is a measure of core inflation which excludes food-at-home and regulated prices (CPI_CORE); \\(\\pi^{e}\\) is the market expectations compiled by the Brazilian Central Bank (CPI_EXP); \\(e\\) is an index of commodities prices in USD (CI_USD); and \\(\\tilde{y}\\) is the cycle component obtained from the HP Filter on the GDP series (YGAP).\nLet’s start by importing the data set and visualizing the variables of interest.\n\n\nShow the code\nlibrary(tidyverse)\nlibrary(lubridate)\ncp_data &lt;- readRDS('data/ch12_cp_data.rds')\ncp_data %&gt;% \n  pivot_longer(-date, names_to = 'var', values_to = 'value') %&gt;% \n  ggplot(aes(x = date, y = value)) +\n  geom_line(lwd = 1) +\n  theme_light() +\n  facet_wrap(~ var, scales = 'free_y') +\n  labs(\n    title = 'Phillips Curve variables',\n    x = '',\n    y = ''\n  )\n\n\n\n\n\nNext, we need to create the appropriate variables for lagged CPI and YGAP and the percentage change of CI_USD. Then, we fit the model to the data. Note that we are imposing no restrictions on the coefficients at this point, although the structural version of the Phillips curve does (we’ll see how to do so in the next section). In addition, we are using OLS to estimate the coefficients, although an endogeneity-robust method such as the Generalized Method of Moments (GMM) is more suitable.\n\ncp_reg_data &lt;- cp_data %&gt;% \n  select(date, CPI_CORE, CPI_EXP, CI_USD, YGAP) %&gt;% \n  mutate(\n    CPI_CORE_lag = dplyr::lag(CPI_CORE, 1),\n    YGAP_lag     = dplyr::lag(YGAP, 1),\n    dlog_CI_USD  = log(CI_USD/dplyr::lag(CI_USD))*100\n  )\ncp_fit &lt;- lm(CPI_CORE ~ CPI_CORE_lag + CPI_EXP + YGAP_lag + dlog_CI_USD -1, cp_reg_data)\nsummary(cp_fit)\n\n\nCall:\nlm(formula = CPI_CORE ~ CPI_CORE_lag + CPI_EXP + YGAP_lag + dlog_CI_USD - \n    1, data = cp_reg_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.79874 -0.19115 -0.03149  0.16141  0.85059 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \nCPI_CORE_lag 0.656741   0.092798   7.077 8.48e-10 ***\nCPI_EXP      0.089305   0.026820   3.330 0.001381 ** \nYGAP_lag     0.031710   0.011866   2.672 0.009339 ** \ndlog_CI_USD  0.017335   0.004462   3.885 0.000227 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3169 on 71 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.9551,    Adjusted R-squared:  0.9526 \nF-statistic: 377.6 on 4 and 71 DF,  p-value: &lt; 2.2e-16\n\n\nThe coefficients are highly significant and show the expected signs. To check the model validity, it’s crucial that the residuals are zero-mean and exhibit no clear trend. The checkresiduals functions from the forecast package provides a handy summary of the model residuals.\n\nforecast::checkresiduals(cp_fit)\n\n\n\n\n\n    Breusch-Godfrey test for serial correlation of order up to 10\n\ndata:  Residuals\nLM test = 24.948, df = 10, p-value = 0.005445\n\n\nWe can see that the residuals are well-behaved: mean around zero, no outliers and no clear trend. There is an autocorrelation signal in the third lag evidenced by the ACF, but given that it is relatively small and far enough away, I believe it can be overlooked.\nAfter confirming the validity of the model, we can use it for various purposes. The estimated coefficients provide interesting rules of thumb for everyday use. For example, the dlog_CI_USD coefficient measures the pass-through from imported prices to inflation. According to the model, a 10% increase in imported prices adds 0.17p.p to inflation in the current quarter.\nPlotting the model fit is a good way to check deviations of the target variable from its fundamentals (at least those that are taken into account in the model). We can use the augment function from the broom package, which returns a data frame with the fitted values, residuals, etc. The broom package also provides other functions that greatly facilitate the manipulation of the regression results, as we’ll see later.\n\nlibrary(broom)\ncp_fit_plot &lt;- cp_fit %&gt;% \n  augment() %&gt;% \n  left_join(\n    cp_reg_data %&gt;% \n      select(date) %&gt;% \n      rowid_to_column(var = '.rownames') %&gt;% \n      mutate(.rownames = as.character(.rownames))\n  ) %&gt;% \n  mutate(deviation = CPI_CORE - .fitted)\n\ncp_fit_plot %&gt;% \n  ggplot(aes(x = date)) +\n  geom_line(aes(y = CPI_CORE, color = 'Actual'), lwd = 1) +\n  geom_line(aes(y = .fitted, color = 'Model'), lwd = 1) +\n  geom_col(aes(y = deviation, fill = 'Deviation (Actual - Fitted)')) +\n  theme_light() +\n  theme(legend.position = 'top') +\n  scale_fill_manual(values = 'darkgrey') +\n  labs(\n    title = 'CPI Core: Actual vs. Fitted (%QoQ SA)',\n    x = '',\n    y = '%',\n    color = '',\n    fill = ''\n  )\n\n\n\n\nWhat is the role of economic activity in inflation in recent quarters? What about external factors, have they played a significant role in the overall result? It’s a common practice to quantify the contribution of each variable to the observed value in a given period. This kind of decomposition is obtained by multiplying the value of each variable in the period by the respective model coefficient.\nThis time, we can make use of the tidy function from the broom package which returns the model’s coefficients in tidy format.\n\ncp_decomp &lt;- cp_fit_plot %&gt;% \n  select(date, names(cp_fit$coefficients)) %&gt;% \n  pivot_longer(-date, names_to = 'term', values_to = 'value') %&gt;% \n  left_join(\n    cp_fit %&gt;% \n      broom::tidy() %&gt;% \n      select(term, estimate)\n  ) %&gt;% \n  mutate(contribution = value*estimate) %&gt;% \n  bind_rows(\n    cp_fit_plot %&gt;% \n      select(date, contribution = .resid) %&gt;% \n      mutate(term = 'residual')\n  )\n\ncp_decomp %&gt;% \n  ggplot(aes(x = date)) +\n  geom_col(aes(y = contribution, fill = term)) +\n  theme_light() +\n  scale_fill_brewer(type = 'qual', palette = 6) +\n  labs(\n    title = 'Contribution of each variable to Core CPI (p.p)',\n    x = '', \n    y = '', \n    fill = 'Variable'\n  )\n\n\n\n\nWe can see that inertia and expectations are the main drivers of inflation throughout the sample, although for specific periods economic activity and imported inflation played a significant role. I also included the residual term because it’s also important to know when factors other than those incorporated into the model are relevant to the outcome and the magnitude of its contribution to the overall result.\nObviously, the model can be used to produce forecasts. For this, we need to provide values for the exogenous variables including the lagged CPI. In the next section we’ll see a more complete approach on how to provide scenarios for future values. For now, I’ll only take the last value of each variable and add a small random variation.\n\nset.seed(123)\nnew_values &lt;- tibble(\n  CPI_CORE_lag = last(cp_reg_data$CPI_CORE),\n  CPI_EXP      = last(cp_reg_data$CPI_EXP)+rnorm(1),\n  YGAP_lag     = last(cp_reg_data$YGAP)+rnorm(1),\n  dlog_CI_USD  = last(cp_reg_data$dlog_CI_USD)+rnorm(1)\n)\npredict(cp_fit, new_values)\n\n       1 \n1.341488"
  },
  {
    "objectID": "multiple_equations.html",
    "href": "multiple_equations.html",
    "title": "11  Multiple equations model",
    "section": "",
    "text": "In the previous section, we saw how to estimate single equations to analyze the effect of exogenous variables on the outcome. However, economic relationships are often treated as a system of endogenous variables. This kind of framework allows us to analyze the effect of each variable on the whole system and the resulting feedbacks.\nFor example, interest rate hikes are expected to lower inflation through their contractionary impact on economic activity. In addition, the more restrictive monetary policy stance should lead to an appreciation of the exchange rate through greater capital inflows – this is especially true for emerging markets.\nIn the Phillips Curve we saw in the previous section, all these links were present but implicit. We could make all these links explicit by creating specific equations for both the exchange rate and capital inflows. In summary, working with systems of endogenous equations makes it possible to greatly expand the scope of the analysis by incorporating any conceived relationship.\nThis can be cumbersome to do manually, but it becomes quite simple with the bimets package. It provides a very concise and friendly interface to write down the whole system of equations, estimate its parameters and forecast based on scenarios for the exogenous variables. Those who want to delve deeper into all the features available in the package can refer to the vignette. For the purpose of this section, it will suffice to present a variation of the standard three-equation macroeconomic model composed of an IS curve, a Phillips Curve and a Monetary policy rule. The model is described as follows.\nThe IS curve relates the output gap to its own lag, the deviation of the real ex-ante interest rate from its equilibrium and the deviation of the terms of trade from its trend.\n\\[ \\tilde{y_t} = \\beta_1\\tilde{y}_{t-1} + \\beta_2(i_t - \\pi^{e}_{t,t+4|t} - i^{eq}_t)_{t-3} + \\beta_3\\tilde{ToT}_t + \\epsilon^{\\tilde{y}}_t \\] The Phillips Curve is the same from the previous section and relates the core CPI to its own lag, the expectation for the next twelve months, the percentage change in imported prices and output gap.\n\\[ \\pi_t = \\beta_4\\pi_{t-1} + \\beta_5\\pi^{e}_{t,t+4|t} + (1-\\beta_4 - \\beta_5)\\Delta e_{t-1} + \\beta_7\\tilde{y}_{t-1} + \\epsilon_t^{\\pi} \\] Lastly, the monetary policy rule relates the nominal interest rate to its own lags, the deviation of expected inflation from the target and the nominal equilibrium rate.\n\\[ i_t = \\beta_8i_{t-1} + \\beta_9i_{t-2} + (1-\\beta_8-\\beta_9)(\\pi^{e}_{t,t+4|t} - \\bar{\\pi_t}) + \\beta_{11}(i_t^{eq} + \\bar{\\pi}_t) + \\epsilon_t^{i} \\] The first step is to write down the system of equations according to the standard adopted by the package. A careful look at the equations blocks below should be enough to understand how they work.\n\nmodel_spec &lt;- \"\nMODEL\n\nCOMMENT&gt; IS Curve\nBEHAVIORAL&gt; YGAP\nTSRANGE 2005 1 2022 4\nEQ&gt; YGAP = b1*TSLAG(YGAP,1) + b2*TSLAG((MPR- CPI_EXP - IR_EQ),1) + b3*TOT_GAP\nCOEFF&gt; b1 b2 b3\n\nCOMMENT&gt; Phillips Curve\nBEHAVIORAL&gt; CPI_CORE\nTSRANGE 2005 1 2022 4\nEQ&gt; CPI_CORE = b4*TSLAG(CPI_CORE,1) + b5*CPI_EXP + b6*(TSDELTALOG(CI_USD)) + b7*TSLAG(YGAP,1)\nCOEFF&gt; b4 b5 b6 b7\nRESTRICT&gt; b4+b5+b6=1\n\nCOMMENT&gt; Monetary Policy Rule\nBEHAVIORAL&gt; MPR\nTSRANGE 2005 1 2022 4\nEQ&gt; MPR = b8*TSLAG(MPR,1) + b9*TSLAG(MPR,2) + b10*(CPI_EXP - CPI_TARGET_ADJ) + b11*(IR_EQ + CPI_TARGET_ADJ)\nCOEFF&gt; b8 b9 b10 b11\nRESTRICT&gt; b8+b9+b10=1\n\nEND\n\"\n\nThe next step is to load the model specification and the data. The data must be supplied as a list of time series objects (ts).\n\nlibrary(tidyverse)\nlibrary(bimets)\nbr_economy_data &lt;- readRDS('data/ch13_br_economy_data.rds') \nmacro_model     &lt;- LOAD_MODEL(modelText = model_spec)\n\nAnalyzing behaviorals...\nAnalyzing identities...\nOptimizing...\nLoaded model \"model_spec\":\n    3 behaviorals\n    0 identities\n   11 coefficients\n...LOAD MODEL OK\n\nmodel_data_ts   &lt;- br_economy_data %&gt;% \n  pivot_longer(-date, names_to = 'var', values_to = 'value') %&gt;% \n  plyr::dlply(\n    .variables = 'var', \n    .fun = function(x){\n      TIMESERIES(x$value, START = c(2004,1), FREQ = 4)\n    }\n  )\nmacro_model &lt;- LOAD_MODEL_DATA(\n  macro_model, \n  model_data_ts\n  )\n\nLoad model data \"model_data_ts\" into model \"model_spec\"...\n...LOAD MODEL DATA OK\n\n\nAt this point we are ready to estimate the model coefficients. By default the equations are estimated using OLS, but it’s also possible to use Instrumental Variables (IV). I used quietly = TRUE to suppress the model output print because it’s lengthy. But I do recommend you to take a glance at it by setting this argument to FALSE or by removing it altogether.\n\nmodel_fit &lt;- ESTIMATE(\n  macro_model, \n  estTech = 'OLS',\n  quietly = TRUE\n)\n\nFinally, we can use our estimated model to produce forecasts for future values. For this, we first need to provide future values for the exogenous variables. The TSEXTEND function makes this process very simple by providing several ways to extend the time series of the exogenous variables. In the example below, I made different assumptions to illustrate some of the possibilities.\nFor example, I assumed that both the CPI target and the equilibrium real interest rate will remain constant; the expectations for CPI will evolve according to a linear trend; the gap of terms of trade will decrease by 2.4% each quarter, which is the mean of the last four quarters; and the imported prices will decrease 2% each quarter, partially reverting the surge of the last years.\n\nmodel_fit$modelData &lt;-  within(\n  model_fit$modelData, {\n    CPI_TARGET_ADJ = TSEXTEND(CPI_TARGET_ADJ, UPTO=c(2026,1),EXTMODE='CONSTANT')\n    CPI_EXP        = TSEXTEND(CPI_EXP,        UPTO=c(2026,1),EXTMODE='LINEAR')\n    CI_USD         = TSEXTEND(CI_USD,         UPTO=c(2026,1),EXTMODE='MYRATE', FACTOR = (1-0.02))\n    IR_EQ          = TSEXTEND(IR_EQ,          UPTO=c(2026,1),EXTMODE='CONSTANT')\n    TOT_GAP        = TSEXTEND(TOT_GAP,        UPTO=c(2026,1),EXTMODE='MYCONST', FACTOR = -2.4)\n  }\n)\n\nWe can convert this object into tidy format in order to make it easier to plot all the extended time series in a single panel.\n\n\nShow the code\ndo.call(ts.union, model_fit$modelData) %&gt;%\n  timetk::tk_tbl() %&gt;% \n  pivot_longer(-index, names_to = 'var', values_to = 'value') %&gt;% \n  filter(!var %in% c('CPI_CORE', 'YGAP', 'MPR')) %&gt;%\n  mutate(scenario = if_else(index &gt;= '2023 Q1', 'Y', 'N')) %&gt;% \n  ggplot(aes(x = index, y = value, color = scenario)) +\n  geom_line(lwd = 1) +\n  scale_color_manual(values = c('black', 'red')) +\n  facet_wrap(~ var, scales = 'free_y') +\n  theme(legend.position = 'top') +\n  zoo::scale_x_yearqtr(n = 5, format = '%Y') +\n  labs(\n    title = 'Scenarios for exogenous variables',\n    x = '',\n    y = ''\n  )\n\n\n\n\n\nIn the last step we call the SIMULATE function. Again, with a few lines of code we can transform the output into tidy format to produce a prettier plot.\n\nmodel_sim &lt;- SIMULATE(\n  model_fit,\n  simType        = 'FORECAST',\n  TSRANGE        = c(2023,1,2025,1),\n  simConvergence = 0.00001,\n  simIterLimit   = 100,\n  quietly        = TRUE\n)\n\n\n\nShow the code\noutput &lt;- do.call(ts.intersect, model_fit$modelData) %&gt;% \n  timetk::tk_tbl(rename_index = 'date') %&gt;% \n  mutate(type = 'Observed') %&gt;% \n  bind_rows(\n    do.call(ts.intersect, model_sim$simulation[1:3]) %&gt;% \n      timetk::tk_tbl(rename_index = 'date') %&gt;% \n      mutate(type = 'Forecast')\n  ) %&gt;% \n  mutate(date = zoo::as.yearqtr(date))\noutput %&gt;% \n  select(date, YGAP, CPI_CORE, MPR, type) %&gt;% \n  pivot_longer(-c(date, type), names_to = 'var', values_to = 'value') %&gt;% \n  ggplot(aes(x = date)) +\n  geom_line(aes(y = value, color = type, linetype = type)) +\n  scale_linetype_manual(values = c(2,1)) +\n  zoo::scale_x_yearqtr(n = 10, format = '%YQ%q') +\n  facet_wrap(~ var, scales = 'free_y', nrow = 3) +\n  theme(legend.position = 'top') +\n  labs(\n    title = 'Forecasts for economic variables',\n    x = '',\n    y = '',\n    linetype = '',\n    color = ''\n    )"
  },
  {
    "objectID": "ss_intro.html",
    "href": "ss_intro.html",
    "title": "12  Introduction",
    "section": "",
    "text": "When making a draft of this book, I didn’t intend to devote an entire chapter to state-space (SS) models until I started using it a lot in my daily work and then realized it was too important to be neglected. In addition, the subject often goes unmentioned in both data science and econometrics textbooks despite its usefulness in many applications where alternatives methods are certainly inferior.\nAfter a brief introduction, the next two sections will focus on basic applications of SS models: 1. Estimate time-varying coefficients from regression models; and 2. Estimate a common underlying process when there are multiple sources of information. For those interested in delving into the details of the subject, I strongly recommend start by reading the excellent Holmes, Scheuerell, and Ward (2021) link – which will be a valuable reference for the following applications.\nLet’s think about state-space models as a representation of an idea rather than a method. Suppose we need to measure air temperature in Rio de Janeiro city. We can’t observe it directly, but we have available data collected by a sensor. This sensor performs very well on average, although it’s subject to measurement errors. The main goal is to accurately estimate the real air temperature from the somewhat noisy data we get from the sensor.\nIn the SS representation, this problem is summarised by two equations. The observation equation tell us the data we observe (\\(y_t\\)) is the real unobserved temperature (\\(x_t\\)) plus \\(v_t\\) which is a Gaussian error with zero mean and variance \\(\\sigma^2_v\\). We could add terms for seasonality, exogenous regressors or dummies variables as well. For now, we’ll stick to the simplest specification. In matrix form:\n\\[\n\\begin{align}\ny_t = Z_tx_{t} + v_t\n\\end{align}\n\\]\nThe state process (or transition) equation describes how the unobservable real air temperature (the hidden state, \\(x_t\\)) evolves over time. It’s usually characterized by an autoregressive process – often a random walk –, where \\(w_t\\) is also a Gaussian error term with zero mean and variance \\(\\sigma^2_w\\). In matrix form:\n\\[\n\\begin{align}\nx_t = B_tx_{t-1} + w_t\n\\end{align}\n\\]\nFinding the hidden state trajectory requires us to solve for this linear stochastic dynamical system, which can be accomplished by several algorithms – the most known are the Expectation Maximization (EM) and the Kalman Filter and Smoother, the latter you may already have heard of. In the next two sections, we’ll see how to build and estimate such models with the MARSS package.\n\n\n\n\nHolmes, E. E., M. D. Scheuerell, and E. J. Ward. 2021. Analysis of Multivariate Time Series Using the MARSS Package, Version 3.11.4. NOAA Fisheries, Northwest Fisheries Science Center."
  },
  {
    "objectID": "ss_tv_coef.html#footnotes",
    "href": "ss_tv_coef.html#footnotes",
    "title": "13  Time-varying regression coefficient",
    "section": "",
    "text": "We’ll use a AR(1) model for simplicity, but the same logic applies to more complex models.↩︎"
  },
  {
    "objectID": "ss_common.html#writing-out-the-dfm-as-marss",
    "href": "ss_common.html#writing-out-the-dfm-as-marss",
    "title": "14  Dynamic Factor Model",
    "section": "14.1 Writing out the DFM as MARSS",
    "text": "14.1 Writing out the DFM as MARSS\nWe start by writing out the above DFM in MARSS form. The LHS of the observation equation contains now the \\(k\\) time series corresponding to each observation of the common underlying process, all of them is a function of a single hidden process (the common component) plus a Gaussian innovation term.1\n\\[\n\\underbrace{\n\\begin{bmatrix}\ny_{1t} \\\\\ny_{2t} \\\\\ny_{3t} \\\\\n\\vdots \\\\\ny_{kt}\n\\end{bmatrix}\n}_{y_t}\n=\n\\underbrace{\n\\begin{bmatrix}\nz_1 \\\\\nz_2 \\\\\nz_3 \\\\\n\\vdots \\\\\nz_k\n\\end{bmatrix}\n}_{Z}\n\\begin{bmatrix}\nx_t\n\\end{bmatrix}\n+\nv_t \\sim N\n\\begin{pmatrix}\n0,\n\\underbrace{\n\\begin{bmatrix}\n\\sigma^2_{y_{1t}} & 0 & 0 & \\dots & 0 \\\\\n0 & \\sigma^2_{y_{2t}} & 0 & \\dots & 0 \\\\\n0 & 0 & \\sigma^2_{y_{3t}} & \\dots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots &  0 \\\\\n0 & 0 & 0 & \\dots & \\sigma^2_{y_{kt}}\n\\end{bmatrix}\n}_{R}\n\\end{pmatrix}\n\\] The transition equation, in turn, is a function of its own \\(p\\) lags. In the MARSS form each lag will be represented by a separate variable, although only the first one, \\(x_t\\), will have an autoregressive structure. The others are simply treated as identities.\n\\[\n\\underbrace{\n\\begin{bmatrix}\nx_{t} \\\\\nx_{t-1} \\\\\nx_{t-2} \\\\\n\\vdots \\\\\nx_{t-p+1} \\\\\n\\end{bmatrix}\n}_{x_t}\n=\n\\underbrace{\n\\begin{bmatrix}\nb_1 & b_2 & b_3 & \\dots & b_p \\\\\n1 & 0 & 0 & \\dots & 0 \\\\\n0 & 1 & 0 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\dots & 1\n\\end{bmatrix}\n}_{B}\n\\underbrace{\n\\begin{bmatrix}\nx_{t-1} \\\\\nx_{t-2} \\\\\nx_{t-3} \\\\\n\\vdots \\\\\nx_{t-p}\n\\end{bmatrix}\n}_{x_{t-1}}\n+\nw_t \\sim N(0, Q)\n\\]"
  },
  {
    "objectID": "ss_common.html#importing-data",
    "href": "ss_common.html#importing-data",
    "title": "14  Dynamic Factor Model",
    "section": "14.2 Importing data",
    "text": "14.2 Importing data\nFor this exercise, we’ll use data on the 51 subsectors that make up Brazilian CPI. All these time series are the monthly percentage change accumulated in the last twelve months. This eliminates the need to add seasonal components to the model.\nFor aesthetic reasons, the piece of code that downloads and prepare the data has been omitted. The interested reader can display it by clicking on ‘Show the code’. Basically, the final output is \\(k \\times t\\) matrix of the \\(k\\) subsectors time series over \\(t\\) time steps.\n\n\nShow the code\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(sidrar)\nlibrary(MARSS)\n# Import CPI data\nsidra_codes &lt;- list(\n  'cpi_asof2020'     = '/t/7060/n1/all/v/63/p/all/c315/all/d/v2265%202',\n  'cpi_upto2019'     = '/t/1419/n1/all/v/63/p/all/c315/all/d/v2265%202',\n  'weights_asof2020' = '/t/7060/n1/all/v/66/p/all/c315/all/d/v2265%202',\n  'peso_upto2019'    = '/t/1419/n1/all/v/66/p/all/c315/all/d/v2265%202'\n  )\ncpi_data &lt;- purrr::map(.x = sidra_codes, .f = ~ get_sidra(api = .x))\ncpi_df   &lt;- cpi_data %&gt;% \n  plyr::ldply() %&gt;% \n  select(\n    var   = `Variável`, \n    item  = `Geral, grupo, subgrupo, item e subitem`,\n    date  = `Mês (Código)`,\n    value = Valor\n  ) %&gt;% \n  mutate(\n    item = ifelse(item == 'Índice geral', '1001.CPI Headline', item),\n    var  = ifelse(str_detect(var, 'Variação'), 'CPI', 'Weight')\n    ) %&gt;% \n  separate(col = item, into = c('code', 'item'), sep = '\\\\.') %&gt;% \n  mutate(date = paste0(date, '01') %&gt;% ymd()) %&gt;% \n  mutate(across(contains('code'), ~ as.numeric(.x))) %&gt;% \n  relocate(date) %&gt;%\n  filter(str_length(code) == 4) %&gt;% \n  unite(col = 'item', c('code', 'item'), sep = '.') %&gt;% \n  arrange(date) %&gt;% \n  plyr::dlply(.variables = 'var') %&gt;% \n  map(.f = ~ .x %&gt;% select(-var))\n\nCPI_reg &lt;- cpi_df$CPI %&gt;% \n  pivot_wider(\n    names_from = 'item', \n    values_from = 'value'\n    ) %&gt;% \n  select(where(~ sum(is.na(.x)) == 0))\n\nCPI_twelveMonths &lt;- CPI_reg %&gt;% \n  arrange(date) %&gt;% \n  mutate(across(-date, ~ ((RcppRoll::roll_prodr(1+.x/100, 12))-1)*100)) %&gt;% \n  filter(between(date, ymd('2012-12-01'), ymd('2022-12-01')))\n\nCPI_ts &lt;- CPI_twelveMonths %&gt;% \n  arrange(date) %&gt;%\n  select(-c(date, contains('geral'))) %&gt;%\n  ts(start = c(2012,12), freq = 12) %&gt;% \n  as.matrix() %&gt;% \n  t()"
  },
  {
    "objectID": "ss_common.html#estimating-the-model",
    "href": "ss_common.html#estimating-the-model",
    "title": "14  Dynamic Factor Model",
    "section": "14.3 Estimating the model",
    "text": "14.3 Estimating the model\nNext, we declare the system matrices. Note that by defining the \\(R\\) matrix as diagonal and unequal, we are assuming that the shocks to prices across the subsectors are independent. This is arguably a very strong assumption, but considering cross-dependence between subsectors would greatly increase the number of parameters to be estimated. Also, we are considering four lags autorregressive terms in prices dynamics.\n\n# Define model's specification\nmodel.spec   &lt;- list()\nmodel.spec$B &lt;- matrix(\n  list(\n    \"b1\", 1, 0, 0, \n    \"b2\", 0, 1, 0, \n    \"b3\", 0, 0, 1, \n    \"b4\", 0, 0, 0\n    ), 4, 4)\nmodel.spec$Z &lt;- matrix(\n  list(0), nrow(CPI_ts), 4)\nmodel.spec$Z[, 1] &lt;- rownames(CPI_ts)\nmodel.spec$Q &lt;- \"diagonal and equal\"\nmodel.spec$R &lt;- \"diagonal and unequal\"\nmodel.spec$A &lt;- \"zero\" \n# Estimate using the EM Algorithm\nmodel.em &lt;- MARSS(\n  CPI_ts, \n  model  = model.spec, \n  inits  = list(x0 = 0), \n  silent = TRUE\n  )"
  },
  {
    "objectID": "ss_common.html#results",
    "href": "ss_common.html#results",
    "title": "14  Dynamic Factor Model",
    "section": "14.4 Results",
    "text": "14.4 Results\nAfter estimating the system, we are ready to compute the common inflation component, \\(\\chi_{it}\\). Basically, \\(\\chi_{it}\\) is the dot product of the \\(\\lambda_i\\) elements (which is the \\(Z\\) matrix) and \\(f_t\\) (which is the estimate of the hidden state, \\(x_t\\)). The aggregate common inflation is obtained as the sum of the contributions of each individual subsector to the headline CPI, using their original weights.\n\n# Compute model's components\nft         &lt;- model.em$states[1, ]\nnames(ft)  &lt;- CPI_twelveMonths$date\nft_df      &lt;- ft %&gt;% \n  as.data.frame() %&gt;% \n  rownames_to_column() %&gt;% \n  magrittr::set_colnames(c('date', 'ft'))\nlambda   &lt;- model.em$par$Z %&gt;% \n  as.data.frame() %&gt;% \n  tibble::rownames_to_column() %&gt;% \n  magrittr::set_colnames(c('item', 'lambda'))\nchi_it   &lt;- lambda %&gt;% \n  mutate(\n    chi_it = map(\n      .x = lambda, \n      .f = ~ {\n        df &lt;- .x*ft %&gt;% \n          as.data.frame()\n        df &lt;- df %&gt;% \n          rownames_to_column() %&gt;% \n          magrittr::set_colnames(c('date', 'chi_it')) %&gt;% \n          mutate(date = ymd(date))\n      }\n    ) \n  ) %&gt;% \n  unnest(chi_it) %&gt;% \n  left_join(\n    cpi_df$Weight %&gt;% \n      rename(c('weight' = 'value'))\n  ) %&gt;% \n  relocate(date)\n# Commpute the common trend\ncommon_trend &lt;- chi_it %&gt;% \n  group_by(date) %&gt;% \n  summarise(\"Common Inflation\" = sum(chi_it*weight)/sum(weight)) %&gt;%  \n  ungroup() %&gt;% \n  left_join(\n    CPI_twelveMonths %&gt;% \n      select(date, `CPI Headline` = contains('CPI'))\n  )\n\n\n\nShow the code\ncommon_trend %&gt;% \n  pivot_longer(-date, names_to = 'var', values_to = 'value') %&gt;% \n  ggplot(aes(x = date)) +\n  geom_line(aes(y = value, color = var), lwd = 1) +\n  theme(legend.position = 'top') +\n  labs(\n    title = 'Brazil Common Inflation - % 12-month',\n    x = '',\n    y = '% 12-month',\n    color = '')\n\n\n\n\n\n\n\n\n\nEli, Nir, Haberkorn Flora, and Cascaldi-Garcia Danilo. 2021. “International Measures of Common Inflation.”\n\n\nHolmes, E. E., M. D. Scheuerell, and E. J. Ward. 2021. Analysis of Multivariate Time Series Using the MARSS Package, Version 3.11.4. NOAA Fisheries, Northwest Fisheries Science Center."
  },
  {
    "objectID": "ss_common.html#footnotes",
    "href": "ss_common.html#footnotes",
    "title": "14  Dynamic Factor Model",
    "section": "",
    "text": "Eli, Flora, and Danilo (2021) consider the innovation term as following an AR(1) process. Incorporating this assumption would require an even more complex transition equation.↩︎"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Eli, Nir, Haberkorn Flora, and Cascaldi-Garcia Danilo. 2021.\n“International Measures of Common Inflation.”\n\n\nGrolemund, G., and H. Wickham. 2017. R for Data Science, 1st\nEdition. O’Reilly Media.\n\n\nHamilton, James D. 2017. “Why You Should Never Use the\nHodrick-Prescott Filter.”\n\n\nHolmes, E. E., M. D. Scheuerell, and E. J. Ward. 2021. Analysis of\nMultivariate Time Series Using the MARSS Package, Version 3.11.4.\nNOAA Fisheries, Northwest Fisheries Science Center.\n\n\nHyndman, R. J., and G. Athanasopoulos. 2018. Forecasting: Principles\nand Practice, 2nd Edition. OTexts: Melbourne, Australia.\n\n\nHyndman, Robert J., and Anne B. Koehler. 2005. “Another Look at\nMeasures of Forecast Accuracy.”\n\n\nWickham, H., Danielle Navarro, and Thomas Lin Pedersen. 2019.\nGgplot2: Elegant Graphics for Data Analysis, 3rd Edition.\nSpringer."
  }
]